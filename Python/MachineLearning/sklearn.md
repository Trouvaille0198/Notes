# ç®€ä»‹

# ç‰¹å¾å·¥ç¨‹

(Feature Engineering)

ç‰¹å¾å·¥ç¨‹æ˜¯ä½¿ç”¨ä¸“ä¸šèƒŒæ™¯çŸ¥è¯†å’ŒæŠ€å·§å¤„ç†æ•°æ®**ï¼Œ**ä½¿å¾—ç‰¹å¾èƒ½åœ¨æœºå™¨å­¦ä¹ ç®—æ³•ä¸Šå‘æŒ¥æ›´å¥½çš„ä½œç”¨çš„è¿‡ç¨‹

## æ•°æ®é›†

*scikit-learn* æä¾›äº†ä¸€äº›æ ‡å‡†æ•°æ®é›†

```python
from sklearn import datasets
```

### å°è§„æ¨¡ load_\*()

***datasets.load_\*()***

è·å–å°è§„æ¨¡æ•°æ®é›†ï¼Œæ•°æ®åŒ…å«åœ¨datasetsé‡Œ

```python
data1 = datasets.load_iris()
data2 = datasets.load_boston()
```

### å¤§è§„æ¨¡ fetch_\*()

***datasets.fetch_\*(data_home, subset)***

è·å–å¤§è§„æ¨¡æ•°æ®é›†ï¼Œéœ€è¦ä»ç½‘ç»œä¸Šä¸‹è½½

- *data_home*ï¼šæ•°æ®é›†ä¸‹è½½çš„ç›®å½•ï¼Œé»˜è®¤æ˜¯ ~/scikit_learn_data/
- *subset*ï¼šé€‰æ‹©è¦åŠ è½½çš„æ•°æ®é›†ã€‚'train'æˆ–è€…'test'ï¼Œ'all'ï¼Œå¯é€‰

### è¿”å›å€¼

load å’Œ fetch è¿”å›çš„æ•°æ®ç±»å‹ä¸º *datasets.base.Bunch* (å­—å…¸ç»§æ‰¿)

- *data*ï¼šç‰¹å¾æ•°æ®æ•°ç»„ï¼Œæ˜¯äºŒç»´ numpy.ndarray æ•°ç»„
- *targetï¼š*æ ‡ç­¾æ•°ç»„ï¼Œæ˜¯ n_samples çš„ä¸€ç»´ numpy.ndarray æ•°ç»„
- *DESCR*ï¼šæ•°æ®æè¿°
- *feature_names*ï¼šç‰¹å¾å,æ–°é—»æ•°æ®ï¼Œæ‰‹å†™æ•°å­—ã€å›å½’æ•°æ®é›†æ²¡æœ‰
- *target_names*ï¼šæ ‡ç­¾å

```python
from sklearn.datasets import load_iris
# è·å–é¸¢å°¾èŠ±æ•°æ®é›†
iris = load_iris()
print("é¸¢å°¾èŠ±æ•°æ®é›†çš„è¿”å›å€¼ï¼š\n", iris) # å°†æ‰€æœ‰å‚æ•°å…¨éƒ¨è¿”å›ï¼Œè¿”å›å€¼æ˜¯ä¸€ä¸ªç»§æ‰¿è‡ªå­—å…¸çš„Bench
print("é¸¢å°¾èŠ±çš„ç‰¹å¾å€¼:\n", iris.data)
print("é¸¢å°¾èŠ±çš„ç›®æ ‡å€¼ï¼š\n", iris.target)
print("é¸¢å°¾èŠ±ç‰¹å¾çš„åå­—ï¼š\n", iris.feature_names)
print("é¸¢å°¾èŠ±ç›®æ ‡å€¼çš„åå­—ï¼š\n", iris.target_names)
print("é¸¢å°¾èŠ±çš„æè¿°ï¼š\n", iris.DESCR)

# åŒæ ·å¯ä»¥å†™ä½œè¯¸å¦‚iris['data']çš„æ ¼å¼
```

è¾“å‡º

```pyhton
 é¸¢å°¾èŠ±çš„ç‰¹å¾å€¼:
 [[5.1 3.5 1.4 0.2]
 [4.9 3.  1.4 0.2]
 [4.7 3.2 1.3 0.2]
 [4.6 3.1 1.5 0.2]
 ...
 [6.3 2.5 5.  1.9]
 [6.5 3.  5.2 2. ]
 [6.2 3.4 5.4 2.3]
 [5.9 3.  5.1 1.8]]
    
 é¸¢å°¾èŠ±çš„ç›®æ ‡å€¼ï¼š
 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]
 
 é¸¢å°¾èŠ±ç‰¹å¾çš„åå­—ï¼š
 ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
 
 é¸¢å°¾èŠ±ç›®æ ‡å€¼çš„åå­—ï¼š
 ['setosa' 'versicolor' 'virginica']
 
 é¸¢å°¾èŠ±çš„æè¿°ï¼š
 .. _iris_dataset:

Iris plants dataset
--------------------

**Data Set Characteristics:**

    :Number of Instances: 150 (50 in each of three classes)
 ...
   - Many, many more ...
```

## æ•°æ®é›†åˆ’åˆ†

### ç®€å•åˆ’åˆ† train_test_split

```python
from sklearn.model_selection import train_test_split
```

***train_test_split ( arrays, \*options )***

- xï¼šæ•°æ®é›†çš„ç‰¹å¾å€¼
- yï¼šæ•°æ®é›†çš„æ ‡ç­¾å€¼
- test_sizeï¼šæµ‹è¯•é›†çš„å¤§å°ï¼Œä¸€èˆ¬ä¸º float
- random_stateï¼šéšæœºæ•°ç§å­,ä¸åŒçš„ç§å­ä¼šé€ æˆä¸åŒçš„éšæœºé‡‡æ ·ç»“æœã€‚ç›¸åŒçš„ç§å­é‡‡æ ·ç»“æœç›¸åŒã€‚

è¿”å›ï¼šæµ‹è¯•é›†ç‰¹å¾å€¼ï¼Œæµ‹è¯•é›†æ ‡ç­¾ï¼Œè®­ç»ƒé›†ç‰¹å¾å€¼ï¼Œè®­ç»ƒé›†æ ‡ç­¾ï¼ˆé»˜è®¤éšæœºå–ï¼‰

```python
x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=22)
```

### K æŠ˜äº¤å‰éªŒè¯

ï¼ˆKFold Cross Validationï¼‰

```python
from sklearn.model_selection import KFold
```

***KFold ( n_splits=5, \*, shuffle=False, random_state=None )***

è¿”å› k æŠ˜å¯¹è±¡

å‚æ•°

- *n_splits*ï¼šK å­é›†ä¸ªæ•°ï¼Œint, default=5
- *shuffle*ï¼šæ˜¯å¦è¦æ´—ç‰Œï¼ˆæ‰“ä¹±æ•°æ®ï¼‰ï¼Œbool, default=False
- *random_state*ï¼šint or RandomState instance, default=None

æ–¹æ³•

- *get_n_splits([X, y, groups])*ï¼šè¿”å›è¿­ä»£æ¬¡æ•°ï¼ŒReturns the number of splitting iterations in the cross-validator
- *split(X)*ï¼šç”Ÿæˆå™¨ï¼Œè¿”å›è®­ç»ƒå’Œæµ‹è¯•é›†çš„ç´¢å¼•å€¼ï¼ŒGenerate indices to split data into training and test set.

```python
from sklearn.model_selection import KFold
from sklearn import datasets

# æ•°æ®é›†å¯¼å…¥
iris = datasets.load_iris()
x = iris.data
y = iris.target
# KFold
kf = KFold(n_splits=5)
# è¾“å‡ºåˆ’åˆ†æ•°
print(kf.get_n_splits(x))
# åˆ’åˆ†æ•°æ®é›†
for train_index, test_index in kf.split(x):
    print("TRAIN:", train_index, "\nTEST:", test_index)
    x_train, x_test = x[train_index], x[test_index]
    y_train, y_test = y[train_index], y[test_index]
```

è¾“å‡º

```python
5

TRAIN: [ 30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47
  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65
  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83
  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101
 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119
 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137
 138 139 140 141 142 143 144 145 146 147 148 149] 
TEST: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29]

TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  60  61  62  63  64  65
  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83
  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101
 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119
 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137
 138 139 140 141 142 143 144 145 146 147 148 149] 
TEST: [30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53
 54 55 56 57 58 59]

TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  90  91  92  93  94  95  96  97  98  99 100 101
 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119
 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137
 138 139 140 141 142 143 144 145 146 147 148 149] 
TEST: [60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83
 84 85 86 87 88 89]

TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137
 138 139 140 141 142 143 144 145 146 147 148 149] 
TEST: [ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119]

TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119] 
TEST: [120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137
 138 139 140 141 142 143 144 145 146 147 148 149]
```

## ç‰¹å¾æŠ½å–

ï¼ˆfeature extractionï¼‰

å°†ä»»æ„æ•°æ®ï¼ˆå¦‚æ–‡æœ¬æˆ–å›¾åƒï¼‰è½¬æ¢ä¸ºå¯ç”¨äºæœºå™¨å­¦ä¹ çš„æ•°å­—ç‰¹å¾

```python
import sklearn.feature_extraction
```

### å­—å…¸ç‰¹å¾æå– DictVectorizer 

å®ƒæ˜¯ä¸€ä¸ªè½¬æ¢å™¨ï¼Œåº”ç”¨æ—¶éœ€è¦è¿›è¡Œå®ä¾‹åŒ–

#### API

***sklearn.feature_extraction.DictVectorizer(sparse=True,â€¦)***

- *DictVectorizer.fit_transform(X)*
  - Xï¼šå­—å…¸æˆ–è€…åŒ…å«å­—å…¸çš„è¿­ä»£å™¨
  - è¿”å› sparse çŸ©é˜µæˆ– array æ•°ç»„
- *DictVectorizer.inverse_transform(X)*
  - Xï¼šarray æ•°ç»„æˆ–è€… sparse çŸ©é˜µ 
  - è¿”å›è½¬æ¢ä¹‹å‰æ•°æ®æ ¼å¼
- *DictVectorizer.get_feature_names()* 
  - è¿”å›ç±»åˆ«åç§°

#### ä¾‹

æµç¨‹åˆ†æ

- å®ä¾‹åŒ–ç±» DictVectorizer
- è°ƒç”¨ fit_transform æ–¹æ³•è¾“å…¥æ•°æ®å¹¶è½¬æ¢ï¼ˆæ³¨æ„è¿”å›æ ¼å¼ï¼‰

```python
from sklearn.feature_extraction import DictVectorizer

data = [{'city': 'åŒ—äº¬','temperature':100}, {'city': 'ä¸Šæµ·','temperature':60}, {'city': 'æ·±åœ³','temperature':30}]

# 1ã€å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
transfer = DictVectorizer(sparse=False)# æ‹’ç»è¿”å›ç¨€ç–çŸ©é˜µ
# 2ã€è°ƒç”¨fit_transform
data = transfer.fit_transform(data)

print("è¿”å›çš„ç»“æœ:\n", data)
print("ç‰¹å¾åå­—ï¼š\n", transfer.get_feature_names())
```

è¾“å‡º

```python
è¿”å›çš„ç»“æœ:
[[  0.   1.   0. 100.]
 [  1.   0.   0.  60.]
 [  0.   0.   1.  30.]]
ç‰¹å¾åå­—ï¼š
 ['city=ä¸Šæµ·', 'city=åŒ—äº¬', 'city=æ·±åœ³', 'temperature']
```

è‹¥è¿”å›ç¨€ç–çŸ©é˜µï¼Œæ”¹sparse=True

```python
è¿”å›çš„ç»“æœ:
(0, 1)	1.0
(0, 3)	100.0
(1, 0)	1.0
(1, 3)	60.0
(2, 2)	1.0
(2, 3)	30.0
ç‰¹å¾åå­—ï¼š
 ['city=ä¸Šæµ·', 'city=åŒ—äº¬', 'city=æ·±åœ³', 'temperature']
```

è¿™ä¸ªå¤„ç†æ•°æ®çš„æŠ€å·§å«åš *one-hot* ç¼–ç 

### æ–‡æœ¬è¯é¢‘ç‰¹å¾æå– text.CountVectorizer

å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œè¯é¢‘ç‰¹å¾å€¼åŒ–

å®ƒæ˜¯ä¸€ä¸ªè½¬æ¢å™¨ï¼Œåº”ç”¨æ—¶éœ€è¦è¿›è¡Œå®ä¾‹åŒ–

#### API

***CountVectorizer(stop_words=[])***

- CountVectorizer.fit_transform(X)
  -  Xï¼šæ–‡æœ¬æˆ–è€…åŒ…å«æ–‡æœ¬å­—ç¬¦ä¸²çš„å¯è¿­ä»£å¯¹è±¡
  - è¿”å› sparse çŸ©é˜µ
- CountVectorizer.inverse_transform(X)
  -  Xï¼šarray æ•°ç»„æˆ–è€… sparse çŸ©é˜µ 
  - è¿”å›è½¬æ¢ä¹‹å‰æ•°æ®æ ¼
- CountVectorizer.get_feature_names() 
  - è¿”å›å€¼å•è¯åˆ—è¡¨

#### ä¾‹

æµç¨‹åˆ†æ

- å®ä¾‹åŒ–ç±» CountVectorizer
- è°ƒç”¨ fit_transform æ–¹æ³•è¾“å…¥æ•°æ®å¹¶è½¬æ¢ ï¼ˆæ³¨æ„è¿”å›æ ¼å¼ï¼Œåˆ©ç”¨ toarray() è¿›è¡Œ sparse çŸ©é˜µè½¬æ¢ array æ•°ç»„ï¼‰

```python
from sklearn.feature_extraction.text import CountVectorizer

data = ["life is short,i like like python",
        "life is too long,i dislike python"]

# 1ã€å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
transfer = CountVectorizer()
# 2ã€è°ƒç”¨fit_transform
data = transfer.fit_transform(data)

print("æ–‡æœ¬ç‰¹å¾æŠ½å–çš„ç»“æœï¼š\n", data.toarray())
print("è¿”å›ç‰¹å¾åå­—ï¼š\n", transfer.get_feature_names())
```

è¾“å‡ºï¼ˆå› ä¸ºæ²¡æœ‰ sparse å‚æ•°ï¼Œè‹¥è¦è½¬æ¢æˆäºŒç»´æ•°ç»„å½¢å¼ï¼Œéœ€è¦åˆ©ç”¨ `toarray()`ï¼‰

```python
æ–‡æœ¬ç‰¹å¾æŠ½å–çš„ç»“æœï¼š
 [[0 1 1 2 0 1 1 0]
 [1 1 1 0 1 1 0 1]]
è¿”å›ç‰¹å¾åå­—ï¼š
 ['dislike', 'is', 'life', 'like', 'long', 'python', 'short', 'too']
```

è‹¥ç›´æ¥è¿”å›ç¨€ç–çŸ©é˜µ

```python
æ–‡æœ¬ç‰¹å¾æŠ½å–çš„ç»“æœï¼š
(0, 2)	1
(0, 1)	1
(0, 6)	1
(0, 3)	2
(0, 5)	1
(1, 2)	1
(1, 1)	1
(1, 5)	1
(1, 7)	1
(1, 4)	1
(1, 0)	1
è¿”å›ç‰¹å¾åå­—ï¼š
 ['dislike', 'is', 'life', 'like', 'long', 'python', 'short', 'too']
```

#### ä¸­æ–‡å¤„ç†

ä½¿ç”¨ jieba åˆ†è¯åº“

***jieba.cut()***

- è¿”å›è¯è¯­ç»„æˆçš„ç”Ÿæˆå™¨

åˆ†æ

- å‡†å¤‡å¥å­ï¼Œåˆ©ç”¨jieba.cutè¿›è¡Œåˆ†è¯
- å®ä¾‹åŒ–CountVectorizer
- å°†åˆ†è¯ç»“æœå˜æˆå­—ç¬¦ä¸²å½“ä½œfit_transformçš„è¾“å…¥å€¼

```python
from sklearn.feature_extraction.text import CountVectorizer
import jieba


def cut_word(text):
    # ç”¨ç»“å·´å¯¹ä¸­æ–‡å­—ç¬¦ä¸²è¿›è¡Œåˆ†è¯
    text = " ".join(list(jieba.cut(text)))
    return text


data = ["ä¸€ç§è¿˜æ˜¯ä¸€ç§ä»Šå¤©å¾ˆæ®‹é…·ï¼Œæ˜å¤©æ›´æ®‹é…·ï¼Œåå¤©å¾ˆç¾å¥½ï¼Œä½†ç»å¯¹å¤§éƒ¨åˆ†æ˜¯æ­»åœ¨æ˜å¤©æ™šä¸Šï¼Œæ‰€ä»¥æ¯ä¸ªäººä¸è¦æ”¾å¼ƒä»Šå¤©ã€‚",
        "æˆ‘ä»¬çœ‹åˆ°çš„ä»å¾ˆè¿œæ˜Ÿç³»æ¥çš„å…‰æ˜¯åœ¨å‡ ç™¾ä¸‡å¹´ä¹‹å‰å‘å‡ºçš„ï¼Œè¿™æ ·å½“æˆ‘ä»¬çœ‹åˆ°å®‡å®™æ—¶ï¼Œæˆ‘ä»¬æ˜¯åœ¨çœ‹å®ƒçš„è¿‡å»ã€‚",
        "å¦‚æœåªç”¨ä¸€ç§æ–¹å¼äº†è§£æŸæ ·äº‹ç‰©ï¼Œä½ å°±ä¸ä¼šçœŸæ­£äº†è§£å®ƒã€‚äº†è§£äº‹ç‰©çœŸæ­£å«ä¹‰çš„ç§˜å¯†å–å†³äºå¦‚ä½•å°†å…¶ä¸æˆ‘ä»¬æ‰€äº†è§£çš„äº‹ç‰©ç›¸è”ç³»ã€‚"]

# å°†åŸå§‹æ•°æ®è½¬æ¢æˆåˆ†å¥½è¯çš„å½¢å¼
text_list = []
for sent in data:
    text_list.append(cut_word(sent))
print(text_list)

# 1ã€å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
transfer = CountVectorizer()
# 2ã€è°ƒç”¨fit_transform
data = transfer.fit_transform(text_list)

print("æ–‡æœ¬ç‰¹å¾æŠ½å–çš„ç»“æœï¼š\n", data.toarray())
print("è¿”å›ç‰¹å¾åå­—ï¼š\n", transfer.get_feature_names())
```

è¾“å‡º

```python

['ä¸€ç§ è¿˜æ˜¯ ä¸€ç§ ä»Šå¤© å¾ˆ æ®‹é…· ï¼Œ æ˜å¤© æ›´ æ®‹é…· ï¼Œ åå¤© å¾ˆ ç¾å¥½ ï¼Œ ä½† ç»å¯¹ å¤§éƒ¨åˆ† æ˜¯ æ­» åœ¨ æ˜å¤© æ™šä¸Š ï¼Œ æ‰€ä»¥ æ¯ä¸ª äºº ä¸è¦ æ”¾å¼ƒ ä»Šå¤© ã€‚', 'æˆ‘ä»¬ çœ‹åˆ° çš„ ä» å¾ˆ è¿œ æ˜Ÿç³» æ¥ çš„ å…‰æ˜¯åœ¨ å‡ ç™¾ä¸‡å¹´ ä¹‹å‰ å‘å‡º çš„ ï¼Œ è¿™æ · å½“ æˆ‘ä»¬ çœ‹åˆ° å®‡å®™ æ—¶ ï¼Œ æˆ‘ä»¬ æ˜¯ åœ¨ çœ‹ å®ƒ çš„ è¿‡å» ã€‚', 'å¦‚æœ åªç”¨ ä¸€ç§ æ–¹å¼ äº†è§£ æŸæ · äº‹ç‰© ï¼Œ ä½  å°± ä¸ä¼š çœŸæ­£ äº†è§£ å®ƒ ã€‚ äº†è§£ äº‹ç‰© çœŸæ­£ å«ä¹‰ çš„ ç§˜å¯† å–å†³äº å¦‚ä½• å°† å…¶ ä¸ æˆ‘ä»¬ æ‰€ äº†è§£ çš„ äº‹ç‰© ç›¸ è”ç³» ã€‚']

æ–‡æœ¬ç‰¹å¾æŠ½å–çš„ç»“æœï¼š
 [[2 0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 1
  0]
 [0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 0
  1]
 [1 1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0
  0]]
è¿”å›ç‰¹å¾åå­—ï¼š
 ['ä¸€ç§', 'ä¸ä¼š', 'ä¸è¦', 'ä¹‹å‰', 'äº†è§£', 'äº‹ç‰©', 'ä»Šå¤©', 'å…‰æ˜¯åœ¨', 'å‡ ç™¾ä¸‡å¹´', 'å‘å‡º', 'å–å†³äº', 'åªç”¨', 'åå¤©', 'å«ä¹‰', 'å¤§éƒ¨åˆ†', 'å¦‚ä½•', 'å¦‚æœ', 'å®‡å®™', 'æˆ‘ä»¬', 'æ‰€ä»¥', 'æ”¾å¼ƒ', 'æ–¹å¼', 'æ˜å¤©', 'æ˜Ÿç³»', 'æ™šä¸Š', 'æŸæ ·', 'æ®‹é…·', 'æ¯ä¸ª', 'çœ‹åˆ°', 'çœŸæ­£', 'ç§˜å¯†', 'ç»å¯¹', 'ç¾å¥½', 'è”ç³»', 'è¿‡å»', 'è¿˜æ˜¯', 'è¿™æ ·']
```

### Tf-idf æ–‡æœ¬ç‰¹å¾æå– text.TfidfVectorizer

TF-IDF çš„ä¸»è¦æ€æƒ³æ˜¯ï¼šå¦‚æœæŸä¸ªè¯æˆ–çŸ­è¯­åœ¨ä¸€ç¯‡æ–‡ç« ä¸­å‡ºç°çš„æ¦‚ç‡é«˜ï¼Œå¹¶ä¸”åœ¨å…¶ä»–æ–‡ç« ä¸­å¾ˆå°‘å‡ºç°ï¼Œåˆ™è®¤ä¸ºæ­¤è¯æˆ–è€…çŸ­è¯­å…·æœ‰å¾ˆå¥½çš„ç±»åˆ«åŒºåˆ†èƒ½åŠ›ï¼Œé€‚åˆç”¨æ¥åˆ†ç±»ã€‚

TF-IDF ä½œç”¨ï¼šç”¨ä»¥è¯„ä¼°ä¸€å­—è¯å¯¹äºä¸€ä¸ªæ–‡ä»¶é›†æˆ–ä¸€ä¸ªè¯­æ–™åº“ä¸­çš„å…¶ä¸­ä¸€ä»½æ–‡ä»¶çš„é‡è¦ç¨‹åº¦ã€‚

#### å…¬å¼

- è¯é¢‘ï¼ˆterm frequencyï¼Œtfï¼‰æŒ‡çš„æ˜¯æŸä¸€ä¸ªç»™å®šçš„è¯è¯­åœ¨è¯¥æ–‡ä»¶ä¸­å‡ºç°çš„é¢‘ç‡
- é€†å‘æ–‡æ¡£é¢‘ç‡ï¼ˆinverse document frequencyï¼Œidfï¼‰æ˜¯ä¸€ä¸ªè¯è¯­æ™®éé‡è¦æ€§çš„åº¦é‡ã€‚ç”±æ€»æ–‡ä»¶æ•°ç›®é™¤ä»¥åŒ…å«è¯¥è¯è¯­ä¹‹æ–‡ä»¶çš„æ•°ç›®ï¼Œå†å°†å¾—åˆ°çš„å•†å–ä»¥ 10 ä¸ºåº•çš„å¯¹æ•°å¾—åˆ°

$$
tfidf_{i,j}=tf_{i,j}\times idf_i
$$

>  å‡å¦‚ä¸€ç¯‡æ–‡ä»¶çš„æ€»è¯è¯­æ•°æ˜¯ 100 ä¸ªï¼Œè€Œè¯è¯­"éå¸¸"å‡ºç°äº† 5 æ¬¡ï¼Œé‚£ä¹ˆ"éå¸¸"ä¸€è¯åœ¨è¯¥æ–‡ä»¶ä¸­çš„è¯é¢‘å°±æ˜¯5/100=0.05ã€‚è€Œè®¡ç®—æ–‡ä»¶é¢‘ç‡ï¼ˆIDFï¼‰çš„æ–¹æ³•æ˜¯ä»¥æ–‡ä»¶é›†çš„æ–‡ä»¶æ€»æ•°ï¼Œé™¤ä»¥å‡ºç°"éå¸¸"ä¸€è¯çš„æ–‡ä»¶æ•°ã€‚æ‰€ä»¥ï¼Œå¦‚æœ"éå¸¸"ä¸€è¯åœ¨1,000ä»½æ–‡ä»¶å‡ºç°è¿‡ï¼Œè€Œæ–‡ä»¶æ€»æ•°æ˜¯ 10,000,000 ä»½çš„è¯ï¼Œå…¶é€†å‘æ–‡ä»¶é¢‘ç‡å°±æ˜¯ lgï¼ˆ10,000,000 / 1,0000ï¼‰=3ã€‚æœ€å"éå¸¸"å¯¹äºè¿™ç¯‡æ–‡æ¡£çš„tf-idfçš„åˆ†æ•°ä¸º0.05 * 3=0.15

#### API

***TfidfVectorizer(stop_words=[])***

- TfidfVectorizer.fit_transform(X)
  -  Xï¼šæ–‡æœ¬æˆ–è€…åŒ…å«æ–‡æœ¬å­—ç¬¦ä¸²çš„å¯è¿­ä»£å¯¹è±¡
  - è¿”å›sparseçŸ©é˜µ
- TfidfVectorizer.inverse_transform(X)
  -  Xï¼šarrayæ•°ç»„æˆ–è€…sparseçŸ©é˜µ 
  - è¿”å›è½¬æ¢ä¹‹å‰æ•°æ®æ ¼
- TfidfVectorizer.get_feature_names() 
  - è¿”å›å€¼å•è¯åˆ—è¡¨

#### ä¾‹

```python
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba

def cut_word(text):
    # ç”¨ç»“å·´å¯¹ä¸­æ–‡å­—ç¬¦ä¸²è¿›è¡Œåˆ†è¯
    text = " ".join(list(jieba.cut(text)))
    return text

data = ["ä¸€ç§è¿˜æ˜¯ä¸€ç§ä»Šå¤©å¾ˆæ®‹é…·ï¼Œæ˜å¤©æ›´æ®‹é…·ï¼Œåå¤©å¾ˆç¾å¥½ï¼Œä½†ç»å¯¹å¤§éƒ¨åˆ†æ˜¯æ­»åœ¨æ˜å¤©æ™šä¸Šï¼Œæ‰€ä»¥æ¯ä¸ªäººä¸è¦æ”¾å¼ƒä»Šå¤©ã€‚",
        "æˆ‘ä»¬çœ‹åˆ°çš„ä»å¾ˆè¿œæ˜Ÿç³»æ¥çš„å…‰æ˜¯åœ¨å‡ ç™¾ä¸‡å¹´ä¹‹å‰å‘å‡ºçš„ï¼Œè¿™æ ·å½“æˆ‘ä»¬çœ‹åˆ°å®‡å®™æ—¶ï¼Œæˆ‘ä»¬æ˜¯åœ¨çœ‹å®ƒçš„è¿‡å»ã€‚",
        "å¦‚æœåªç”¨ä¸€ç§æ–¹å¼äº†è§£æŸæ ·äº‹ç‰©ï¼Œä½ å°±ä¸ä¼šçœŸæ­£äº†è§£å®ƒã€‚äº†è§£äº‹ç‰©çœŸæ­£å«ä¹‰çš„ç§˜å¯†å–å†³äºå¦‚ä½•å°†å…¶ä¸æˆ‘ä»¬æ‰€äº†è§£çš„äº‹ç‰©ç›¸è”ç³»ã€‚"]

# å°†åŸå§‹æ•°æ®è½¬æ¢æˆåˆ†å¥½è¯çš„å½¢å¼
text_list = []
for sent in data:
    text_list.append(cut_word(sent))
print(text_list)

# 1ã€å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
transfer = TfidfVectorizer(stop_words=['ä¸€ç§', 'ä¸ä¼š', 'ä¸è¦'])
# 2ã€è°ƒç”¨fit_transform
data = transfer.fit_transform(text_list)

print("æ–‡æœ¬ç‰¹å¾æŠ½å–çš„ç»“æœï¼š\n", data.toarray())
print("è¿”å›ç‰¹å¾åå­—ï¼š\n", transfer.get_feature_names())
```

è¾“å‡º

```python
['ä¸€ç§ è¿˜æ˜¯ ä¸€ç§ ä»Šå¤© å¾ˆ æ®‹é…· ï¼Œ æ˜å¤© æ›´ æ®‹é…· ï¼Œ åå¤© å¾ˆ ç¾å¥½ ï¼Œ ä½† ç»å¯¹ å¤§éƒ¨åˆ† æ˜¯ æ­» åœ¨ æ˜å¤© æ™šä¸Š ï¼Œ æ‰€ä»¥ æ¯ä¸ª äºº ä¸è¦ æ”¾å¼ƒ ä»Šå¤© ã€‚', 'æˆ‘ä»¬ çœ‹åˆ° çš„ ä» å¾ˆ è¿œ æ˜Ÿç³» æ¥ çš„ å…‰æ˜¯åœ¨ å‡ ç™¾ä¸‡å¹´ ä¹‹å‰ å‘å‡º çš„ ï¼Œ è¿™æ · å½“ æˆ‘ä»¬ çœ‹åˆ° å®‡å®™ æ—¶ ï¼Œ æˆ‘ä»¬ æ˜¯ åœ¨ çœ‹ å®ƒ çš„ è¿‡å» ã€‚', 'å¦‚æœ åªç”¨ ä¸€ç§ æ–¹å¼ äº†è§£ æŸæ · äº‹ç‰© ï¼Œ ä½  å°± ä¸ä¼š çœŸæ­£ äº†è§£ å®ƒ ã€‚ äº†è§£ äº‹ç‰© çœŸæ­£ å«ä¹‰ çš„ ç§˜å¯† å–å†³äº å¦‚ä½• å°† å…¶ ä¸ æˆ‘ä»¬ æ‰€ äº†è§£ çš„ äº‹ç‰© ç›¸ è”ç³» ã€‚']

æ–‡æœ¬ç‰¹å¾æŠ½å–çš„ç»“æœï¼š
 [[0.         0.         0.         0.43643578 0.         0.
  0.         0.         0.         0.21821789 0.         0.21821789
  0.         0.         0.         0.         0.21821789 0.21821789
  0.         0.43643578 0.         0.21821789 0.         0.43643578
  0.21821789 0.         0.         0.         0.21821789 0.21821789
  0.         0.         0.21821789 0.        ]
 [0.2410822  0.         0.         0.         0.2410822  0.2410822
  0.2410822  0.         0.         0.         0.         0.
  0.         0.         0.2410822  0.55004769 0.         0.
  0.         0.         0.2410822  0.         0.         0.
  0.         0.48216441 0.         0.         0.         0.
  0.         0.2410822  0.         0.2410822 ]
 [0.         0.644003   0.48300225 0.         0.         0.
  0.         0.16100075 0.16100075 0.         0.16100075 0.
  0.16100075 0.16100075 0.         0.12244522 0.         0.
  0.16100075 0.         0.         0.         0.16100075 0.
  0.         0.         0.3220015  0.16100075 0.         0.
  0.16100075 0.         0.         0.        ]]
è¿”å›ç‰¹å¾åå­—ï¼š
 ['ä¹‹å‰', 'äº†è§£', 'äº‹ç‰©', 'ä»Šå¤©', 'å…‰æ˜¯åœ¨', 'å‡ ç™¾ä¸‡å¹´', 'å‘å‡º', 'å–å†³äº', 'åªç”¨', 'åå¤©', 'å«ä¹‰', 'å¤§éƒ¨åˆ†', 'å¦‚ä½•', 'å¦‚æœ', 'å®‡å®™', 'æˆ‘ä»¬', 'æ‰€ä»¥', 'æ”¾å¼ƒ', 'æ–¹å¼', 'æ˜å¤©', 'æ˜Ÿç³»', 'æ™šä¸Š', 'æŸæ ·', 'æ®‹é…·', 'æ¯ä¸ª', 'çœ‹åˆ°', 'çœŸæ­£', 'ç§˜å¯†', 'ç»å¯¹', 'ç¾å¥½', 'è”ç³»', 'è¿‡å»', 'è¿˜æ˜¯', 'è¿™æ ·']
```

## ç‰¹å¾é¢„å¤„ç†

ï¼ˆfeature preprocessingï¼‰

é€šè¿‡ä¸€äº›è½¬æ¢å‡½æ•°å°†ç‰¹å¾æ•°æ®è½¬æ¢æˆæ›´åŠ é€‚åˆç®—æ³•æ¨¡å‹çš„ç‰¹å¾æ•°æ®è¿‡ç¨‹

æ•°æ®çš„æ— é‡çº²å¤„ç†ï¼š**ä½¿ä¸åŒè§„æ ¼çš„æ•°æ®è½¬æ¢åˆ°åŒä¸€è§„æ ¼**

- å½’ä¸€åŒ–
- æ ‡å‡†åŒ–

ç‰¹å¾çš„å•ä½æˆ–è€…å¤§å°ç›¸å·®è¾ƒå¤§ï¼Œæˆ–è€…æŸç‰¹å¾çš„æ–¹å·®ç›¸æ¯”å…¶ä»–çš„ç‰¹å¾è¦å¤§å‡ºå‡ ä¸ªæ•°é‡çº§**ï¼Œ**å®¹æ˜“å½±å“ï¼ˆæ”¯é…ï¼‰ç›®æ ‡ç»“æœï¼Œä½¿å¾—ä¸€äº›ç®—æ³•æ— æ³•å­¦ä¹ åˆ°å…¶å®ƒçš„ç‰¹å¾ï¼Œæ‰€ä»¥è¦è¿›è¡Œå½’ä¸€åŒ–/æ ‡å‡†åŒ–ã€‚

```python
import sklearn.preprocessing
```

### å½’ä¸€åŒ–

é€šè¿‡å¯¹åŸå§‹æ•°æ®è¿›è¡Œå˜æ¢æŠŠæ•°æ®æ˜ å°„åˆ°ï¼ˆé»˜è®¤ä¸ºï¼‰[0,1] ä¹‹é—´

#### å…¬å¼

$$
X'=\cfrac{x-min}{max-min}\\
X''=X'*(mx-mi)+mi
$$

>  ä½œç”¨äºæ¯ä¸€åˆ—ï¼Œmax ä¸ºä¸€åˆ—çš„æœ€å¤§å€¼ï¼Œmin ä¸ºä¸€åˆ—çš„æœ€å°å€¼ï¼ŒXâ€™â€™ä¸ºæœ€ç»ˆç»“æœï¼Œmxï¼Œmiåˆ†åˆ«ä¸ºæŒ‡å®šåŒºé—´å€¼ï¼Œé»˜è®¤mxä¸º1ï¼Œmiä¸º0

#### API

***MinMaxScaler (feature_range=(0,1)â€¦ )***

- *MinMaxScalar.fit_transform(X)*
  - Xï¼šnumpy arrayæ ¼å¼çš„æ•°æ® [n_samples,n_features]
  - è¿”å›å€¼ï¼šè½¬æ¢åçš„å½¢çŠ¶ç›¸åŒçš„array

#### ä¾‹

ä»¥ä¸‹ä¸ºæ•°æ®å®ä¾‹

<img src="https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20210118192214857.png" alt="image-20210118192214857" style="zoom:80%;" />

- å®ä¾‹åŒ– MinMaxScalar

- é€šè¿‡ fit_transform è½¬æ¢

```python
from sklearn.preprocessing import MinMaxScaler
import pandas as pd

path = "../Data/Dating.txt"
data = pd.read_csv(path)

# 1ã€å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
transfer = MinMaxScaler(feature_range=(0, 1)) # é»˜è®¤ MIN=0, MAX=1
# 2ã€è°ƒç”¨fit_transform
data = transfer.fit_transform(data[['milage','Liters','Consumtime']]) # éœ€è¦ä¼ numpy arrayæ ¼å¼, è¿”å›array

print("æœ€å°å€¼æœ€å¤§å€¼å½’ä¸€åŒ–å¤„ç†çš„ç»“æœï¼š\n", data)
```

è¾“å‡º

```python
æœ€å°å€¼æœ€å¤§å€¼å½’ä¸€åŒ–å¤„ç†çš„ç»“æœï¼š
 [[0.43582641 0.58819286 0.53237967]
 [0.         0.48794044 1.        ]
 [0.19067405 0.         0.43571351]
 [1.         1.         0.19139157]
 [0.3933518  0.01947089 0.        ]]
```

æ³¨æ„æœ€å¤§å€¼æœ€å°å€¼æ˜¯å˜åŒ–çš„ï¼Œå¦å¤–ï¼Œæœ€å¤§å€¼ä¸æœ€å°å€¼éå¸¸å®¹æ˜“å—å¼‚å¸¸ç‚¹å½±å“ï¼Œæ‰€ä»¥è¿™ç§æ–¹æ³•é²æ£’æ€§è¾ƒå·®ï¼Œåªé€‚åˆä¼ ç»Ÿç²¾ç¡®å°æ•°æ®åœºæ™¯

### æ ‡å‡†åŒ–

é€šè¿‡å¯¹åŸå§‹æ•°æ®è¿›è¡Œå˜æ¢æŠŠæ•°æ®å˜æ¢åˆ°å‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1çš„èŒƒå›´å†…

ä¼˜åŠ¿

- å¯¹äºå½’ä¸€åŒ–æ¥è¯´ï¼šå¦‚æœå‡ºç°å¼‚å¸¸ç‚¹ï¼Œå½±å“äº†æœ€å¤§å€¼å’Œæœ€å°å€¼ï¼Œé‚£ä¹ˆç»“æœæ˜¾ç„¶ä¼šå‘ç”Ÿæ”¹å˜
- å¯¹äºæ ‡å‡†åŒ–æ¥è¯´ï¼šå¦‚æœå‡ºç°å¼‚å¸¸ç‚¹ï¼Œç”±äºå…·æœ‰ä¸€å®šæ•°æ®é‡ï¼Œå°‘é‡çš„å¼‚å¸¸ç‚¹å¯¹äºå¹³å‡å€¼çš„å½±å“å¹¶ä¸å¤§ï¼Œä»è€Œæ–¹å·®æ”¹å˜è¾ƒå°

#### å…¬å¼

$$
X'=\cfrac{x-mean}{\sigma}
$$

> ä½œç”¨äºæ¯ä¸€åˆ—ï¼Œmean ä¸ºå¹³å‡å€¼ï¼ŒÏƒ ä¸ºæ ‡å‡†å·®

#### API

***StandardScaler( )***

- *StandardScaler.fit_transform(X)*
  - Xï¼šnumpy array æ ¼å¼çš„æ•°æ®[n_samples, n_features]
  - è¿”å›å€¼ï¼šè½¬æ¢åçš„å½¢çŠ¶ç›¸åŒçš„array
- *StandardScaler.mean_*
  - è¿”å›å€¼ï¼šæ¯ä¸€åˆ—ç‰¹å¾çš„å¹³å‡å€¼
- *StandardScaler.var_*
  - è¿”å›å€¼ï¼šæ¯ä¸€åˆ—ç‰¹å¾çš„æ–¹å·®

#### ä¾‹

åŒæ ·å¯¹ 2.4.1 çš„æ•°æ®è¿›è¡Œå¤„ç†

- å®ä¾‹åŒ– StandardScaler

- é€šè¿‡fit_transformè½¬æ¢

```python
import pandas as pd
from sklearn.preprocessing import StandardScaler

data = pd.read_csv(path)

# 1ã€å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
transfer = StandardScaler() # å€¼éƒ½åœ¨0é™„è¿‘,æ‰€ä»¥æœ‰è´Ÿæ•°æ˜¯æ­£å¸¸çš„
# 2ã€è°ƒç”¨fit_transform
data = transfer.fit_transform(data[['milage','Liters','Consumtime']]) 

print("æ ‡å‡†åŒ–çš„ç»“æœ:\n", data)
print("æ¯ä¸€åˆ—ç‰¹å¾çš„å¹³å‡å€¼ï¼š\n", transfer.mean_)
print("æ¯ä¸€åˆ—ç‰¹å¾çš„æ–¹å·®ï¼š\n", transfer.var_)
```

è¾“å‡º

```python
æ ‡å‡†åŒ–çš„ç»“æœ:
[[ 0.0947602   0.44990013  0.29573441]
 [-1.20166916  0.18312874  1.67200507]
 [-0.63448132 -1.11527928  0.01123265]
 [ 1.77297701  1.54571769 -0.70784025]
 [-0.03158673 -1.06346729 -1.27113187]]
æ¯ä¸€åˆ—ç‰¹å¾çš„å¹³å‡å€¼ï¼š
 [3.8988000e+04 6.3478996e+00 7.9924800e-01]
æ¯ä¸€åˆ—ç‰¹å¾çš„æ–¹å·®ï¼š
 [4.15683072e+08 1.93505309e+01 2.73652475e-01]
```

## ç‰¹å¾é™ç»´

ï¼ˆFeature Dimension Reduceï¼‰

é™ç»´æ˜¯æŒ‡åœ¨æŸäº›é™å®šæ¡ä»¶ä¸‹ï¼Œé™ä½éšæœºå˜é‡ï¼ˆç‰¹å¾ï¼‰ä¸ªæ•°ï¼Œå¾—åˆ°ä¸€ç»„â€œä¸ç›¸å…³â€ä¸»å˜é‡çš„è¿‡ç¨‹

ä¸¤ç§æ–¹å¼

- ç‰¹å¾é€‰æ‹©
- ä¸»æˆåˆ†åˆ†æï¼ˆå¯ä»¥ç†è§£ä¸€ç§ç‰¹å¾æå–çš„æ–¹å¼ï¼‰

### ç‰¹å¾é€‰æ‹©

```python
import sklearn.feature_selection
```

æ•°æ®ä¸­åŒ…å«å†—ä½™æˆ–æ— å…³å˜é‡ï¼ˆæˆ–ç§°ç‰¹å¾ã€å±æ€§ã€æŒ‡æ ‡ç­‰ï¼‰ï¼Œæ—¨åœ¨ä»åŸæœ‰ç‰¹å¾ä¸­æ‰¾å‡ºä¸»è¦ç‰¹å¾

æ–¹æ³•

- è¿‡æ»¤å¼ï¼ˆFilterï¼‰ï¼šä¸»è¦æ¢ç©¶ç‰¹å¾æœ¬èº«ç‰¹ç‚¹ã€ç‰¹å¾ä¸ç‰¹å¾å’Œç›®æ ‡å€¼ä¹‹é—´å…³è”
  - æ–¹å·®é€‰æ‹©æ³•ï¼šä½æ–¹å·®ç‰¹å¾è¿‡æ»¤
  - ç›¸å…³ç³»æ•°
- åµŒå…¥å¼ï¼ˆEmbeddedï¼‰ï¼šç®—æ³•è‡ªåŠ¨é€‰æ‹©ç‰¹å¾ï¼ˆç‰¹å¾ä¸ç›®æ ‡å€¼ä¹‹é—´çš„å…³è”ï¼‰
  - å†³ç­–æ ‘ï¼šä¿¡æ¯ç†µã€ä¿¡æ¯å¢ç›Š
  - æ­£åˆ™åŒ–ï¼šL1ã€L2
  - æ·±åº¦å­¦ä¹ ï¼šå·ç§¯ç­‰

#### ä½æ–¹å·®ç‰¹å¾è¿‡æ»¤

åˆ é™¤ä½æ–¹å·®çš„ä¸€äº›ç‰¹å¾

##### API

***VarianceThreshold(threshold = 0.0)***

- *Variance.fit_transform(X)*
  - Xï¼šnumpy array æ ¼å¼çš„æ•°æ® [n_samples, n_features]
  - è¿”å›å€¼ï¼šè®­ç»ƒé›†å·®å¼‚ä½äº threshold çš„ç‰¹å¾å°†è¢«åˆ é™¤ã€‚é»˜è®¤å€¼æ˜¯ä¿ç•™æ‰€æœ‰éé›¶æ–¹å·®ç‰¹å¾ï¼Œå³åˆ é™¤æ‰€æœ‰æ ·æœ¬ä¸­å…·æœ‰ç›¸åŒå€¼çš„ç‰¹å¾ã€‚

##### ä¾‹

å¤„ç†ä»¥ä¸‹ä¾‹å­

<img src="https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20210118192214857.png" alt="image-20210118192214857" style="zoom:80%;" />

åˆ†æ

- åˆå§‹åŒ– VarianceThreshold ï¼ŒæŒ‡å®šé˜€å€¼æ–¹å·®

- è°ƒç”¨ fit_transform

```python
from sklearn.feature_selection import VarianceThreshold
import pandas as pd

path="../Data/Dating.txt"
data = pd.read_csv(path)

# 1ã€å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
transfer = VarianceThreshold(threshold=1)
# 2ã€è°ƒç”¨fit_transform
data = transfer.fit_transform(data.iloc[:, 0:-1])

print("åˆ é™¤ä½æ–¹å·®ç‰¹å¾çš„ç»“æœï¼š\n", data)
print("å½¢çŠ¶ï¼š\n", data.shape)
```

è¾“å‡º

```python
åˆ é™¤ä½æ–¹å·®ç‰¹å¾çš„ç»“æœï¼š
 [[4.0920000e+04 8.3269760e+00]
 [1.4488000e+04 7.1534690e+00]
 [2.6052000e+04 1.4418710e+00]
 [7.5136000e+04 1.3147394e+01]
 [3.8344000e+04 1.6697880e+00]]
å½¢çŠ¶ï¼š
 (5, 2)
```

#### ç›¸å…³ç³»æ•°

å»é™¤ç›¸å…³ç‰¹å¾ï¼ˆcorrelated featureï¼‰çš„å½±å“

ä½¿ç”¨ Scipy å®ç°

```python
from scipy.stats import pearsonr
```

##### åŸç†

çš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆPearson Correlation Coefficientï¼‰ï¼šåæ˜ å˜é‡ä¹‹é—´ç›¸å…³å…³ç³»å¯†åˆ‡ç¨‹åº¦çš„ç»Ÿè®¡æŒ‡æ ‡
$$
r=\cfrac{n\sum xy=\sum x\sum y}{\sqrt{n\sum x^2 -(\sum x)^2}\sqrt{n\sum y^2-(\sum y)^2}}
$$
ç›¸å…³ç³»æ•°çš„å€¼ä»‹äºâ€“1ä¸+1ä¹‹é—´ï¼Œå³â€“1â‰¤ r â‰¤+1ã€‚å…¶æ€§è´¨å¦‚ä¸‹ï¼š

- å½“r > 0æ—¶ï¼Œè¡¨ç¤ºä¸¤å˜é‡æ­£ç›¸å…³ï¼Œr < 0æ—¶ï¼Œä¸¤å˜é‡ä¸ºè´Ÿç›¸å…³
- å½“|r|=1æ—¶ï¼Œè¡¨ç¤ºä¸¤å˜é‡ä¸ºå®Œå…¨ç›¸å…³ï¼Œå½“ r=0 æ—¶ï¼Œè¡¨ç¤ºä¸¤å˜é‡é—´æ— ç›¸å…³å…³ç³»
- å½“0<|r|<1æ—¶ï¼Œè¡¨ç¤ºä¸¤å˜é‡å­˜åœ¨ä¸€å®šç¨‹åº¦çš„ç›¸å…³ã€‚ä¸”|r|è¶Šæ¥è¿‘1ï¼Œä¸¤å˜é‡é—´çº¿æ€§å…³ç³»è¶Šå¯†åˆ‡ï¼›|r|è¶Šæ¥è¿‘äº0ï¼Œè¡¨ç¤ºä¸¤å˜é‡çš„çº¿æ€§ç›¸å…³è¶Šå¼±
- ä¸€èˆ¬å¯æŒ‰ä¸‰çº§åˆ’åˆ†ï¼š|r|<0.4ä¸ºä½åº¦ç›¸å…³ï¼›0.4â‰¤|r|<0.7ä¸ºæ˜¾è‘—æ€§ç›¸å…³ï¼›0.7â‰¤|r|<1ä¸ºé«˜åº¦çº¿æ€§ç›¸å…³

##### API

***pearsonr(X, Y)***

- Xï¼šnumpy array æ ¼å¼çš„æ•°æ®
- Yï¼šnumpy array æ ¼å¼çš„æ•°æ®
- è¿”å›å€¼
  - rï¼šç›¸å…³ç³»æ•° [-1ï¼Œ1] ä¹‹é—´
  - p-valueï¼špå€¼ï¼ˆpå€¼è¶Šå°ï¼Œè¡¨ç¤ºç›¸å…³ç³»æ•°è¶Šæ˜¾è‘—ï¼Œä¸€èˆ¬på€¼åœ¨500ä¸ªæ ·æœ¬ä»¥ä¸Šæ—¶æœ‰è¾ƒé«˜çš„å¯é æ€§ï¼‰



å¦‚æœç›¸å…³æ€§é«˜å¯ç”¨ä»¥ä¸‹æ–¹æ³•:

1. é€‰å–å…¶ä¸­ä¸€ä¸ªç‰¹å¾

2. ä¸¤ä¸ªç‰¹å¾åŠ æƒæ±‚å’Œ

3. ä¸»æˆåˆ†åˆ†æï¼ˆé«˜ç»´æ•°æ®å˜ä½ç»´ï¼Œèˆå¼ƒåŸç”±æ•°æ®ï¼Œåˆ›é€ æ–°æ•°æ®ï¼Œå¦‚ï¼šå‹ç¼©æ•°æ®ç»´æ•°ï¼Œé™ä½åŸæ•°æ®å¤æ‚åº¦ï¼ŒæŸå¤±å°‘äº†ä¿¡æ¯ï¼‰

##### ä¾‹

ä¸¤ä¸¤ç‰¹å¾ä¹‹é—´è¿›è¡Œç›¸å…³æ€§è®¡ç®—

```python
from scipy.stats import pearsonr
import pandas as pd

path="../Data/Dating.txt"
data = pd.read_csv(path)

# çš®å°”é€Šç›¸å…³ç³»æ•°èŒƒå›´[-1,1], å¦‚æœå¤§äº0å°±æ˜¯æ­£ç›¸å…³(è¶Šæ¥è¿‘1å°±è¶Šç›¸å…³), åä¹‹äº¦ç„¶
r = pearsonr(data["milage"], data["Liters"])
print("milageå’ŒLitersçš„ç›¸å…³ç³»æ•°ä¸º:\n", r)

r = pearsonr(data["milage"], data["Consumtime"])
print("milageå’ŒLitersçš„ç›¸å…³ç³»æ•°ä¸º:\n", r)
```

è¾“å‡º

```python
milageå’ŒLitersçš„ç›¸å…³ç³»æ•°ä¸º:
(0.660861943290103, 0.2246299034335304)
milageå’ŒLitersçš„ç›¸å…³ç³»æ•°ä¸º:
(-0.6406267138718624, 0.2441916485876286)
```

### ä¸»æˆåˆ†åˆ†æ

ï¼ˆPCAï¼‰å°†æ•°æ®åˆ†è§£ä¸ºè¾ƒä½ç»´æ•°ç©ºé—´

```python
from sklearn.decomposition import PCA
```

#### æ¦‚å¿µ

- å®šä¹‰ï¼šé«˜ç»´æ•°æ®è½¬åŒ–ä¸ºä½ç»´æ•°æ®çš„è¿‡ç¨‹ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­å¯èƒ½ä¼šèˆå¼ƒåŸæœ‰æ•°æ®ã€åˆ›é€ æ–°çš„å˜é‡
- ä½œç”¨ï¼šæ˜¯æ•°æ®ç»´æ•°å‹ç¼©ï¼Œå°½å¯èƒ½é™ä½åŸæ•°æ®çš„ç»´æ•°ï¼ˆå¤æ‚åº¦ï¼‰ï¼ŒæŸå¤±å°‘é‡ä¿¡æ¯
- åº”ç”¨ï¼šå›å½’åˆ†ææˆ–è€…èšç±»åˆ†æå½“ä¸­

#### API

***PCA(n_components=None)***

- å‚æ•°ï¼š*n_components*
  - å°æ•°ï¼šè¡¨ç¤ºä¿ç•™ç™¾åˆ†ä¹‹å¤šå°‘çš„ä¿¡æ¯
  - æ•´æ•°ï¼šå‡å°‘åˆ°å¤šå°‘ç‰¹å¾

- *PCA.fit_transform(X)*
  - Xï¼šnumpy array æ ¼å¼çš„æ•°æ® [n_samples,n_features]
  - è¿”å›å€¼ï¼šè½¬æ¢åæŒ‡å®šç»´åº¦çš„ array

#### ä¾‹

```python
from sklearn.decomposition import PCA

data = [[2, 8, 4, 5], [3, 8, 5, 5], [10, 5, 1, 0]]  # 3*4çŸ©é˜µ åŒ…å«å››ä¸ªç‰¹å¾

transfer = PCA(n_components=3) # ä¸ºæ•´æ•°å°±æ˜¯è½¬ä¸ºå¤šå°‘ä¸ªç‰¹å¾  ä¿ç•™çš„è‡³å°‘éƒ½æ¯”åŸç‰¹å¾å€¼å°‘ä¸€ä¸ª
data_new = transfer.fit_transform(data)
print("(ä¸»æˆåˆ†åˆ†æ)PCAé™ç»´:\n", data_new)
```

è¾“å‡º

```python
(ä¸»æˆåˆ†åˆ†æ)PCAé™ç»´:
[[-3.57495904e+00 -6.64748145e-01  1.07947657e-16]
 [-3.17447323e+00  6.91574499e-01  1.07947657e-16]
 [ 6.74943227e+00 -2.68263539e-02  1.07947657e-16]]
```

è‹¥ n_components è®¾ä¸º0.95

```python
(ä¸»æˆåˆ†åˆ†æ)PCAé™ç»´:
[[-3.57495904]
 [-3.17447323]
 [ 6.74943227]]
```

# è½¬æ¢å™¨å’Œä¼°è®¡å™¨

## è½¬æ¢å™¨

ï¼ˆtransformerï¼‰

ç‰¹å¾å·¥ç¨‹çš„æ¥å£ç§°ä¹‹ä¸ºè½¬æ¢å™¨ï¼›è½¬æ¢å™¨æ˜¯ç‰¹å¾å·¥ç¨‹çš„çˆ¶ç±»

è°ƒç”¨æ­¥éª¤

1. å®ä¾‹åŒ– (å®ä¾‹åŒ–çš„æ˜¯ä¸€ä¸ªè½¬æ¢å™¨ç±»(Transformer))

2. è°ƒç”¨fit_transform(å¯¹äºæ–‡æ¡£å»ºç«‹åˆ†ç±»è¯é¢‘çŸ©é˜µï¼Œä¸èƒ½åŒæ—¶è°ƒç”¨)

è½¬æ¢å™¨è°ƒç”¨å½¢å¼

- fit_transform()
- fit()
  - æŒ‰å…¬å¼è®¡ç®—
- transform()
  - è¿›è¡Œæœ€ç»ˆçš„è½¬æ¢

## ä¼°è®¡å™¨

ï¼ˆestimatorï¼‰

ä¼°è®¡å™¨å®ç°äº†ç®—æ³•çš„APIï¼Œä¼°è®¡å™¨æ˜¯ç®—æ³•çš„çˆ¶ç±»

- ç”¨äºåˆ†ç±»çš„ä¼°è®¡å™¨ï¼š
  - sklearn.neighbors k-è¿‘é‚»ç®—æ³•
  - sklearn.naive_bayes è´å¶æ–¯
  - sklearn.linear_model.LogisticRegression é€»è¾‘å›å½’
  - sklearn.tree å†³ç­–æ ‘ä¸éšæœºæ£®æ—
- ç”¨äºå›å½’çš„ä¼°è®¡å™¨ï¼š
  - sklearn.linear_model.LinearRegression çº¿æ€§å›å½’
  - sklearn.linear_model.Ridge å²­å›å½’
- ç”¨äºæ— ç›‘ç£å­¦ä¹ çš„ä¼°è®¡å™¨
  - sklearn.cluster.KMeans èšç±»

è°ƒç”¨æ­¥éª¤

1. å®ä¾‹åŒ–ä¼°è®¡å™¨ç±»estimator

2. è¿›è¡Œè®­ç»ƒï¼Œä¸€æ—¦è°ƒç”¨å®Œæ¯•ï¼Œæ„å‘³ç€æ¨¡å‹ç”Ÿæˆ

   - *estimator.fit(x_train, y_train)*

3. æ¨¡å‹è¯„ä¼°

   - ç›´æ¥æ¯”å¯¹çœŸå®å€¼å’Œé¢„æµ‹å€¼

     *y_predict = estimator.predict(x_test)*

     *y_test == y_predict*

   - è®¡ç®—å‡†ç¡®ç‡

     *accuracy = estimator.score(x_test, y_test)*

<img src="https://trou.oss-cn-shanghai.aliyuncs.com/img/ä¼°è®¡å™¨å·¥ä½œæµç¨‹.png" alt="ä¼°è®¡å™¨å·¥ä½œæµç¨‹" style="zoom:67%;" />



## æ¨¡å‹é€‰æ‹©ä¸è°ƒä¼˜

```python
from sklearn.model_selection import GridSearchCV
```

### æ¦‚å¿µ

#### äº¤å‰éªŒè¯

ï¼ˆcross validationï¼‰

å°†æ‹¿åˆ°çš„è®­ç»ƒæ•°æ®ï¼Œåˆ†ä¸ºè®­ç»ƒå’ŒéªŒè¯é›†

#### è¶…å‚æ•°æœç´¢-ç½‘æ ¼æœç´¢

ï¼ˆGrid Searchï¼‰

é€šå¸¸æƒ…å†µä¸‹ï¼Œæœ‰å¾ˆå¤šå‚æ•°æ˜¯éœ€è¦æ‰‹åŠ¨æŒ‡å®šçš„ï¼ˆå¦‚k-è¿‘é‚»ç®—æ³•ä¸­çš„Kå€¼ï¼‰ï¼Œè¿™ç§å«è¶…å‚æ•°ã€‚ä½†æ˜¯æ‰‹åŠ¨è¿‡ç¨‹ç¹æ‚ï¼Œæ‰€ä»¥éœ€è¦å¯¹æ¨¡å‹é¢„è®¾å‡ ç§è¶…å‚æ•°ç»„åˆã€‚æ¯ç»„è¶…å‚æ•°éƒ½é‡‡ç”¨äº¤å‰éªŒè¯æ¥è¿›è¡Œè¯„ä¼°ã€‚æœ€åé€‰å‡ºæœ€ä¼˜å‚æ•°ç»„åˆå»ºç«‹æ¨¡å‹ã€‚

### API

***GridSearchCV(estimator, param_grid=None,cv=None)***

å¯¹ä¼°è®¡å™¨çš„æŒ‡å®šå‚æ•°å€¼è¿›è¡Œè¯¦å°½æœç´¢
- å‚æ•°
  - *estimator*ï¼šä¼°è®¡å™¨å¯¹è±¡
  - *param_grid*ï¼šä¼°è®¡å™¨å‚æ•° ` (dict){â€œn_neighborsâ€:[1,3,5]}`
  - *cv*ï¼šæŒ‡å®šå‡ æŠ˜äº¤å‰éªŒè¯
- æ–¹æ³•
  - *fit*ï¼šè¾“å…¥è®­ç»ƒæ•°æ®
  - *score*ï¼šå‡†ç¡®ç‡
- ç»“æœåˆ†æï¼š
  - *best_params_*ï¼šåœ¨äº¤å‰éªŒè¯ä¸­éªŒè¯çš„æœ€å¥½è¶…å‚æ•°
  - *best_score_*ï¼šåœ¨äº¤å‰éªŒè¯ä¸­éªŒè¯çš„æœ€å¥½ç»“æœ
  - *best_estimator_*ï¼šæœ€å¥½çš„å‚æ•°æ¨¡å‹
  - *cv_results_*ï¼šæ¯æ¬¡äº¤å‰éªŒè¯åçš„éªŒè¯é›†å‡†ç¡®ç‡ç»“æœå’Œè®­ç»ƒé›†å‡†ç¡®ç‡ç»“æœ

## åˆ†ç±»çš„è¯„ä¼°æ–¹æ³•

### åˆ†ç±»è¯„ä¼°æŠ¥å‘ŠAPI

```python
from sklearn.metrics import classification_report
```

***classification_report(y_true, y_pred, labels=[], target_names=None )***

- *y_true*ï¼šçœŸå®ç›®æ ‡å€¼
- *y_pred*ï¼šä¼°è®¡å™¨é¢„æµ‹ç›®æ ‡å€¼
- *labels*ï¼šæŒ‡å®šç±»åˆ«å¯¹åº”çš„æ•°å­—
- *target_names*ï¼šç›®æ ‡ç±»åˆ«åç§°
- *return*ï¼šæ¯ä¸ªç±»åˆ«ç²¾ç¡®ç‡ä¸å¬å›ç‡

### ROC æ›²çº¿ä¸ AUC æŒ‡æ ‡

```python
from sklearn.metrics import roc_auc_score
```

- AUC åªèƒ½ç”¨æ¥è¯„ä»·äºŒåˆ†ç±»
- AUC éå¸¸é€‚åˆè¯„ä»·æ ·æœ¬ä¸å¹³è¡¡ä¸­çš„åˆ†ç±»å™¨æ€§èƒ½

#### TPR ä¸ FPR

- TPR = TP / (TP + FN)
  - æ‰€æœ‰çœŸå®ç±»åˆ«ä¸º1çš„æ ·æœ¬ä¸­ï¼Œé¢„æµ‹ç±»åˆ«ä¸º1çš„æ¯”ä¾‹
  - å³ä¸ºå¬å›ç‡ï¼ˆæŸ¥å…¨ç‡ï¼‰
- FPR = FP / (FP + FN)
  - æ‰€æœ‰çœŸå®ç±»åˆ«ä¸º0çš„æ ·æœ¬ä¸­ï¼Œé¢„æµ‹ç±»åˆ«ä¸º1çš„æ¯”ä¾‹

#### ROC æ›²çº¿

- ROC æ›²çº¿çš„æ¨ªè½´å°±æ˜¯ FPRateï¼Œçºµè½´å°±æ˜¯ TPRateï¼Œå½“äºŒè€…ç›¸ç­‰æ—¶ï¼Œè¡¨ç¤ºçš„æ„ä¹‰åˆ™æ˜¯ï¼šå¯¹äºä¸è®ºçœŸå®ç±»åˆ«æ˜¯ 1 è¿˜æ˜¯ 0 çš„æ ·æœ¬ï¼Œåˆ†ç±»å™¨é¢„æµ‹ä¸º 1 çš„æ¦‚ç‡æ˜¯ç›¸ç­‰çš„ï¼Œæ­¤æ—¶ AUC ä¸º 0.5

<img src="https://trou.oss-cn-shanghai.aliyuncs.com/img/ROC.png" alt="ROC" style="zoom:67%;" />

#### AUC æŒ‡æ ‡

- AUC çš„æ¦‚ç‡æ„ä¹‰æ˜¯éšæœºå–ä¸€å¯¹æ­£è´Ÿæ ·æœ¬ï¼Œæ­£æ ·æœ¬å¾—åˆ†å¤§äºè´Ÿæ ·æœ¬çš„æ¦‚ç‡
- AUC çš„æœ€å°å€¼ä¸º 0.5ï¼Œæœ€å¤§å€¼ä¸º 1ï¼Œå–å€¼è¶Šé«˜è¶Šå¥½
- AUC=1ï¼Œå®Œç¾åˆ†ç±»å™¨ï¼Œé‡‡ç”¨è¿™ä¸ªé¢„æµ‹æ¨¡å‹æ—¶ï¼Œä¸ç®¡è®¾å®šä»€ä¹ˆé˜ˆå€¼éƒ½èƒ½å¾—å‡ºå®Œç¾é¢„æµ‹ã€‚ç»å¤§å¤šæ•°é¢„æµ‹çš„åœºåˆï¼Œä¸å­˜åœ¨å®Œç¾åˆ†ç±»å™¨ã€‚
- 0.5<AUC<1ï¼Œä¼˜äºéšæœºçŒœæµ‹ã€‚è¿™ä¸ªåˆ†ç±»å™¨ï¼ˆæ¨¡å‹ï¼‰å¦¥å–„è®¾å®šé˜ˆå€¼çš„è¯ï¼Œèƒ½æœ‰é¢„æµ‹ä»·å€¼ã€‚

> **æœ€ç»ˆAUCçš„èŒƒå›´åœ¨ [0.5, 1] ä¹‹é—´ï¼Œå¹¶ä¸”è¶Šæ¥è¿‘1è¶Šå¥½**

#### API

***roc_auc_score(y_true, y_score)***

- è®¡ç®— ROC æ›²çº¿é¢ç§¯ï¼Œå³ AUC å€¼
- *y_true*ï¼šæ¯ä¸ªæ ·æœ¬çš„çœŸå®ç±»åˆ«ï¼Œå¿…é¡»ä¸º 0 (åä¾‹), 1 (æ­£ä¾‹)æ ‡è®°
- *y_score*ï¼šæ¯ä¸ªæ ·æœ¬é¢„æµ‹çš„æ¦‚ç‡å€¼

## æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

```python
import joblib
```

- ä¿å­˜ï¼šjoblib.dump( estimator, 'XXX.pkl' )
- åŠ è½½ï¼šestimator = joblib.load( 'XXX.pkl' )

ä¾‹

```python
import joblib

def store_model(estimator, name):
    joblib.dump(estimator, "../../models/"+name)
    return "SUCCESS"


def load_model(name):
    model = joblib.load("../../models/"+name)
    return model
```

# åˆ†ç±»

## KNN ç®—æ³•

> æ ¹æ®é‚»å±…ï¼Œåˆ¤æ–­ç±»åˆ«

```python
from sklearn.neighbors import KNeighborsClassifier
```

ï¼ˆK Nearest Neighborï¼‰å³K - è¿‘é‚»ç®—æ³•

å¦‚æœä¸€ä¸ªæ ·æœ¬åœ¨ç‰¹å¾ç©ºé—´ä¸­çš„ **k ä¸ªæœ€ç›¸ä¼¼(å³ç‰¹å¾ç©ºé—´ä¸­æœ€é‚»è¿‘)çš„æ ·æœ¬ä¸­çš„å¤§å¤šæ•°å±äºæŸä¸€ä¸ªç±»åˆ«**ï¼Œåˆ™è¯¥æ ·æœ¬ä¹Ÿå±äºè¿™ä¸ªç±»åˆ«ã€‚

- ä¼˜ç‚¹ï¼š
  - ç®€å•ï¼Œæ˜“äºç†è§£ï¼Œæ˜“äºå®ç°ï¼Œæ— éœ€è®­ç»ƒ
- ç¼ºç‚¹ï¼š
  - æ‡’æƒ°ç®—æ³•ï¼Œå¯¹æµ‹è¯•æ ·æœ¬åˆ†ç±»æ—¶çš„è®¡ç®—é‡å¤§ï¼Œå†…å­˜å¼€é”€å¤§
  - å¿…é¡»æŒ‡å®š K å€¼ï¼ŒK å€¼é€‰æ‹©ä¸å½“åˆ™åˆ†ç±»ç²¾åº¦ä¸èƒ½ä¿è¯
- ä½¿ç”¨åœºæ™¯ï¼šå°æ•°æ®åœºæ™¯ï¼Œå‡ åƒï½å‡ ä¸‡æ ·æœ¬ï¼Œå…·ä½“åœºæ™¯å…·ä½“ä¸šåŠ¡å»æµ‹è¯•

### API

***KNeighborsClassifier(n_neighbors=5,algorithm='auto')***

- n_neighborsï¼šintï¼Œå¯é€‰ï¼ˆé»˜è®¤= 5ï¼‰ï¼Œä½¿ç”¨çš„é‚»å±…æ•°
- algorithmï¼š*{â€˜autoâ€™ï¼Œâ€˜ball_treeâ€™ï¼Œâ€˜kd_treeâ€™ï¼Œâ€˜bruteâ€™}*ï¼Œå¯é€‰ç”¨äºè®¡ç®—æœ€è¿‘é‚»å±…çš„ç®—æ³•ï¼Œä¸åŒå®ç°æ–¹å¼å½±å“æ•ˆç‡
  - *â€˜ball_treeâ€™* å°†ä¼šä½¿ç”¨ BallTree
  - *â€˜kd_treeâ€™* å°†ä½¿ç”¨ KDTree
  - *â€˜autoâ€™* å°†å°è¯•æ ¹æ®ä¼ é€’ç»™fitæ–¹æ³•çš„å€¼æ¥å†³å®šæœ€åˆé€‚çš„ç®—æ³•

### ä¾‹

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn import datasets
import numpy as np
import pandas as pd

# æ•°æ®é›†å¯¼å…¥
iris = datasets.load_iris()

# æ•°æ®é›†åˆ’åˆ†
x_train, x_test, y_train, y_test = train_test_split(
    iris.data, iris.target, random_state=43)

# æ ‡å‡†åŒ–
transfer_std = StandardScaler()
x_train_std = transfer_std.fit_transform(x_train)
x_test_std = transfer_std.transform(x_test)  # æµ‹è¯•é›†ä¸è¦ç”¨fit, å› ä¸ºè¦ä¿æŒå’Œè®­ç»ƒé›†å¤„ç†æ–¹å¼ä¸€è‡´

# KNN
estimator_knn = KNeighborsClassifier(n_neighbors=3)

# è°ƒä¼˜
param_dict = {"n_neighbors": [1, 3, 5, 7, 9, 11]}
estimator_knn = GridSearchCV(
    estimator_knn, param_grid=param_dict, cv=10)  # 10æŠ˜

# è®­ç»ƒæ¨¡å‹
estimator_knn.fit(x_train_std, y_train)
y_pred = estimator_knn.predict(x_test_std)

print("é¢„æµ‹å€¼ä¸º:", y_pred, "\nçœŸå®å€¼ä¸º:", y_test, "\næ¯”è¾ƒç»“æœä¸º:", y_test == y_pred)
print("å‡†ç¡®ç‡ä¸ºï¼š\n", estimator_knn.score(x_test_std, y_test))

print("æœ€ä½³å‚æ•°:\n", estimator_knn.best_params_)
print("æœ€ä½³ç»“æœ:\n", estimator_knn.best_score_)
print("æœ€ä½³ä¼°è®¡å™¨:\n", estimator_knn.best_estimator_)
print("äº¤å‰éªŒè¯ç»“æœ:\n", estimator_knn.cv_results_)
```

è¾“å‡º

```python
é¢„æµ‹å€¼ä¸º: [0 0 2 1 2 0 2 1 1 1 0 1 2 0 1 1 0 0 2 2 0 0 0 1 2 2 0 1 0 0 1 0 1 1 2 2 1
 2] 
çœŸå®å€¼ä¸º: [0 0 2 1 2 0 2 1 1 1 0 1 2 0 1 1 0 0 2 2 0 0 0 2 2 2 0 1 0 0 1 0 1 1 2 2 1
 2] 
æ¯”è¾ƒç»“æœä¸º: [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True False
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True]
å‡†ç¡®ç‡ä¸ºï¼š
 0.9736842105263158
æœ€ä½³å‚æ•°:
{'n_neighbors': 1}
æœ€ä½³ç»“æœ:
 0.9469696969696969
æœ€ä½³ä¼°è®¡å™¨:
 KNeighborsClassifier(n_neighbors=1)
äº¤å‰éªŒè¯ç»“æœ:
{'mean_fit_time': array([0.00029657, 0.00039995, 0.00039968, 0.00049977, 0.00029998,
       0.00040131]), 'std_fit_time': array([0.00045309, 0.00048983, 0.00048951, 0.00049977, 0.00045822,
       0.0004915 ]), 'mean_score_time': array([0.00089977, 0.00080023, 0.00110025, 0.00080018, 0.00079889,
       0.00080283]), 'std_score_time': array([0.00029992, 0.0004004 , 0.00030082, 0.00040009, 0.00039965,
       0.00040154]), 'param_n_neighbors': masked_array(data=[1, 3, 5, 7, 9, 11],
             mask=[False, False, False, False, False, False],
       fill_value='?',
            dtype=object), 'params': [{'n_neighbors': 1}, {'n_neighbors': 3}, {'n_neighbors': 5}, {'n_neighbors': 7}, {'n_neighbors': 9}, {'n_neighbors': 11}], 'split0_test_score': array([0.91666667, 0.91666667, 0.91666667, 0.91666667, 0.91666667,
       0.91666667]), 'split1_test_score': array([0.91666667, 0.91666667, 0.83333333, 0.91666667, 0.91666667,
       0.91666667]), 'split2_test_score': array([0.90909091, 0.90909091, 0.90909091, 0.90909091, 0.90909091,
       1.        ]), 'split3_test_score': array([0.90909091, 0.90909091, 0.90909091, 0.90909091, 0.90909091,
       0.81818182]), 'split4_test_score': array([1., 1., 1., 1., 1., 1.]), 'split5_test_score': array([0.90909091, 0.90909091, 1.        , 1.        , 1.        ,
       1.        ]), 'split6_test_score': array([1., 1., 1., 1., 1., 1.]), 'split7_test_score': array([0.90909091, 0.81818182, 0.81818182, 0.81818182, 0.81818182,
       0.81818182]), 'split8_test_score': array([1., 1., 1., 1., 1., 1.]), 'split9_test_score': array([1.        , 0.90909091, 1.        , 1.        , 1.        ,
       0.90909091]), 'mean_test_score': array([0.9469697 , 0.92878788, 0.93863636, 0.9469697 , 0.9469697 ,
       0.93787879]), 'std_test_score': array([0.04338734, 0.05412294, 0.06830376, 0.05945884, 0.05945884,
       0.07048305]), 'rank_test_score': array([1, 6, 4, 1, 1, 5])}
```

## æœ´ç´ è´å¶æ–¯ç®—æ³•

ï¼ˆNaive Bayesï¼‰

> ç›¸äº’ç‹¬ç«‹çš„ç‰¹å¾ + è´å¶æ–¯å…¬å¼

```python
from sklearn.naive_bayes import MultinomialNB
```

æœ´ç´ ï¼šç‰¹å¾ä¸ç‰¹å¾ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„

æœ´ç´ è´å¶æ–¯ç®—æ³•ç»å¸¸ç”¨äºæ–‡æœ¬åˆ†ç±», å› ä¸ºæ–‡ç« è½¬æ¢æˆæœºå™¨å­¦ä¹ ç®—æ³•è¯†åˆ«çš„æ•°æ®æ˜¯ä»¥å•è¯ä¸ºç‰¹å¾çš„

- ä¼˜ç‚¹ï¼š
  - æœ´ç´ è´å¶æ–¯æ¨¡å‹å‘æºäºå¤å…¸æ•°å­¦ç†è®ºï¼Œæœ‰ç¨³å®šçš„åˆ†ç±»æ•ˆç‡ã€‚
  - å¯¹ç¼ºå¤±æ•°æ®ä¸å¤ªæ•æ„Ÿï¼Œç®—æ³•ä¹Ÿæ¯”è¾ƒç®€å•ï¼Œå¸¸ç”¨äºæ–‡æœ¬åˆ†ç±»ã€‚
  - åˆ†ç±»å‡†ç¡®åº¦é«˜ï¼Œé€Ÿåº¦å¿«
- ç¼ºç‚¹ï¼š
  - ç”±äºä½¿ç”¨äº†æ ·æœ¬å±æ€§ç‹¬ç«‹æ€§çš„å‡è®¾ï¼Œæ‰€ä»¥å¦‚æœç‰¹å¾å±æ€§æœ‰å…³è”æ—¶å…¶æ•ˆæœä¸å¥½

### åŸç†

#### è´å¶æ–¯å…¬å¼

ä»¥æ–‡æœ¬åˆ†ç±»ä¸ºä¾‹
$$
P(C|F_1,F_2,\ldots)=\cfrac{P(F_1,F_2,\ldots|C)P(C)}{P(F_1,F_2,\ldots)}
$$

- $P(C)$ï¼šæ¯ä¸ªæ–‡æ¡£ç±»åˆ«çš„æ¦‚ç‡(æŸæ–‡æ¡£ç±»åˆ«æ•°ï¼æ€»æ–‡æ¡£æ•°é‡)
- $P(Wâ”‚C)$ï¼šç»™å®šç±»åˆ«ä¸‹ç‰¹å¾ï¼ˆè¢«é¢„æµ‹æ–‡æ¡£ä¸­å‡ºç°çš„è¯ï¼‰çš„æ¦‚ç‡
  - $W$ ä¸ºç»™å®šæ–‡æ¡£çš„ç‰¹å¾å€¼ï¼ˆé¢‘æ•°ç»Ÿè®¡ï¼‰
  - è®¡ç®—æ–¹æ³•ï¼š$P(F_1â”‚C)=N_i/N$ ï¼ˆè®­ç»ƒæ–‡æ¡£ä¸­å»è®¡ç®—ï¼‰
    - $N_i$ï¼šè¯¥ $F_1$ è¯åœ¨ $C$ ç±»åˆ«æ‰€æœ‰æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°
    - $N$ï¼šæ‰€å±ç±»åˆ« $C$ ä¸‹çš„æ–‡æ¡£çš„æ–‡æœ¬æ€»å’Œ
- $P(F_1,F_2,\ldots)$ é¢„æµ‹æ–‡æ¡£ä¸­æ¯ä¸ªè¯çš„æ¦‚ç‡

#### æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ç³»æ•°

ç›®çš„ï¼šé˜²æ­¢è®¡ç®—å‡ºçš„åˆ†ç±»æ¦‚ç‡ä¸º0
$$
P(F_1|C)=\cfrac{N_i+\alpha}{N+\alpha m}
$$

- $\alpha$ï¼šé¢„å…ˆæŒ‡å®šçš„ç³»æ•°ï¼Œé»˜è®¤ä¸º 1
- $m$ï¼šè®­ç»ƒæ–‡æ¡£ä¸­ç‰¹å¾è¯çš„ç§ç±»æ•°

```python
# å› ä¸ºæ ·æœ¬æ•°é‡ä¸å¤Ÿï¼Œä¼šå‡ºç°ç‰¹å¾è¯ä¸åœ¨ä¸€ç±»æ–‡æœ¬ä¸­å‡ºç°çš„æƒ…å†µ
P(å¨±ä¹|å½±é™¢,æ”¯ä»˜å®,äº‘è®¡ç®—) = ğ‘ƒ(å½±é™¢,æ”¯ä»˜å®,äº‘è®¡ç®—|å¨±ä¹)âˆ—P(å¨±ä¹)=(56/121)âˆ—(15/121)âˆ—(0/121)âˆ—(60/90) = 0
# æ­¤æ—¶éœ€è¦å®ç”¨åˆ°æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ç³»æ•°
P(å¨±ä¹|å½±é™¢,æ”¯ä»˜å®,äº‘è®¡ç®—) =P(å½±é™¢,æ”¯ä»˜å®,äº‘è®¡ç®—|å¨±ä¹)P(å¨±ä¹)=(56+1/121+4)(15+1/121+4)(0+1/121+1*4)(60/90) = 0.00002
```

### API

***MultinomialNB(alpha = 1.0)***

- *alpha*ï¼šæ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ç³»æ•°

### ä¾‹ï¼š20ç±»æ–°é—»åˆ†ç±»

åˆ†æ

- åˆ’åˆ†æ•°æ®é›†
- tfidf è¿›è¡Œçš„ç‰¹å¾æŠ½å–
- æœ´ç´ è´å¶æ–¯é¢„æµ‹

```python
from sklearn.datasets import fetch_20newsgroups, load_files
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
import pandas as pd

data = fetch_20newsgroups(subset="all") 
x_train, x_test, y_train, y_test = \
        train_test_split(data.data, data.target, test_size=0.2, random_state=22)
# æ–‡æœ¬åˆ†ç±»
transfer = TfidfVectorizer()  
x_train = transfer.fit_transform(x_train)
x_test = transfer.transform(x_test)  
# æœ´ç´ è´å¶æ–¯
estimator = MultinomialNB()
estimator.fit(x_train, y_train)
y_predict = estimator.predict(x_test)

print("é¢„æµ‹å€¼ä¸º:", y_predict, "\nçœŸå®å€¼ä¸º:", y_test, "\næ¯”è¾ƒç»“æœä¸º:", y_test == y_predict)
score = estimator.score(x_test, y_test)
print("å‡†ç¡®ç‡ä¸º: ", score)
```

è¾“å‡º

```python
é¢„æµ‹å€¼ä¸º: [15 13 16 ... 13  2 13] 
çœŸå®å€¼ä¸º: [15 13 16 ... 13  2 13] 
æ¯”è¾ƒç»“æœä¸º: [ True  True  True ...  True  True  True]
å‡†ç¡®ç‡ä¸º:  0.8511936339522547
```

## å†³ç­–æ ‘

ï¼ˆDecision Treeï¼‰

```python
from sklearn.tree import DecisionTreeClassifier
```

> if - else æ ¹æ®ç‰¹å¾çš„ä¿¡æ¯ç†µç­›é€‰

### åŸç†

#### ä¿¡æ¯ç†µ

$$
H(X)=-\sum\limits_{i=1}^n P(x_i)log_bP(x_i)
$$

#### æ¡ä»¶ä¿¡æ¯ç†µ

$$
H(D|A)=\sum\limits_{i=1}^n \cfrac{|D_i|}{|D|}H(D_i)
$$



#### ä¿¡æ¯å¢ç›Š

å†³ç­–æ ‘çš„åˆ’åˆ†ä¾æ®ä¹‹ä¸€

ç‰¹å¾ $A$ å¯¹è®­ç»ƒæ•°æ®é›† $D$ çš„ä¿¡æ¯å¢ç›Š $g(D,A)$,å®šä¹‰ä¸ºé›†åˆ D çš„ä¿¡æ¯ç†µ $H(D)$ ä¸ç‰¹å¾ $A$ ç»™å®šæ¡ä»¶ä¸‹ $D$ çš„ä¿¡æ¯æ¡ä»¶ç†µ $H(D|A)$ ä¹‹å·®
$$
g(D,A)=H(D)=H(D|A)
$$

#### ä¸‰ç§ç®—æ³•å®ç°

- ID3
  - ä¿¡æ¯å¢ç›Š æœ€å¤§çš„å‡†åˆ™
- C4.5
  - ä¿¡æ¯å¢ç›Šæ¯” æœ€å¤§çš„å‡†åˆ™
- CART
  - åˆ†ç±»æ ‘: åŸºå°¼ç³»æ•° æœ€å°çš„å‡†åˆ™ åœ¨sklearnä¸­å¯ä»¥é€‰æ‹©åˆ’åˆ†çš„é»˜è®¤åŸåˆ™
  - ä¼˜åŠ¿ï¼šåˆ’åˆ†æ›´åŠ ç»†è‡´ï¼ˆä»åé¢ä¾‹å­çš„æ ‘æ˜¾ç¤ºæ¥ç†è§£ï¼‰

### API

***DecisionTreeClassifier(criterion=â€™giniâ€™, max_depth=None,random_state=None)***

å†³ç­–æ ‘åˆ†ç±»å™¨

- criterionï¼šé»˜è®¤æ˜¯â€™giniâ€™ç³»æ•°ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©ä¿¡æ¯å¢ç›Šçš„ç†µâ€™entropyâ€™
- max_depthï¼šæ ‘çš„æ·±åº¦å¤§å°
- random_stateï¼šéšæœºæ•°ç§å­

### ä¿å­˜æ ‘çš„ç»“æ„

```python
from sklearn.tree import export_graphviz
```

***export_graphviz()*** 

è¯¥å‡½æ•°èƒ½å¤Ÿå¯¼å‡ºDOTæ ¼å¼

- tree.export_graphviz(estimator, out_file=path, feature_names)

### ä¾‹

```python
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, export_graphviz

iris = datasets.load_iris()
x_train, x_test, y_train, y_test = train_test_split(
    iris.data, iris.target, random_state=22)
# å†³ç­–æ ‘è®­ç»ƒ
estimator = DecisionTreeClassifier(criterion="entropy")
estimator.fit(x_train, y_train)
# ç”Ÿæˆæ ‘æ–‡ä»¶
export_graphviz(estimator, out_file="tree.dot",
                feature_names=iris.feature_names)

y_pred = estimator.predict(x_test)
print("é¢„æµ‹å€¼ä¸º:", y_pred, "\nçœŸå®å€¼ä¸º:", y_test, "\næ¯”è¾ƒç»“æœä¸º:", y_test == y_pred)
score = estimator.score(x_test, y_test)
print("å‡†ç¡®ç‡ä¸º: ", score)
```

è¾“å‡º

```python
é¢„æµ‹å€¼ä¸º: [0 2 1 2 1 1 1 1 1 0 2 1 2 2 0 2 1 1 1 1 0 2 0 1 2 0 2 2 2 1 0 0 1 1 1 0 0
 0] 
çœŸå®å€¼ä¸º: [0 2 1 2 1 1 1 2 1 0 2 1 2 2 0 2 1 1 2 1 0 2 0 1 2 0 2 2 2 2 0 0 1 1 1 0 0
 0] 
æ¯”è¾ƒç»“æœä¸º: [ True  True  True  True  True  True  True False  True  True  True  True
  True  True  True  True  True  True False  True  True  True  True  True
  True  True  True  True  True False  True  True  True  True  True  True
  True  True]
å‡†ç¡®ç‡ä¸º:  0.9210526315789473
```

## éšæœºæ£®æ—

```python
from sklearn.ensemble import RandomForestClassifier
```

**éšæœºæ£®æ—æ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªå†³ç­–æ ‘çš„åˆ†ç±»å™¨**ï¼Œå¹¶ä¸”å…¶è¾“å‡ºçš„ç±»åˆ«æ˜¯ç”±ä¸ªåˆ«æ ‘è¾“å‡ºçš„ç±»åˆ«çš„ä¼—æ•°è€Œå®šã€‚

### åŸç†

å­¦ä¹ ç®—æ³•æ ¹æ®ä¸‹åˆ—ç®—æ³•è€Œå»ºé€ æ¯æ£µæ ‘

- ç”¨ N æ¥è¡¨ç¤º æ ·æœ¬ä¸ªæ•°ï¼ŒM è¡¨ç¤ºç‰¹å¾æ•°ç›®ã€‚
  - 1ã€æœ‰æ”¾å›åœ°æŠ½æ ·ï¼ˆbootstrapï¼‰ï¼Œä¸€æ¬¡éšæœºé€‰å‡ºä¸€ä¸ªæ ·æœ¬ï¼Œé‡å¤ N æ¬¡
  - 2ã€éšæœºé€‰å‡º m ä¸ªç‰¹å¾, m <<Mï¼Œå»ºç«‹å†³ç­–æ ‘

### API

***RandomForestClassifier(n_estimators=10, criterion=â€™giniâ€™, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2)***

- éšæœºæ£®æ—åˆ†ç±»å™¨
- *n_estimators*ï¼šintegerï¼Œoptionalï¼ˆdefault = 10ï¼‰æ£®æ—é‡Œçš„æ ‘æœ¨æ•°é‡ 120,200,300,500,800,1200
- *criteria*ï¼šstringï¼Œå¯é€‰ï¼ˆdefault =â€œginiâ€ï¼‰åˆ†å‰²ç‰¹å¾çš„æµ‹é‡æ–¹æ³•
- *max_depth*ï¼šinteger æˆ– Noneï¼Œå¯é€‰ï¼ˆé»˜è®¤=æ— ï¼‰æ ‘çš„æœ€å¤§æ·±åº¦ 5,8,15,25,30
- *max_features="autoâ€*ï¼šæ¯ä¸ªå†³ç­–æ ‘çš„æœ€å¤§ç‰¹å¾æ•°é‡
  - If "auto", then `max_features=sqrt(n_features)`.
  - If "sqrt", then `max_features=sqrt(n_features)` (same as "auto").
  - If "log2", then `max_features=log2(n_features)`.
  - If None, then `max_features=n_features`.
- *bootstrap*ï¼šbooleanï¼Œoptionalï¼ˆdefault = Trueï¼‰æ˜¯å¦åœ¨æ„å»ºæ ‘æ—¶ä½¿ç”¨æ”¾å›æŠ½æ ·
- *min_samples_split*ï¼šèŠ‚ç‚¹åˆ’åˆ†æœ€å°‘æ ·æœ¬æ•°
- *min_samples_leaf*ï¼šå¶å­èŠ‚ç‚¹çš„æœ€å°æ ·æœ¬æ•°

- è¶…å‚æ•°ï¼šn_estimator, max_depth, min_samples_split,min_samples_leaf

#### ä¾‹

```python
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

iris = datasets.load_iris()
# æ•°æ®é›†åˆ’åˆ†
x_train, x_test, y_train, y_test = train_test_split(
    iris.data, iris.target, random_state=22)
# RandomForest
estimator_rf = RandomForestClassifier() 
# è¶…å‚è°ƒä¼˜
param_dict = {"n_estimators": [120,200,300,500,800,1200], "max_depth": [5, 8, 15, 25, 30]}
estimator_rf = GridSearchCV(estimator_rf, param_grid=param_dict,cv=5)
# è®­ç»ƒ
estimator_rf.fit(x_train,y_train)
y_pred = estimator_rf.predict(x_test)
# è¾“å‡ºç»“æœ
print("é¢„æµ‹å€¼ä¸º:", y_pred, "\nçœŸå®å€¼ä¸º:", y_test, "\næ¯”è¾ƒç»“æœä¸º:", y_test == y_pred)
print("å‡†ç¡®ç‡ä¸ºï¼š\n", estimator_rf.score(x_test, y_test))

print("æœ€ä½³å‚æ•°:\n", estimator_rf.best_params_)
print("æœ€ä½³ç»“æœ:\n", estimator_rf.best_score_)
print("æœ€ä½³ä¼°è®¡å™¨:\n", estimator_rf.best_estimator_)
print("äº¤å‰éªŒè¯ç»“æœ:\n", estimator_rf.cv_results_)
```

è¾“å‡º

```python
é¢„æµ‹å€¼ä¸º: [0 2 1 2 1 1 1 1 1 0 2 1 2 2 0 2 1 1 1 1 0 2 0 1 2 0 2 2 2 1 0 0 1 1 1 0 0
 0] 
çœŸå®å€¼ä¸º: [0 2 1 2 1 1 1 2 1 0 2 1 2 2 0 2 1 1 2 1 0 2 0 1 2 0 2 2 2 2 0 0 1 1 1 0 0
 0] 
æ¯”è¾ƒç»“æœä¸º: [ True  True  True  True  True  True  True False  True  True  True  True
  True  True  True  True  True  True False  True  True  True  True  True
  True  True  True  True  True False  True  True  True  True  True  True
  True  True]
å‡†ç¡®ç‡ä¸ºï¼š
 0.9210526315789473
æœ€ä½³å‚æ•°:
{'max_depth': 5, 'n_estimators': 120}
æœ€ä½³ç»“æœ:
 0.9553571428571429
æœ€ä½³ä¼°è®¡å™¨:
 RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=5, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=120, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
äº¤å‰éªŒè¯ç»“æœ:
 {'mean_fit_time': array([0.32009826, 0.57197742, ......., 0., 0., 0., 0.])}

```

## é€»è¾‘å›å½’

ï¼ˆLogistic Regressionï¼‰

```python
from sklearn.linear_model import LogisticRegression
```

é€»è¾‘å›å½’æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„ä¸€ç§åˆ†ç±»æ¨¡å‹ï¼Œé€»è¾‘å›å½’æ˜¯ä¸€ç§åˆ†ç±»ç®—æ³•ï¼Œè™½ç„¶åå­—ä¸­å¸¦æœ‰å›å½’ï¼Œä½†æ˜¯å®ƒä¸å›å½’ä¹‹é—´æœ‰ä¸€å®šçš„è”ç³»ã€‚

### åŸç†

#### è¾“å…¥

é€»è¾‘å›å½’çš„è¾“å…¥å°±æ˜¯ä¸€ä¸ªçº¿æ€§å›å½’çš„ç»“æœã€‚
$$
h(w)=w_1x_1+w_2x_2+w_3x_3\ldots+b
$$

#### æ¿€æ´»å‡½æ•°

sigmoid å‡½æ•°
$$
g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}
$$
åˆ†æ

- å›å½’çš„ç»“æœè¾“å…¥åˆ° sigmoid å‡½æ•°å½“ä¸­
- è¾“å‡ºç»“æœï¼š[0, 1] åŒºé—´ä¸­çš„ä¸€ä¸ªæ¦‚ç‡å€¼ï¼Œé»˜è®¤ä¸º 0.5 ä¸ºé˜ˆå€¼

> é€»è¾‘å›å½’æœ€ç»ˆçš„åˆ†ç±»æ˜¯é€šè¿‡å±äºæŸä¸ªç±»åˆ«çš„æ¦‚ç‡å€¼æ¥åˆ¤æ–­æ˜¯å¦å±äºæŸä¸ªç±»åˆ«ï¼Œå¹¶ä¸”è¿™ä¸ªç±»åˆ«é»˜è®¤æ ‡è®°ä¸º 1 (æ­£ä¾‹),å¦å¤–çš„ä¸€ä¸ªç±»åˆ«ä¼šæ ‡è®°ä¸º 0 (åä¾‹)ã€‚ï¼ˆæ–¹ä¾¿æŸå¤±è®¡ç®—ï¼‰

<img src="https://trou.oss-cn-shanghai.aliyuncs.com/img/é€»è¾‘å›å½’è¿ç®—è¿‡ç¨‹.png" alt="é€»è¾‘å›å½’è¿ç®—è¿‡ç¨‹" style="zoom: 67%;" />

#### æŸå¤±å‡½æ•°

é€»è¾‘å›å½’çš„æŸå¤±ï¼Œç§°ä¹‹ä¸º**å¯¹æ•°ä¼¼ç„¶æŸå¤±**
$$
cost(h_\theta(x),y)=\sum\limits_{i=1}^m-y_ilog(h_\theta(x))-(1-y_i)log(1-h_\theta(x))
$$
<img src="https://trou.oss-cn-shanghai.aliyuncs.com/img/æŸå¤±è®¡ç®—è¿‡ç¨‹.png" alt="æŸå¤±è®¡ç®—è¿‡ç¨‹" style="zoom: 67%;" />

#### ä¼˜åŒ–

åŒæ ·ä½¿ç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•ï¼Œå»å‡å°‘æŸå¤±å‡½æ•°çš„å€¼ã€‚è¿™æ ·å»æ›´æ–°é€»è¾‘å›å½’å‰é¢å¯¹åº”ç®—æ³•çš„æƒé‡å‚æ•°ï¼Œæå‡åŸæœ¬å±äº 1 ç±»åˆ«çš„æ¦‚ç‡ï¼Œé™ä½åŸæœ¬æ˜¯ 0 ç±»åˆ«çš„æ¦‚ç‡ã€‚

### API

***LogisticRegression(solver='liblinear', penalty=â€˜l2â€™, C = 1.0)***

- *solver*ï¼šä¼˜åŒ–æ±‚è§£æ–¹å¼ï¼ˆé»˜è®¤å¼€æºçš„ liblinear åº“å®ç°ï¼Œå†…éƒ¨ä½¿ç”¨äº†åæ ‡è½´ä¸‹é™æ³•æ¥è¿­ä»£ä¼˜åŒ–æŸå¤±å‡½æ•°ï¼‰
  - *sag*ï¼šæ ¹æ®æ•°æ®é›†è‡ªåŠ¨é€‰æ‹©ï¼Œéšæœºå¹³å‡æ¢¯åº¦ä¸‹é™
- *penalty*ï¼šæ­£åˆ™åŒ–çš„ç§ç±»
- *C*ï¼šæ­£åˆ™åŒ–åŠ›åº¦



LogisticRegression æ–¹æ³•ç›¸å½“äº `SGDClassifier(loss="log", penalty=" ")`, SGDClassifier å®ç°äº†ä¸€ä¸ªæ™®é€šçš„éšæœºæ¢¯åº¦ä¸‹é™å­¦ä¹ ï¼Œä¹Ÿæ”¯æŒå¹³å‡éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼ˆASGDï¼‰ï¼Œå¯ä»¥é€šè¿‡è®¾ç½® average=Trueã€‚è€Œä½¿ç”¨ LogisticRegression (å®ç°äº† SAG)

### ä¾‹

æ¡ˆä¾‹ï¼šç™Œç—‡åˆ†ç±»é¢„æµ‹-è‰¯ï¼æ¶æ€§ä¹³è…ºç™Œè‚¿ç˜¤é¢„æµ‹

> æ•°æ®æè¿°
>
> ï¼ˆ1ï¼‰699æ¡æ ·æœ¬ï¼Œå…±11åˆ—æ•°æ®ï¼Œç¬¬ä¸€åˆ—ç”¨è¯­æ£€ç´¢çš„idï¼Œå9åˆ—åˆ†åˆ«æ˜¯ä¸è‚¿ç˜¤
>
> ç›¸å…³çš„åŒ»å­¦ç‰¹å¾ï¼Œæœ€åä¸€åˆ—è¡¨ç¤ºè‚¿ç˜¤ç±»å‹çš„æ•°å€¼ã€‚
>
> ï¼ˆ2ï¼‰åŒ…å«16ä¸ªç¼ºå¤±å€¼ï¼Œç”¨ â€?â€ æ ‡å‡ºã€‚

åˆ†æ

- ç¼ºå¤±å€¼å¤„ç†
- æ ‡å‡†åŒ–å¤„ç†
- é€»è¾‘å›å½’é¢„æµ‹

#### åˆå§‹åŒ–

```python
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# æ–‡ä»¶è¯»å–
column_name = ['Sample code number', 'Clump Thickness',
               'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion',
               'Single Epithelial Cell Size',
               'Bare Nuclei', 'Bland Chromatin',
               'Normal Nucleoli', 'Mitoses', 'Class']
original_data = pd.read_csv('../Data/breast-cancer-wisconsin.data',names=column_name)
original_data.head()
# ç¼ºå¤±å€¼å¤„ç†
# ç¬¬ä¸€æ­¥å…ˆæ›¿æ¢ ? ä¸º nan
data = original_data.replace(to_replace="?", value=np.nan)
data.dropna(inplace=True)
print("æ£€æµ‹æ˜¯å¦è¿˜æœ‰ç¼ºå¤±å€¼(å…¨ä¸ºfalseè¡¨ç¤ºæ²¡æœ‰ç¼ºå¤±å€¼)")  # æ£€æµ‹æ˜¯å¦è¿˜æœ‰ç¼ºå¤±å€¼
print(data.isnull().any())
# ç¬¬ä¸‰æ­¥ ç­›é€‰ç‰¹å¾å€¼å’Œç›®æ ‡å€¼
x = data.iloc[:, 1:-1]  # è¡¨ç¤ºæ¯ä¸€è¡Œæ•°æ®éƒ½è¦, ä»ç¬¬ä¸€åˆ—åˆ°å€’æ•°ç¬¬äºŒåˆ—çš„columnå­—æ®µä¹Ÿè¦
y = data["Class"]
x_train, x_test, y_train, y_test = train_test_split(x, y)
```

è¾“å‡º

```python
æ£€æµ‹æ˜¯å¦è¿˜æœ‰ç¼ºå¤±å€¼(å…¨ä¸ºfalseè¡¨ç¤ºæ²¡æœ‰ç¼ºå¤±å€¼)
Sample code number             False
Clump Thickness                False
Uniformity of Cell Size        False
Uniformity of Cell Shape       False
Marginal Adhesion              False
Single Epithelial Cell Size    False
Bare Nuclei                    False
Bland Chromatin                False
Normal Nucleoli                False
Mitoses                        False
Class                          False
```

#### è®­ç»ƒ

```python
# ç¬¬å››æ­¥: å¼€å§‹ç‰¹å¾å·¥ç¨‹
transfer = StandardScaler()
x_train = transfer.fit_transform(x_train)
x_test = transfer.transform(x_test)

# ç¬¬äº”æ­¥, é¢„ä¼°å™¨æµç¨‹
estimator = LogisticRegression()  # é»˜è®¤å‚æ•°
estimator.fit(x_train, y_train)
print("é€»è¾‘å›å½’_æƒé‡ç³»æ•°ä¸º: ", estimator.coef_)
print("é€»è¾‘å›å½’_åç½®ä¸º:", estimator.intercept_)
# ç¬¬å…­æ­¥, æ¨¡å‹è¯„ä¼°
y_predict = estimator.predict(x_test)
print("é€»è¾‘å›å½’_é¢„æµ‹ç»“æœ", y_predict)
print("é€»è¾‘å›å½’_é¢„æµ‹ç»“æœå¯¹æ¯”:", y_test == y_predict)
score = estimator.score(x_test, y_test)
print("å‡†ç¡®ç‡ä¸º:", score)
# 2æ˜¯è‰¯æ€§çš„ 4æ˜¯æ¶æ€§çš„
"""
ä½†æ˜¯å®é™…ä¸Šè¿™ä¸ªé¢„æµ‹ç»“æœä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„, ä»¥ä¸Šåªèƒ½è¯´æ˜é¢„æµ‹çš„æ­£ç¡®ä¸å¦,
è€Œäº‹å®ä¸Š, æˆ‘ä»¬éœ€è¦ä¸€ç§è¯„ä¼°æ–¹å¼æ¥æ˜¾ç¤ºæˆ‘ä»¬å¯¹æ¶æ€§breastçš„é¢„æµ‹æˆåŠŸç‡, ä¹Ÿå°±æ˜¯å¬å›ç‡
åŒæ—¶å¯ä»¥æŸ¥çœ‹F1-scoreçš„ç¨³å¥æ€§
(å¬å›ç‡å’Œç²¾ç¡®ç‡çœ‹ç¬”è®°å’Œæˆªå›¾)
æ‰€ä»¥ä¸‹é¢æ¢ä¸€ç§è¯„ä¼°æ–¹æ³•
"""
```

è¾“å‡º

```python
é€»è¾‘å›å½’_æƒé‡ç³»æ•°ä¸º:  [[1.20895973 0.34430535 0.93605533 0.50117234 0.22296947 1.22295345
  0.81648447 0.64096012 0.71930684]]
é€»è¾‘å›å½’_åç½®ä¸º: [-0.9875554]
é€»è¾‘å›å½’_é¢„æµ‹ç»“æœ [2 4 2 2 4 4 2 2 4 2 2 2 4 4 4 2 2 4 4 4 4 2 4 4 4 2 2 4 2 2 4 2 4 4 4 2 4
 2 2 2 2 4 4 2 4 2 2 2 2 4 2 2 2 4 4 2 4 2 2 2 2 2 2 4 4 4 2 2 2 2 2 4 2 2
 2 2 4 2 2 2 2 2 2 2 2 4 2 2 4 2 2 2 4 2 4 2 2 2 2 2 2 2 2 2 4 4 2 2 4 4 2
 2 2 4 2 4 2 2 4 4 2 2 2 2 4 2 2 2 4 2 2 2 2 2 4 2 4 2 4 2 2 4 2 2 2 4 4 4
 2 2 2 2 4 4 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 4 2]
é€»è¾‘å›å½’_é¢„æµ‹ç»“æœå¯¹æ¯”: 541     True
288     True
395     True
409     True
568     True
       ...  
442     True
51     False
370     True
304     True
363     True
Name: Class, Length: 171, dtype: bool
å‡†ç¡®ç‡ä¸º: 0.9766081871345029
```

#### æŸ¥çœ‹ç²¾ç¡®ç‡ï¼Œå¬å›ç‡ï¼ŒF1-score

```python
Score = classification_report(y_test, y_predict, labels=[2, 4],
                              target_names=["è‰¯æ€§", "æ¶æ€§"])
print("æŸ¥çœ‹ç²¾ç¡®ç‡,å¬å›ç‡,F1-score\n", Score)
# supportè¡¨ç¤ºæ ·æœ¬é‡
```

è¾“å‡º

```python
æŸ¥çœ‹ç²¾ç¡®ç‡,å¬å›ç‡,F1-score
               precision    recall  f1-score   support

          è‰¯æ€§       0.98      0.98      0.98       114
          æ¶æ€§       0.96      0.96      0.96        57

    accuracy                           0.98       171
   macro avg       0.97      0.97      0.97       171
weighted avg       0.98      0.98      0.98       171
```

#### æŸ¥çœ‹ ROC æ›²çº¿å’Œ AUC æŒ‡æ ‡

```python
"""
ROCæ›²çº¿å’ŒAUCæŒ‡æ ‡(æ ·æœ¬åˆ†ç±»ä¸å‡è¡¡çš„æƒ…å†µä¸‹,å¯ä»¥ä½¿ç”¨è¿™ç§æ–¹æ³•)
AUC = 0.5 æ˜¯ççŒœæ¨¡å‹
AUC = 1 æ˜¯æœ€å¥½çš„æ¨¡å‹
AUC < 0.5 å±äºåå‘æ¯’å¥¶
æ›´å¤šçš„çœ‹æˆªå›¾
"""
# éœ€è¦è½¬æ¢ä¸º0,1è¡¨ç¤º
y_true = np.where(y_test > 3, 1, 0)  # è¡¨ç¤ºå¤§äº3ä¸º1,åä¹‹ä¸º0(classå€¼ä¸º2å’Œ4)
return_value = roc_auc_score(y_true, y_predict)
print("ROCæ›²çº¿å’ŒAUCè¿”å›å€¼ä¸º(ä¸‰è§’å½¢é¢ç§¯)", return_value)

fpr, tpr, thresholds = roc_curve(y_true, y_predict)
plt.plot(fpr, tpr)
plt.show()
```

è¾“å‡º

<img src="https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20210127225009225.png" alt="image-20210127225009225" style="zoom:67%;" />



# å›å½’

ï¼ˆRegressionï¼‰

- å°è§„æ¨¡æ•°æ®ï¼š
  - **LinearRegression(ä¸èƒ½è§£å†³æ‹Ÿåˆé—®é¢˜)**
  - å²­å›å½’
- å¤§è§„æ¨¡æ•°æ®ï¼šSGDRegressor

## çº¿æ€§å›å½’

```python
from sklearn.linear_model import LinearRegression, SGDRegressor
```

ï¼ˆLinear Regressionï¼‰

çº¿æ€§å›å½’ (Linear regression) æ˜¯åˆ©ç”¨**å›å½’æ–¹ç¨‹(å‡½æ•°)**å¯¹ä¸€ä¸ªæˆ–**å¤šä¸ªè‡ªå˜é‡(ç‰¹å¾å€¼)å’Œå› å˜é‡(ç›®æ ‡å€¼)ä¹‹é—´**å…³ç³»è¿›è¡Œå»ºæ¨¡çš„ä¸€ç§åˆ†ææ–¹å¼

### API

#### æ­£è§„æ–¹ç¨‹

  æ­£è§„æ–¹ç¨‹çš„ä¼˜åŒ–æ–¹æ³•ï¼Œä¸èƒ½è§£å†³æ‹Ÿåˆé—®é¢˜ï¼Œä¸€æ¬¡æ€§æ±‚è§£ï¼Œé’ˆå¯¹å°æ•°æ®

- ***LinearRegression(fit_intercept=True)***
  - *fit_intercept*ï¼šæ˜¯å¦è®¡ç®—åç½®
  - å±æ€§
    - *LinearRegression.coef_*ï¼šæƒé‡ç³»æ•°ï¼ˆå›å½’ç³»æ•°ï¼‰
    - *LinearRegression.intercept_*ï¼šåç½®

#### æ¢¯åº¦ä¸‹é™

å…¶å®æ˜¯éšæœºæ¢¯åº¦ä¸‹é™

- ***SGDRegressor(loss="squared_loss", fit_intercept=True, learning_rate ='invscaling', eta0=0.01)***
  - SGDRegressorç±»å®ç°äº†éšæœºæ¢¯åº¦ä¸‹é™å­¦ä¹ ï¼Œå®ƒæ”¯æŒä¸åŒçš„**losså‡½æ•°å’Œæ­£åˆ™åŒ–æƒ©ç½šé¡¹**æ¥æ‹Ÿåˆçº¿æ€§å›å½’æ¨¡å‹ã€‚
  - *loss*ï¼šæŸå¤±ç±»å‹
    - **loss=â€squared_lossâ€: æ™®é€šæœ€å°äºŒä¹˜æ³•**
  - *fit_intercept*ï¼šæ˜¯å¦è®¡ç®—åç½®
  - *learning_rate*ï¼šstring, optional
    - å­¦ä¹ ç‡å¡«å……ï¼Œå¯¹äºä¸€ä¸ªå¸¸æ•°å€¼çš„å­¦ä¹ ç‡æ¥è¯´ï¼Œå¯ä»¥ä½¿ç”¨learning_rate=â€™constantâ€™ ï¼Œå¹¶ä½¿ç”¨eta0æ¥æŒ‡å®šå­¦ä¹ ç‡ã€‚
    - 'constant'ï¼šeta = eta0
    - 'optimal'ï¼šeta = 1.0 / (alpha \* (t + t0)) [default]
    - 'invscaling'ï¼šeta = eta0 / pow(t, power_t=0.25)
  - *max_iter*ï¼šè¿­ä»£æ¬¡æ•°
  - å±æ€§
    - *SGDRegressor.coef_*ï¼šå›å½’ç³»æ•°
    - *SGDRegressor.intercept_*ï¼šåç½®

#### å¯¹æ¯”

|       æ¢¯åº¦ä¸‹é™       |            æ­£è§„æ–¹ç¨‹             |
| :------------------: | :-----------------------------: |
|    éœ€è¦é€‰æ‹©å­¦ä¹ ç‡    |             ä¸éœ€è¦              |
|     éœ€è¦è¿­ä»£æ±‚è§£     |          ä¸€æ¬¡è¿ç®—å¾—å‡º           |
| ç‰¹å¾æ•°é‡è¾ƒå¤§å¯ä»¥ä½¿ç”¨ | éœ€è¦è®¡ç®—æ–¹ç¨‹ï¼Œæ—¶é—´å¤æ‚åº¦é«˜O(n3) |

### å›å½’æ€§èƒ½è¯„ä¼°

#### å‡æ–¹è¯¯å·® MSE

```python
from sklearn.metrics import mean_squared_error
```

å‡æ–¹è¯¯å·®ï¼ˆMean Squared Errorï¼‰MSE è¯„ä»·æœºåˆ¶

***mean_squared_error(y_test, y_pred)***

- å‡æ–¹è¯¯å·®å›å½’æŸå¤±
- *y_test*ï¼šçœŸå®å€¼
- *y_pred*ï¼šé¢„æµ‹å€¼
- *return*ï¼šæµ®ç‚¹æ•°ç»“æœ

#### å¹³å‡ç»å¯¹è¯¯å·® MAE

```python
from sklearn.metrics import mean_absolute_error
```

***mean_absolute_error(y_test, y_pred)***

- å¹³å‡ç»å¯¹è¯¯å·®å›å½’æŸå¤±
- *y_test*ï¼šçœŸå®å€¼
- *y_pred*ï¼šé¢„æµ‹å€¼
- *return*ï¼šæµ®ç‚¹æ•°ç»“æœ

### ä¾‹

#### åˆå§‹åŒ–

```python
from sklearn.linear_model import LinearRegression, SGDRegressor
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error  # å‡æ–¹è¯¯å·®

boston_data = load_boston()
print("ç‰¹å¾æ•°é‡ä¸º:(æ ·æœ¬æ•°,ç‰¹å¾æ•°)", boston_data.data.shape)
x_train, x_test, y_train, y_test = train_test_split(boston_data.data,
                                                    boston_data.target, random_state=22)
# æ ‡å‡†åŒ–
transfer = StandardScaler()
x_train = transfer.fit_transform(x_train)
x_test = transfer.transform(x_test)
```

#### æ­£è§„æ–¹ç¨‹

```python
estimator = LinearRegression()
estimator.fit(x_train, y_train)

print("æ­£è§„æ–¹ç¨‹_æƒé‡ç³»æ•°ä¸º: ", estimator.coef_)
print("æ­£è§„æ–¹ç¨‹_åç½®ä¸º:", estimator.intercept_)

y_predict = estimator.predict(x_test)
error = mean_squared_error(y_test, y_predict)
print("æ­£è§„æ–¹ç¨‹_æˆ¿ä»·é¢„æµ‹:", y_predict)
print("æ­£è§„æ–¹ç¨‹_å‡åˆ†è¯¯å·®:", error)
```

è¾“å‡º

```python
æ­£è§„æ–¹ç¨‹_æƒé‡ç³»æ•°ä¸º:  [-0.64817766  1.14673408 -0.05949444  0.74216553 -1.95515269  2.70902585
 -0.07737374 -3.29889391  2.50267196 -1.85679269 -1.75044624  0.87341624
 -3.91336869]
æ­£è§„æ–¹ç¨‹_åç½®ä¸º: 22.62137203166228
æ­£è§„æ–¹ç¨‹_æˆ¿ä»·é¢„æµ‹: [28.22944896 31.5122308  21.11612841 32.6663189  20.0023467  19.07315705
 21.09772798 19.61400153 19.61907059 32.87611987 20.97911561 27.52898011
 15.54701758 19.78630176 ......
 15.17700342  3.81620663 29.18194848 20.68544417 22.32934783 28.01568563
 28.58237108]
æ­£è§„æ–¹ç¨‹_å‡åˆ†è¯¯å·®: 20.627513763095386
```

#### æ¢¯åº¦ä¸‹é™

```python
estimator = SGDRegressor(learning_rate="constant", eta0=0.01, max_iter=10000)
# estimator = SGDRegressor(penalty='l2', loss="squared_loss")  # è¿™æ ·è®¾ç½®å°±ç›¸å½“äºå²­å›å½’, ä½†æ˜¯å»ºè®®ç”¨Ridgeæ–¹æ³•
estimator.fit(x_train, y_train)

print("æ¢¯åº¦ä¸‹é™_æƒé‡ç³»æ•°ä¸º: ", estimator.coef_)
print("æ¢¯åº¦ä¸‹é™_åç½®ä¸º:", estimator.intercept_)

y_predict = estimator.predict(x_test)
error = mean_squared_error(y_test, y_predict)
print("æ¢¯åº¦ä¸‹é™_æˆ¿ä»·é¢„æµ‹:", y_predict)
print("æ¢¯åº¦ä¸‹é™_å‡åˆ†è¯¯å·®:", error)
```

è¾“å‡º

```python
æ¢¯åº¦ä¸‹é™_æƒé‡ç³»æ•°ä¸º:  [-0.63057536  1.10395195  0.0426204   1.11219718 -1.91486635  2.72806163
 -0.05021542 -3.52443232  2.47863671 -1.62374879 -1.9093765   1.08972091
 -4.48569927]
æ¢¯åº¦ä¸‹é™_åç½®ä¸º: [22.36660176]
æ¢¯åº¦ä¸‹é™_æˆ¿ä»·é¢„æµ‹: [28.44139247 31.71808756 20.86031611 34.1638423  19.35660167 19.18397968
 20.97064914 18.87641833 18.87914517 ......
 14.57654182  2.54082058 29.38973401 20.80732646 21.65598607 27.85659704
 29.41864109]
æ¢¯åº¦ä¸‹é™_å‡åˆ†è¯¯å·®: 20.997365545229272
```

## å²­å›å½’

ï¼ˆRidge Regressionï¼‰

```python
from sklearn.linear_model import Ridge, RidgeCV
```

å²­å›å½’ï¼Œå…¶å®ä¹Ÿæ˜¯ä¸€ç§çº¿æ€§å›å½’ã€‚åªä¸è¿‡åœ¨ç®—æ³•å»ºç«‹å›å½’æ–¹ç¨‹æ—¶å€™ï¼ŒåŠ ä¸Šæ­£åˆ™åŒ–çš„é™åˆ¶ï¼Œä»è€Œè¾¾åˆ°è§£å†³è¿‡æ‹Ÿåˆçš„æ•ˆæœ

### åŸç†

æ­£åˆ™åŒ–ç±»åˆ«

- L2 æ­£åˆ™åŒ–
  - ä½œç”¨ï¼šå¯ä»¥ä½¿å¾—å…¶ä¸­ä¸€äº› W çš„éƒ½å¾ˆå°ï¼Œéƒ½æ¥è¿‘äº 0ï¼Œå‰Šå¼±æŸä¸ªç‰¹å¾çš„å½±å“
  - ä¼˜ç‚¹ï¼šè¶Šå°çš„å‚æ•°è¯´æ˜æ¨¡å‹è¶Šç®€å•ï¼Œè¶Šç®€å•çš„æ¨¡å‹åˆ™è¶Šä¸å®¹æ˜“äº§ç”Ÿè¿‡æ‹Ÿåˆç°è±¡
  - Ridge å›å½’
- L1æ­£åˆ™åŒ–
  - ä½œç”¨ï¼šå¯ä»¥ä½¿å¾—å…¶ä¸­ä¸€äº›Wçš„å€¼ç›´æ¥ä¸º0ï¼Œåˆ é™¤è¿™ä¸ªç‰¹å¾çš„å½±å“
  - LASSO å›å½’

> çº¿æ€§å›å½’çš„æŸå¤±å‡½æ•°ç”¨æœ€å°äºŒä¹˜æ³•ï¼Œç­‰ä»·äºå½“é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„è¯¯å·®æ»¡è¶³æ­£æ€åˆ†å¸ƒæ—¶çš„æå¤§ä¼¼ç„¶ä¼°è®¡ï¼›å²­å›å½’çš„æŸå¤±å‡½æ•°ï¼Œæ˜¯æœ€å°äºŒä¹˜æ³•+L2èŒƒæ•°ï¼Œç­‰ä»·äºå½“é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„è¯¯å·®æ»¡è¶³æ­£æ€åˆ†å¸ƒï¼Œä¸”æƒé‡å€¼ä¹Ÿæ»¡è¶³æ­£æ€åˆ†å¸ƒï¼ˆå…ˆéªŒåˆ†å¸ƒï¼‰æ—¶çš„æœ€å¤§åéªŒä¼°è®¡ï¼›LASSOçš„æŸå¤±å‡½æ•°ï¼Œæ˜¯æœ€å°äºŒä¹˜æ³•+L1èŒƒæ•°ï¼Œç­‰ä»·äºç­‰ä»·äºå½“é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„è¯¯å·®æ»¡è¶³æ­£æ€åˆ†å¸ƒï¼Œä¸”ä¸”æƒé‡å€¼æ»¡è¶³æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒï¼ˆå…ˆéªŒåˆ†å¸ƒï¼‰æ—¶çš„æœ€å¤§åéªŒä¼°è®¡

### API

#### å¸¸è§„å²­å›å½’

***Ridge(alpha=1.0, fit_intercept=True,solver="auto", normalize=False)***

- å…·æœ‰ L2 æ­£åˆ™åŒ–çš„çº¿æ€§å›å½’

- *alpha*ï¼šæ­£åˆ™åŒ–åŠ›åº¦ï¼Œä¹Ÿå« Î»ï¼Œå³æƒ©ç½šé¡¹ç³»æ•°
  
  - Î»å–å€¼ï¼š0~1 1~10
  
- *solver*ï¼šä¼šæ ¹æ®æ•°æ®è‡ªåŠ¨é€‰æ‹©ä¼˜åŒ–æ–¹æ³•
  
  - *sag*ï¼šå¦‚æœæ•°æ®é›†ã€ç‰¹å¾éƒ½æ¯”è¾ƒå¤§ï¼Œé€‰æ‹©è¯¥éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–
  
- *normalize*ï¼šæ•°æ®æ˜¯å¦è¿›è¡Œæ ‡å‡†åŒ–
  
  - `normalize=False`ï¼šå¯ä»¥åœ¨ fit ä¹‹å‰è°ƒç”¨ preprocessing.StandardScaler æ ‡å‡†åŒ–æ•°æ®
  
- å±æ€§

  - *Ridge.coef_*ï¼šå›å½’æƒé‡

  - *Ridge.intercept_*ï¼šå›å½’åç½®

    

Ridge æ–¹æ³•ç›¸å½“äº `SGDRegressor(penalty='l2', loss="squared_loss")`, åªä¸è¿‡ SGDRegressor å®ç°äº†ä¸€ä¸ªæ™®é€šçš„éšæœºæ¢¯åº¦ä¸‹é™å­¦ä¹ ï¼Œæ¨èä½¿ç”¨Ridge(å®ç°äº†SAG)

#### äº¤å‰éªŒè¯å²­å›å½’

***RidgeCV(_BaseRidgeCV, RegressorMixin)***

- å…·æœ‰ L2 æ­£åˆ™åŒ–çš„çº¿æ€§å›å½’ï¼Œå¯ä»¥è¿›è¡Œäº¤å‰éªŒè¯
- *alphas*ï¼šalpha åˆ—è¡¨
- *cv*ï¼šäº¤å‰éªŒè¯æ¬¡æ•°
- *coef_*ï¼šå›å½’ç³»æ•°

### ä¾‹

```python
from sklearn.linear_model import Ridge, RidgeCV
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error  # å‡æ–¹è¯¯å·®

boston_data = load_boston()
x_train, x_test, y_train, y_test = train_test_split(boston_data.data,
                                                    boston_data.target, random_state=22)

# æ ‡å‡†åŒ–
transfer = StandardScaler()
x_train = transfer.fit_transform(x_train)
x_test = transfer.transform(x_test)

# å²­å›å½’
estimator = Ridge(max_iter=10000, alpha=0.5)  # å²­å›å½’
# estimator = RidgeCV(alphas=[0.1, 0.2, 0.3, 0.5])  # åŠ äº†äº¤å‰éªŒè¯çš„å²­å›å½’
estimator.fit(x_train, y_train)

print("å²­å›å½’_æƒé‡ç³»æ•°ä¸º: ", estimator.coef_)
print("å²­å›å½’_åç½®ä¸º:", estimator.intercept_)

y_predict = estimator.predict(x_test)
error = mean_squared_error(y_test, y_predict)
print("å²­å›å½’_æˆ¿ä»·é¢„æµ‹:", y_predict)
print("å²­å›å½’_å‡åˆ†è¯¯å·®:", error)
```

è¾“å‡º

```python
å²­å›å½’_æƒé‡ç³»æ•°ä¸º:  [-0.64193209  1.13369189 -0.07675643  0.74427624 -1.93681163  2.71424838
 -0.08171268 -3.27871121  2.45697934 -1.81200596 -1.74659067  0.87272606
 -3.90544403]
å²­å›å½’_åç½®ä¸º: 22.62137203166228
å²­å›å½’_æˆ¿ä»·é¢„æµ‹: [28.22536271 31.50554479 21.13191715 32.65799504 20.02127243 19.07245621
 21.10832868 19.61646071 ......15.19441045  3.81755887 29.1743764  20.68219692 22.33163756 28.01411044
 28.55668351]
å²­å›å½’_å‡åˆ†è¯¯å·®: 20.641771606180914
```

# èšç±»

```python
from sklearn.cluster import KMeans
```

K-meansï¼ˆKå‡å€¼èšç±»ï¼‰

- ç‰¹ç‚¹ï¼šé‡‡ç”¨è¿­ä»£å¼ç®—æ³•ï¼Œç›´è§‚æ˜“æ‡‚å¹¶ä¸”éå¸¸å®ç”¨
- ç¼ºç‚¹ï¼šå®¹æ˜“æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜è§£(å¤šæ¬¡èšç±»)

## èšç±»æ­¥éª¤

1. éšæœºè®¾ç½® K ä¸ªç‰¹å¾ç©ºé—´å†…çš„ç‚¹ä½œä¸ºåˆå§‹çš„èšç±»ä¸­å¿ƒ

2. å¯¹äºå…¶ä»–æ¯ä¸ªç‚¹è®¡ç®—åˆ° K ä¸ªä¸­å¿ƒçš„è·ç¦»ï¼ŒæœªçŸ¥çš„ç‚¹é€‰æ‹©æœ€è¿‘çš„ä¸€ä¸ªèšç±»ä¸­å¿ƒç‚¹ä½œä¸ºæ ‡è®°ç±»åˆ«

3. æ¥ç€å¯¹ç€æ ‡è®°çš„èšç±»ä¸­å¿ƒä¹‹åï¼Œé‡æ–°è®¡ç®—å‡ºæ¯ä¸ªèšç±»çš„æ–°ä¸­å¿ƒç‚¹ï¼ˆå¹³å‡å€¼ï¼‰

4. å¦‚æœè®¡ç®—å¾—å‡ºçš„æ–°ä¸­å¿ƒç‚¹ä¸åŸä¸­å¿ƒç‚¹ä¸€æ ·ï¼Œé‚£ä¹ˆç»“æŸï¼Œå¦åˆ™é‡æ–°è¿›è¡Œç¬¬äºŒæ­¥è¿‡ç¨‹

<img src="https://trou.oss-cn-shanghai.aliyuncs.com/img/K-meansè¿‡ç¨‹åˆ†æ.png" alt="K-meansè¿‡ç¨‹åˆ†æ" style="zoom:67%;" />

## API

***KMeans(n_clusters=8, init=â€˜k-means++â€™â€¦)***

- n_clustersï¼šå¼€å§‹çš„èšç±»ä¸­å¿ƒæ•°é‡
- initï¼šåˆå§‹åŒ–æ–¹æ³•ï¼Œé»˜è®¤ä¸º'k-means ++â€™

*KMeans.labels_*ï¼šé»˜è®¤æ ‡è®°çš„ç±»å‹ï¼Œå¯ä»¥å’ŒçœŸå®å€¼æ¯”è¾ƒï¼ˆä¸æ˜¯å€¼æ¯”è¾ƒï¼‰

##  è½®å»“ç³»æ•°

```python
from sklearn.metrics import silhouette_score
```

è½®å»“ç³»æ•°ä½œä¸º Kmeans çš„æ€§èƒ½è¯„ä¼°æŒ‡æ ‡

### å…¬å¼

$$
sc_i=\cfrac{b_i-a_i}{max(b_i,a_i)}
$$

> æ³¨ï¼šå¯¹äºæ¯ä¸ªç‚¹ $i$ ä¸ºå·²èšç±»æ•°æ®ä¸­çš„æ ·æœ¬ ï¼Œ$b_i$ ä¸º $i$ åˆ°å…¶å®ƒæ—ç¾¤çš„æ‰€æœ‰æ ·æœ¬çš„è·ç¦»æœ€å°å€¼ï¼Œ$a_i$ ä¸º $i$ åˆ°æœ¬èº«ç°‡çš„è·ç¦»å¹³å‡å€¼ã€‚æœ€ç»ˆè®¡ç®—å‡ºæ‰€æœ‰çš„æ ·æœ¬ç‚¹çš„è½®å»“ç³»æ•°å¹³å‡å€¼

### è½®å»“ç³»æ•°å€¼åˆ†æ

<img src="https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20210118223723565.png" alt="image-20210118223723565" style="zoom: 60%;" />

**åˆ†æè¿‡ç¨‹**ï¼ˆä»¥ä¸€ä¸ªè“1ç‚¹ä¸ºä¾‹ï¼‰

1. è®¡ç®—å‡ºè“1ç¦»æœ¬èº«æ—ç¾¤æ‰€æœ‰ç‚¹çš„è·ç¦»çš„å¹³å‡å€¼$a_i$

2. è“1åˆ°å…¶å®ƒä¸¤ä¸ªæ—ç¾¤çš„è·ç¦»è®¡ç®—å‡ºå¹³å‡å€¼çº¢å¹³å‡ï¼Œç»¿å¹³å‡ï¼Œå–æœ€å°çš„é‚£ä¸ªè·ç¦»ä½œä¸º$b_i$

3. æ ¹æ®å…¬å¼ï¼šæç«¯å€¼è€ƒè™‘ï¼šå¦‚æœ$b_i >>a_i$ï¼Œé‚£ä¹ˆå…¬å¼ç»“æœè¶‹è¿‘äº1ï¼›å¦‚æœ$a_i>>>b_i$ï¼Œé‚£ä¹ˆå…¬å¼ç»“æœè¶‹è¿‘äº-1

**ç»“è®º**ï¼šå¦‚æœ$b_i>>a_i$ï¼Œè¶‹è¿‘äº1ï¼Œæ•ˆæœå¥½ï¼Œ $b_i<<a_i$ï¼Œè¶‹è¿‘äº-1ï¼Œæ•ˆæœä¸å¥½ã€‚è½®å»“ç³»æ•°çš„å€¼æ˜¯ä»‹äº [-1,1] ï¼Œè¶Šè¶‹è¿‘äº1ä»£è¡¨å†…èšåº¦å’Œåˆ†ç¦»åº¦éƒ½ç›¸å¯¹è¾ƒä¼˜

### API

- ***silhouette_score(X, labels)***ï¼šè®¡ç®—æ‰€æœ‰æ ·æœ¬çš„å¹³å‡è½®å»“ç³»æ•°
  - Xï¼šç‰¹å¾å€¼
  - labelsï¼šè¢«èšç±»æ ‡è®°çš„ç›®æ ‡å€¼ï¼ˆèšç±»ç»“æœï¼‰

## ä¾‹

åˆ†æ

1. é™ç»´ä¹‹åçš„æ•°æ®

2. k-meansèšç±»

3. èšç±»ç»“æœæ˜¾ç¤º
4. ç”¨æˆ·èšç±»ç»“æœè¯„ä¼°

```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.datasets.samples_generator import make_blobs
# Xä¸ºæ ·æœ¬ç‰¹å¾ï¼ŒYä¸ºæ ·æœ¬ç°‡ç±»åˆ«ï¼Œ å…±1000ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬2ä¸ªç‰¹å¾ï¼Œå…±4ä¸ªç°‡ï¼Œç°‡ä¸­å¿ƒåœ¨[-1,-1], [0,0],[1,1], [2,2]ï¼Œ ç°‡æ–¹å·®åˆ†åˆ«ä¸º[0.4, 0.2, 0.2]
X, y = make_blobs(n_samples=1000, n_features=2, centers=[[-1,-1], [0,0], [1,1], [2,2]], cluster_std=[0.4, 0.2, 0.2, 0.2])
plt.scatter(X[:, 0], X[:, 1], marker='o')
plt.show()
```

æ¡ˆä¾‹æ ·æœ¬

<img src="https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20210119103734104.png" alt="image-20210119103734104" style="zoom:67%;" />

```python
from sklearn.cluster import KMeans

#y_pred = KMeans(n_clusters=4).fit_predict(X)
estimator = KMeans(n_clusters=4, init='k-means++')
estimator.fit(X)
y_pred = estimator.predict(X)

plt.scatter(X[:, 0], X[:, 1], c=y_pred)
plt.show()
```

<img src="https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20210119104047678.png" alt="image-20210119104047678" style="zoom:67%;" />

```python
from sklearn.metrics import silhouette_score 

score = silhouette_score(X, y_pred)
print("æ¨¡å‹è½®å»“ç³»æ•°ä¸º(1 æœ€å¥½, -1 æœ€å·®):", score)
```

è¾“å‡º

```python
æ¨¡å‹è½®å»“ç³»æ•°ä¸º(1 æœ€å¥½, -1 æœ€å·®): 0.6634549555891298
```

