# ä¸€ã€ç®€ä»‹

# äºŒã€ç‰¹å¾å·¥ç¨‹

(Feature Engineering)

ç‰¹å¾å·¥ç¨‹æ˜¯ä½¿ç”¨ä¸“ä¸šèƒŒæ™¯çŸ¥è¯†å’ŒæŠ€å·§å¤„ç†æ•°æ®**ï¼Œ**ä½¿å¾—ç‰¹å¾èƒ½åœ¨æœºå™¨å­¦ä¹ ç®—æ³•ä¸Šå‘æŒ¥æ›´å¥½çš„ä½œç”¨çš„è¿‡ç¨‹

## 2.1 æ•°æ®é›†

*scikit-learn* æä¾›äº†ä¸€äº›æ ‡å‡†æ•°æ®é›†

```python
from sklearn import datasets
```

### 2.1.1 å°è§„æ¨¡ load_\*()

***datasets.load_\*()***

è·å–å°è§„æ¨¡æ•°æ®é›†ï¼Œæ•°æ®åŒ…å«åœ¨datasetsé‡Œ

```python
data1 = datasets.load_iris()
data2 = datasets.load_boston()
```

### 2.1.2 å¤§è§„æ¨¡ fetch_\*()

***datasets.fetch_\*(data_home, subset)***

è·å–å¤§è§„æ¨¡æ•°æ®é›†ï¼Œéœ€è¦ä»ç½‘ç»œä¸Šä¸‹è½½

- *data_home*ï¼šæ•°æ®é›†ä¸‹è½½çš„ç›®å½•ï¼Œé»˜è®¤æ˜¯ ~/scikit_learn_data/
- *subset*ï¼šé€‰æ‹©è¦åŠ è½½çš„æ•°æ®é›†ã€‚'train'æˆ–è€…'test'ï¼Œ'all'ï¼Œå¯é€‰

### 2.1.3 è¿”å›å€¼

load å’Œ fetch è¿”å›çš„æ•°æ®ç±»å‹ä¸º *datasets.base.Bunch* (å­—å…¸ç»§æ‰¿)

- *data*ï¼šç‰¹å¾æ•°æ®æ•°ç»„ï¼Œæ˜¯äºŒç»´ numpy.ndarray æ•°ç»„
- *targetï¼š*æ ‡ç­¾æ•°ç»„ï¼Œæ˜¯ n_samples çš„ä¸€ç»´ numpy.ndarray æ•°ç»„
- *DESCR*ï¼šæ•°æ®æè¿°
- *feature_names*ï¼šç‰¹å¾å,æ–°é—»æ•°æ®ï¼Œæ‰‹å†™æ•°å­—ã€å›å½’æ•°æ®é›†æ²¡æœ‰
- *target_names*ï¼šæ ‡ç­¾å

```python
from sklearn.datasets import load_iris
# è·å–é¸¢å°¾èŠ±æ•°æ®é›†
iris = load_iris()
print("é¸¢å°¾èŠ±æ•°æ®é›†çš„è¿”å›å€¼ï¼š\n", iris) # å°†æ‰€æœ‰å‚æ•°å…¨éƒ¨è¿”å›ï¼Œè¿”å›å€¼æ˜¯ä¸€ä¸ªç»§æ‰¿è‡ªå­—å…¸çš„Bench
print("é¸¢å°¾èŠ±çš„ç‰¹å¾å€¼:\n", iris.data)
print("é¸¢å°¾èŠ±çš„ç›®æ ‡å€¼ï¼š\n", iris.target)
print("é¸¢å°¾èŠ±ç‰¹å¾çš„åå­—ï¼š\n", iris.feature_names)
print("é¸¢å°¾èŠ±ç›®æ ‡å€¼çš„åå­—ï¼š\n", iris.target_names)
print("é¸¢å°¾èŠ±çš„æè¿°ï¼š\n", iris.DESCR)

# åŒæ ·å¯ä»¥å†™ä½œè¯¸å¦‚iris['data']çš„æ ¼å¼
```

è¾“å‡º

```pyhton
 é¸¢å°¾èŠ±çš„ç‰¹å¾å€¼:
 [[5.1 3.5 1.4 0.2]
 [4.9 3.  1.4 0.2]
 [4.7 3.2 1.3 0.2]
 [4.6 3.1 1.5 0.2]
 ...
 [6.3 2.5 5.  1.9]
 [6.5 3.  5.2 2. ]
 [6.2 3.4 5.4 2.3]
 [5.9 3.  5.1 1.8]]
    
 é¸¢å°¾èŠ±çš„ç›®æ ‡å€¼ï¼š
 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]
 
 é¸¢å°¾èŠ±ç‰¹å¾çš„åå­—ï¼š
 ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
 
 é¸¢å°¾èŠ±ç›®æ ‡å€¼çš„åå­—ï¼š
 ['setosa' 'versicolor' 'virginica']
 
 é¸¢å°¾èŠ±çš„æè¿°ï¼š
 .. _iris_dataset:

Iris plants dataset
--------------------

**Data Set Characteristics:**

    :Number of Instances: 150 (50 in each of three classes)
 ...
   - Many, many more ...
```

## 2.2 æ•°æ®é›†åˆ’åˆ†

```python
from sklearn.model_selection import train_test_split
```

***sklearn.model_selection.train_test_split( arrays, \*options )***

- xï¼šæ•°æ®é›†çš„ç‰¹å¾å€¼
- yï¼šæ•°æ®é›†çš„æ ‡ç­¾å€¼
- test_sizeï¼šæµ‹è¯•é›†çš„å¤§å°ï¼Œä¸€èˆ¬ä¸ºfloat
- random_stateï¼šéšæœºæ•°ç§å­,ä¸åŒçš„ç§å­ä¼šé€ æˆä¸åŒçš„éšæœºé‡‡æ ·ç»“æœã€‚ç›¸åŒçš„ç§å­é‡‡æ ·ç»“æœç›¸åŒã€‚

è¿”å›ï¼šæµ‹è¯•é›†ç‰¹å¾å€¼ï¼Œæµ‹è¯•é›†æ ‡ç­¾ï¼Œè®­ç»ƒé›†ç‰¹å¾å€¼ï¼Œè®­ç»ƒé›†æ ‡ç­¾ï¼ˆé»˜è®¤éšæœºå–ï¼‰

```python
x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=22)
```

## 2.3 ç‰¹å¾æŠ½å–

ï¼ˆfeature extractionï¼‰

å°†ä»»æ„æ•°æ®ï¼ˆå¦‚æ–‡æœ¬æˆ–å›¾åƒï¼‰è½¬æ¢ä¸ºå¯ç”¨äºæœºå™¨å­¦ä¹ çš„æ•°å­—ç‰¹å¾

```python
import sklearn.feature_extraction
```

### 2.3.1 å­—å…¸ç‰¹å¾æå– DictVectorizer 

å®ƒæ˜¯ä¸€ä¸ªè½¬æ¢å™¨ï¼Œåº”ç”¨æ—¶éœ€è¦è¿›è¡Œå®ä¾‹åŒ–

#### 1ï¼‰API

***sklearn.feature_extraction.DictVectorizer(sparse=True,â€¦)***

- *DictVectorizer.fit_transform(X)*
  - Xï¼šå­—å…¸æˆ–è€…åŒ…å«å­—å…¸çš„è¿­ä»£å™¨
  - è¿”å›sparseçŸ©é˜µæˆ–arrayæ•°ç»„
- *DictVectorizer.inverse_transform(X)*
  - Xï¼šarrayæ•°ç»„æˆ–è€…sparseçŸ©é˜µ 
  - è¿”å›è½¬æ¢ä¹‹å‰æ•°æ®æ ¼å¼
- *DictVectorizer.get_feature_names()* 
  - è¿”å›ç±»åˆ«åç§°

#### 2ï¼‰ä¾‹

æµç¨‹åˆ†æ

- å®ä¾‹åŒ–ç±»DictVectorizer
- è°ƒç”¨fit_transformæ–¹æ³•è¾“å…¥æ•°æ®å¹¶è½¬æ¢ï¼ˆæ³¨æ„è¿”å›æ ¼å¼ï¼‰

```python
from sklearn.feature_extraction import DictVectorizer

data = [{'city': 'åŒ—äº¬','temperature':100}, {'city': 'ä¸Šæµ·','temperature':60}, {'city': 'æ·±åœ³','temperature':30}]

# 1ã€å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
transfer = DictVectorizer(sparse=False)# æ‹’ç»è¿”å›ç¨€ç–çŸ©é˜µ
# 2ã€è°ƒç”¨fit_transform
data = transfer.fit_transform(data)

print("è¿”å›çš„ç»“æœ:\n", data)
print("ç‰¹å¾åå­—ï¼š\n", transfer.get_feature_names())
```

è¾“å‡º

```python
è¿”å›çš„ç»“æœ:
[[  0.   1.   0. 100.]
 [  1.   0.   0.  60.]
 [  0.   0.   1.  30.]]
ç‰¹å¾åå­—ï¼š
 ['city=ä¸Šæµ·', 'city=åŒ—äº¬', 'city=æ·±åœ³', 'temperature']
```

è‹¥è¿”å›ç¨€ç–çŸ©é˜µï¼Œæ”¹sparse=True

```python
è¿”å›çš„ç»“æœ:
(0, 1)	1.0
(0, 3)	100.0
(1, 0)	1.0
(1, 3)	60.0
(2, 2)	1.0
(2, 3)	30.0
ç‰¹å¾åå­—ï¼š
 ['city=ä¸Šæµ·', 'city=åŒ—äº¬', 'city=æ·±åœ³', 'temperature']
```

è¿™ä¸ªå¤„ç†æ•°æ®çš„æŠ€å·§å«åš *one-hot* ç¼–ç 

### 2.3.2 æ–‡æœ¬è¯é¢‘ç‰¹å¾æå– text.CountVectorizer

å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œè¯é¢‘ç‰¹å¾å€¼åŒ–

å®ƒæ˜¯ä¸€ä¸ªè½¬æ¢å™¨ï¼Œåº”ç”¨æ—¶éœ€è¦è¿›è¡Œå®ä¾‹åŒ–

#### 1ï¼‰API

***sklearn.feature_extraction.text.CountVectorizer(stop_words=[])***

- CountVectorizer.fit_transform(X)
  -  Xï¼šæ–‡æœ¬æˆ–è€…åŒ…å«æ–‡æœ¬å­—ç¬¦ä¸²çš„å¯è¿­ä»£å¯¹è±¡
  - è¿”å›sparseçŸ©é˜µ
- CountVectorizer.inverse_transform(X)
  -  Xï¼šarrayæ•°ç»„æˆ–è€…sparseçŸ©é˜µ 
  - è¿”å›è½¬æ¢ä¹‹å‰æ•°æ®æ ¼
- CountVectorizer.get_feature_names() 
  - è¿”å›å€¼å•è¯åˆ—è¡¨

#### 2ï¼‰ä¾‹

æµç¨‹åˆ†æ

- å®ä¾‹åŒ–ç±»CountVectorizer
- è°ƒç”¨fit_transformæ–¹æ³•è¾“å…¥æ•°æ®å¹¶è½¬æ¢ ï¼ˆæ³¨æ„è¿”å›æ ¼å¼ï¼Œåˆ©ç”¨toarray()è¿›è¡ŒsparseçŸ©é˜µè½¬æ¢arrayæ•°ç»„ï¼‰

```python
from sklearn.feature_extraction.text import CountVectorizer

data = ["life is short,i like like python",
        "life is too long,i dislike python"]

# 1ã€å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
transfer = CountVectorizer()
# 2ã€è°ƒç”¨fit_transform
data = transfer.fit_transform(data)

print("æ–‡æœ¬ç‰¹å¾æŠ½å–çš„ç»“æœï¼š\n", data.toarray())
print("è¿”å›ç‰¹å¾åå­—ï¼š\n", transfer.get_feature_names())
```

è¾“å‡ºï¼ˆå› ä¸ºæ²¡æœ‰ sparse å‚æ•°ï¼Œè‹¥è¦è½¬æ¢æˆäºŒç»´æ•°ç»„å½¢å¼ï¼Œéœ€è¦åˆ©ç”¨toarray()ï¼‰

```python
æ–‡æœ¬ç‰¹å¾æŠ½å–çš„ç»“æœï¼š
 [[0 1 1 2 0 1 1 0]
 [1 1 1 0 1 1 0 1]]
è¿”å›ç‰¹å¾åå­—ï¼š
 ['dislike', 'is', 'life', 'like', 'long', 'python', 'short', 'too']
```

è‹¥ç›´æ¥è¿”å›ç¨€ç–çŸ©é˜µ

```python
æ–‡æœ¬ç‰¹å¾æŠ½å–çš„ç»“æœï¼š
(0, 2)	1
(0, 1)	1
(0, 6)	1
(0, 3)	2
(0, 5)	1
(1, 2)	1
(1, 1)	1
(1, 5)	1
(1, 7)	1
(1, 4)	1
(1, 0)	1
è¿”å›ç‰¹å¾åå­—ï¼š
 ['dislike', 'is', 'life', 'like', 'long', 'python', 'short', 'too']
```

#### 3ï¼‰ä¸­æ–‡å¤„ç†

ä½¿ç”¨jiebaåˆ†è¯åº“

***jieba.cut()***

- è¿”å›è¯è¯­ç»„æˆçš„ç”Ÿæˆå™¨

åˆ†æ

- å‡†å¤‡å¥å­ï¼Œåˆ©ç”¨jieba.cutè¿›è¡Œåˆ†è¯
- å®ä¾‹åŒ–CountVectorizer
- å°†åˆ†è¯ç»“æœå˜æˆå­—ç¬¦ä¸²å½“ä½œfit_transformçš„è¾“å…¥å€¼

```python
from sklearn.feature_extraction.text import CountVectorizer
import jieba


def cut_word(text):
    # ç”¨ç»“å·´å¯¹ä¸­æ–‡å­—ç¬¦ä¸²è¿›è¡Œåˆ†è¯
    text = " ".join(list(jieba.cut(text)))
    return text


data = ["ä¸€ç§è¿˜æ˜¯ä¸€ç§ä»Šå¤©å¾ˆæ®‹é…·ï¼Œæ˜å¤©æ›´æ®‹é…·ï¼Œåå¤©å¾ˆç¾å¥½ï¼Œä½†ç»å¯¹å¤§éƒ¨åˆ†æ˜¯æ­»åœ¨æ˜å¤©æ™šä¸Šï¼Œæ‰€ä»¥æ¯ä¸ªäººä¸è¦æ”¾å¼ƒä»Šå¤©ã€‚",
        "æˆ‘ä»¬çœ‹åˆ°çš„ä»å¾ˆè¿œæ˜Ÿç³»æ¥çš„å…‰æ˜¯åœ¨å‡ ç™¾ä¸‡å¹´ä¹‹å‰å‘å‡ºçš„ï¼Œè¿™æ ·å½“æˆ‘ä»¬çœ‹åˆ°å®‡å®™æ—¶ï¼Œæˆ‘ä»¬æ˜¯åœ¨çœ‹å®ƒçš„è¿‡å»ã€‚",
        "å¦‚æœåªç”¨ä¸€ç§æ–¹å¼äº†è§£æŸæ ·äº‹ç‰©ï¼Œä½ å°±ä¸ä¼šçœŸæ­£äº†è§£å®ƒã€‚äº†è§£äº‹ç‰©çœŸæ­£å«ä¹‰çš„ç§˜å¯†å–å†³äºå¦‚ä½•å°†å…¶ä¸æˆ‘ä»¬æ‰€äº†è§£çš„äº‹ç‰©ç›¸è”ç³»ã€‚"]

# å°†åŸå§‹æ•°æ®è½¬æ¢æˆåˆ†å¥½è¯çš„å½¢å¼
text_list = []
for sent in data:
    text_list.append(cut_word(sent))
print(text_list)

# 1ã€å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
transfer = CountVectorizer()
# 2ã€è°ƒç”¨fit_transform
data = transfer.fit_transform(text_list)

print("æ–‡æœ¬ç‰¹å¾æŠ½å–çš„ç»“æœï¼š\n", data.toarray())
print("è¿”å›ç‰¹å¾åå­—ï¼š\n", transfer.get_feature_names())
```

è¾“å‡º

```python

['ä¸€ç§ è¿˜æ˜¯ ä¸€ç§ ä»Šå¤© å¾ˆ æ®‹é…· ï¼Œ æ˜å¤© æ›´ æ®‹é…· ï¼Œ åå¤© å¾ˆ ç¾å¥½ ï¼Œ ä½† ç»å¯¹ å¤§éƒ¨åˆ† æ˜¯ æ­» åœ¨ æ˜å¤© æ™šä¸Š ï¼Œ æ‰€ä»¥ æ¯ä¸ª äºº ä¸è¦ æ”¾å¼ƒ ä»Šå¤© ã€‚', 'æˆ‘ä»¬ çœ‹åˆ° çš„ ä» å¾ˆ è¿œ æ˜Ÿç³» æ¥ çš„ å…‰æ˜¯åœ¨ å‡ ç™¾ä¸‡å¹´ ä¹‹å‰ å‘å‡º çš„ ï¼Œ è¿™æ · å½“ æˆ‘ä»¬ çœ‹åˆ° å®‡å®™ æ—¶ ï¼Œ æˆ‘ä»¬ æ˜¯ åœ¨ çœ‹ å®ƒ çš„ è¿‡å» ã€‚', 'å¦‚æœ åªç”¨ ä¸€ç§ æ–¹å¼ äº†è§£ æŸæ · äº‹ç‰© ï¼Œ ä½  å°± ä¸ä¼š çœŸæ­£ äº†è§£ å®ƒ ã€‚ äº†è§£ äº‹ç‰© çœŸæ­£ å«ä¹‰ çš„ ç§˜å¯† å–å†³äº å¦‚ä½• å°† å…¶ ä¸ æˆ‘ä»¬ æ‰€ äº†è§£ çš„ äº‹ç‰© ç›¸ è”ç³» ã€‚']

æ–‡æœ¬ç‰¹å¾æŠ½å–çš„ç»“æœï¼š
 [[2 0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 1
  0]
 [0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 0
  1]
 [1 1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0
  0]]
è¿”å›ç‰¹å¾åå­—ï¼š
 ['ä¸€ç§', 'ä¸ä¼š', 'ä¸è¦', 'ä¹‹å‰', 'äº†è§£', 'äº‹ç‰©', 'ä»Šå¤©', 'å…‰æ˜¯åœ¨', 'å‡ ç™¾ä¸‡å¹´', 'å‘å‡º', 'å–å†³äº', 'åªç”¨', 'åå¤©', 'å«ä¹‰', 'å¤§éƒ¨åˆ†', 'å¦‚ä½•', 'å¦‚æœ', 'å®‡å®™', 'æˆ‘ä»¬', 'æ‰€ä»¥', 'æ”¾å¼ƒ', 'æ–¹å¼', 'æ˜å¤©', 'æ˜Ÿç³»', 'æ™šä¸Š', 'æŸæ ·', 'æ®‹é…·', 'æ¯ä¸ª', 'çœ‹åˆ°', 'çœŸæ­£', 'ç§˜å¯†', 'ç»å¯¹', 'ç¾å¥½', 'è”ç³»', 'è¿‡å»', 'è¿˜æ˜¯', 'è¿™æ ·']
```

### 2.3.3 Tf-idfæ–‡æœ¬ç‰¹å¾æå– text.TfidfVectorizer

TF-IDFçš„ä¸»è¦æ€æƒ³æ˜¯ï¼šå¦‚æœæŸä¸ªè¯æˆ–çŸ­è¯­åœ¨ä¸€ç¯‡æ–‡ç« ä¸­å‡ºç°çš„æ¦‚ç‡é«˜ï¼Œå¹¶ä¸”åœ¨å…¶ä»–æ–‡ç« ä¸­å¾ˆå°‘å‡ºç°ï¼Œåˆ™è®¤ä¸ºæ­¤è¯æˆ–è€…çŸ­è¯­å…·æœ‰å¾ˆå¥½çš„ç±»åˆ«åŒºåˆ†èƒ½åŠ›ï¼Œé€‚åˆç”¨æ¥åˆ†ç±»ã€‚

TF-IDFä½œç”¨ï¼šç”¨ä»¥è¯„ä¼°ä¸€å­—è¯å¯¹äºä¸€ä¸ªæ–‡ä»¶é›†æˆ–ä¸€ä¸ªè¯­æ–™åº“ä¸­çš„å…¶ä¸­ä¸€ä»½æ–‡ä»¶çš„é‡è¦ç¨‹åº¦ã€‚

#### 1ï¼‰å…¬å¼

- è¯é¢‘ï¼ˆterm frequencyï¼Œtfï¼‰æŒ‡çš„æ˜¯æŸä¸€ä¸ªç»™å®šçš„è¯è¯­åœ¨è¯¥æ–‡ä»¶ä¸­å‡ºç°çš„é¢‘ç‡
- é€†å‘æ–‡æ¡£é¢‘ç‡ï¼ˆinverse document frequencyï¼Œidfï¼‰æ˜¯ä¸€ä¸ªè¯è¯­æ™®éé‡è¦æ€§çš„åº¦é‡ã€‚ç”±æ€»æ–‡ä»¶æ•°ç›®é™¤ä»¥åŒ…å«è¯¥è¯è¯­ä¹‹æ–‡ä»¶çš„æ•°ç›®ï¼Œå†å°†å¾—åˆ°çš„å•†å–ä»¥10ä¸ºåº•çš„å¯¹æ•°å¾—åˆ°

$$
tfidf_{i,j}=tf_{i,j}\times idf_i
$$

>  å‡å¦‚ä¸€ç¯‡æ–‡ä»¶çš„æ€»è¯è¯­æ•°æ˜¯100ä¸ªï¼Œè€Œè¯è¯­"éå¸¸"å‡ºç°äº†5æ¬¡ï¼Œé‚£ä¹ˆ"éå¸¸"ä¸€è¯åœ¨è¯¥æ–‡ä»¶ä¸­çš„è¯é¢‘å°±æ˜¯5/100=0.05ã€‚è€Œè®¡ç®—æ–‡ä»¶é¢‘ç‡ï¼ˆIDFï¼‰çš„æ–¹æ³•æ˜¯ä»¥æ–‡ä»¶é›†çš„æ–‡ä»¶æ€»æ•°ï¼Œé™¤ä»¥å‡ºç°"éå¸¸"ä¸€è¯çš„æ–‡ä»¶æ•°ã€‚æ‰€ä»¥ï¼Œå¦‚æœ"éå¸¸"ä¸€è¯åœ¨1,000ä»½æ–‡ä»¶å‡ºç°è¿‡ï¼Œè€Œæ–‡ä»¶æ€»æ•°æ˜¯10,000,000ä»½çš„è¯ï¼Œå…¶é€†å‘æ–‡ä»¶é¢‘ç‡å°±æ˜¯lgï¼ˆ10,000,000 / 1,0000ï¼‰=3ã€‚æœ€å"éå¸¸"å¯¹äºè¿™ç¯‡æ–‡æ¡£çš„tf-idfçš„åˆ†æ•°ä¸º0.05 * 3=0.15

#### 2ï¼‰API

***sklearn.feature_extraction.text.TfidfVectorizer(stop_words=[])***

- TfidfVectorizer.fit_transform(X)
  -  Xï¼šæ–‡æœ¬æˆ–è€…åŒ…å«æ–‡æœ¬å­—ç¬¦ä¸²çš„å¯è¿­ä»£å¯¹è±¡
  - è¿”å›sparseçŸ©é˜µ
- TfidfVectorizer.inverse_transform(X)
  -  Xï¼šarrayæ•°ç»„æˆ–è€…sparseçŸ©é˜µ 
  - è¿”å›è½¬æ¢ä¹‹å‰æ•°æ®æ ¼
- TfidfVectorizer.get_feature_names() 
  - è¿”å›å€¼å•è¯åˆ—è¡¨

#### 3ï¼‰ä¾‹

```python
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba

def cut_word(text):
    # ç”¨ç»“å·´å¯¹ä¸­æ–‡å­—ç¬¦ä¸²è¿›è¡Œåˆ†è¯
    text = " ".join(list(jieba.cut(text)))
    return text

data = ["ä¸€ç§è¿˜æ˜¯ä¸€ç§ä»Šå¤©å¾ˆæ®‹é…·ï¼Œæ˜å¤©æ›´æ®‹é…·ï¼Œåå¤©å¾ˆç¾å¥½ï¼Œä½†ç»å¯¹å¤§éƒ¨åˆ†æ˜¯æ­»åœ¨æ˜å¤©æ™šä¸Šï¼Œæ‰€ä»¥æ¯ä¸ªäººä¸è¦æ”¾å¼ƒä»Šå¤©ã€‚",
        "æˆ‘ä»¬çœ‹åˆ°çš„ä»å¾ˆè¿œæ˜Ÿç³»æ¥çš„å…‰æ˜¯åœ¨å‡ ç™¾ä¸‡å¹´ä¹‹å‰å‘å‡ºçš„ï¼Œè¿™æ ·å½“æˆ‘ä»¬çœ‹åˆ°å®‡å®™æ—¶ï¼Œæˆ‘ä»¬æ˜¯åœ¨çœ‹å®ƒçš„è¿‡å»ã€‚",
        "å¦‚æœåªç”¨ä¸€ç§æ–¹å¼äº†è§£æŸæ ·äº‹ç‰©ï¼Œä½ å°±ä¸ä¼šçœŸæ­£äº†è§£å®ƒã€‚äº†è§£äº‹ç‰©çœŸæ­£å«ä¹‰çš„ç§˜å¯†å–å†³äºå¦‚ä½•å°†å…¶ä¸æˆ‘ä»¬æ‰€äº†è§£çš„äº‹ç‰©ç›¸è”ç³»ã€‚"]

# å°†åŸå§‹æ•°æ®è½¬æ¢æˆåˆ†å¥½è¯çš„å½¢å¼
text_list = []
for sent in data:
    text_list.append(cut_word(sent))
print(text_list)

# 1ã€å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
transfer = TfidfVectorizer(stop_words=['ä¸€ç§', 'ä¸ä¼š', 'ä¸è¦'])
# 2ã€è°ƒç”¨fit_transform
data = transfer.fit_transform(text_list)

print("æ–‡æœ¬ç‰¹å¾æŠ½å–çš„ç»“æœï¼š\n", data.toarray())
print("è¿”å›ç‰¹å¾åå­—ï¼š\n", transfer.get_feature_names())
```

è¾“å‡º

```python
['ä¸€ç§ è¿˜æ˜¯ ä¸€ç§ ä»Šå¤© å¾ˆ æ®‹é…· ï¼Œ æ˜å¤© æ›´ æ®‹é…· ï¼Œ åå¤© å¾ˆ ç¾å¥½ ï¼Œ ä½† ç»å¯¹ å¤§éƒ¨åˆ† æ˜¯ æ­» åœ¨ æ˜å¤© æ™šä¸Š ï¼Œ æ‰€ä»¥ æ¯ä¸ª äºº ä¸è¦ æ”¾å¼ƒ ä»Šå¤© ã€‚', 'æˆ‘ä»¬ çœ‹åˆ° çš„ ä» å¾ˆ è¿œ æ˜Ÿç³» æ¥ çš„ å…‰æ˜¯åœ¨ å‡ ç™¾ä¸‡å¹´ ä¹‹å‰ å‘å‡º çš„ ï¼Œ è¿™æ · å½“ æˆ‘ä»¬ çœ‹åˆ° å®‡å®™ æ—¶ ï¼Œ æˆ‘ä»¬ æ˜¯ åœ¨ çœ‹ å®ƒ çš„ è¿‡å» ã€‚', 'å¦‚æœ åªç”¨ ä¸€ç§ æ–¹å¼ äº†è§£ æŸæ · äº‹ç‰© ï¼Œ ä½  å°± ä¸ä¼š çœŸæ­£ äº†è§£ å®ƒ ã€‚ äº†è§£ äº‹ç‰© çœŸæ­£ å«ä¹‰ çš„ ç§˜å¯† å–å†³äº å¦‚ä½• å°† å…¶ ä¸ æˆ‘ä»¬ æ‰€ äº†è§£ çš„ äº‹ç‰© ç›¸ è”ç³» ã€‚']

æ–‡æœ¬ç‰¹å¾æŠ½å–çš„ç»“æœï¼š
 [[0.         0.         0.         0.43643578 0.         0.
  0.         0.         0.         0.21821789 0.         0.21821789
  0.         0.         0.         0.         0.21821789 0.21821789
  0.         0.43643578 0.         0.21821789 0.         0.43643578
  0.21821789 0.         0.         0.         0.21821789 0.21821789
  0.         0.         0.21821789 0.        ]
 [0.2410822  0.         0.         0.         0.2410822  0.2410822
  0.2410822  0.         0.         0.         0.         0.
  0.         0.         0.2410822  0.55004769 0.         0.
  0.         0.         0.2410822  0.         0.         0.
  0.         0.48216441 0.         0.         0.         0.
  0.         0.2410822  0.         0.2410822 ]
 [0.         0.644003   0.48300225 0.         0.         0.
  0.         0.16100075 0.16100075 0.         0.16100075 0.
  0.16100075 0.16100075 0.         0.12244522 0.         0.
  0.16100075 0.         0.         0.         0.16100075 0.
  0.         0.         0.3220015  0.16100075 0.         0.
  0.16100075 0.         0.         0.        ]]
è¿”å›ç‰¹å¾åå­—ï¼š
 ['ä¹‹å‰', 'äº†è§£', 'äº‹ç‰©', 'ä»Šå¤©', 'å…‰æ˜¯åœ¨', 'å‡ ç™¾ä¸‡å¹´', 'å‘å‡º', 'å–å†³äº', 'åªç”¨', 'åå¤©', 'å«ä¹‰', 'å¤§éƒ¨åˆ†', 'å¦‚ä½•', 'å¦‚æœ', 'å®‡å®™', 'æˆ‘ä»¬', 'æ‰€ä»¥', 'æ”¾å¼ƒ', 'æ–¹å¼', 'æ˜å¤©', 'æ˜Ÿç³»', 'æ™šä¸Š', 'æŸæ ·', 'æ®‹é…·', 'æ¯ä¸ª', 'çœ‹åˆ°', 'çœŸæ­£', 'ç§˜å¯†', 'ç»å¯¹', 'ç¾å¥½', 'è”ç³»', 'è¿‡å»', 'è¿˜æ˜¯', 'è¿™æ ·']
```

## 2.4 ç‰¹å¾é¢„å¤„ç†

ï¼ˆfeature preprocessingï¼‰

é€šè¿‡ä¸€äº›è½¬æ¢å‡½æ•°å°†ç‰¹å¾æ•°æ®è½¬æ¢æˆæ›´åŠ é€‚åˆç®—æ³•æ¨¡å‹çš„ç‰¹å¾æ•°æ®è¿‡ç¨‹

æ•°æ®çš„æ— é‡çº²å¤„ç†ï¼š**ä½¿ä¸åŒè§„æ ¼çš„æ•°æ®è½¬æ¢åˆ°åŒä¸€è§„æ ¼**

- å½’ä¸€åŒ–
- æ ‡å‡†åŒ–

ç‰¹å¾çš„å•ä½æˆ–è€…å¤§å°ç›¸å·®è¾ƒå¤§ï¼Œæˆ–è€…æŸç‰¹å¾çš„æ–¹å·®ç›¸æ¯”å…¶ä»–çš„ç‰¹å¾è¦å¤§å‡ºå‡ ä¸ªæ•°é‡çº§**ï¼Œ**å®¹æ˜“å½±å“ï¼ˆæ”¯é…ï¼‰ç›®æ ‡ç»“æœï¼Œä½¿å¾—ä¸€äº›ç®—æ³•æ— æ³•å­¦ä¹ åˆ°å…¶å®ƒçš„ç‰¹å¾ï¼Œæ‰€ä»¥è¦è¿›è¡Œå½’ä¸€åŒ–/æ ‡å‡†åŒ–ã€‚

```python
import sklearn.preprocessing
```

### 2.4.1 å½’ä¸€åŒ–

é€šè¿‡å¯¹åŸå§‹æ•°æ®è¿›è¡Œå˜æ¢æŠŠæ•°æ®æ˜ å°„åˆ°ï¼ˆé»˜è®¤ä¸ºï¼‰[0,1] ä¹‹é—´

#### 1ï¼‰å…¬å¼

$$
X'=\cfrac{x-min}{max-min}\\
X''=X'*(mx-mi)+mi
$$

>  ä½œç”¨äºæ¯ä¸€åˆ—ï¼Œmax ä¸ºä¸€åˆ—çš„æœ€å¤§å€¼ï¼Œmin ä¸ºä¸€åˆ—çš„æœ€å°å€¼ï¼ŒXâ€™â€™ä¸ºæœ€ç»ˆç»“æœï¼Œmxï¼Œmiåˆ†åˆ«ä¸ºæŒ‡å®šåŒºé—´å€¼ï¼Œé»˜è®¤mxä¸º1ï¼Œmiä¸º0

#### 2ï¼‰API

***sklearn.preprocessing.MinMaxScaler (feature_range=(0,1)â€¦ )***

- *MinMaxScalar.fit_transform(X)*
  - Xï¼šnumpy arrayæ ¼å¼çš„æ•°æ® [n_samples,n_features]
  - è¿”å›å€¼ï¼šè½¬æ¢åçš„å½¢çŠ¶ç›¸åŒçš„array

#### 3ï¼‰ä¾‹

ä»¥ä¸‹ä¸ºæ•°æ®å®ä¾‹

<img src="https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20210118192214857.png" alt="image-20210118192214857" style="zoom:80%;" />

- å®ä¾‹åŒ–MinMaxScalar

- é€šè¿‡fit_transformè½¬æ¢

```python
from sklearn.preprocessing import MinMaxScaler
import pandas as pd

path = "../Data/Dating.txt"
data = pd.read_csv(path)

# 1ã€å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
transfer = MinMaxScaler(feature_range=(0, 1)) # é»˜è®¤ MIN=0, MAX=1
# 2ã€è°ƒç”¨fit_transform
data = transfer.fit_transform(data[['milage','Liters','Consumtime']]) # éœ€è¦ä¼ numpy arrayæ ¼å¼, è¿”å›array

print("æœ€å°å€¼æœ€å¤§å€¼å½’ä¸€åŒ–å¤„ç†çš„ç»“æœï¼š\n", data)
```

è¾“å‡º

```python
æœ€å°å€¼æœ€å¤§å€¼å½’ä¸€åŒ–å¤„ç†çš„ç»“æœï¼š
 [[0.43582641 0.58819286 0.53237967]
 [0.         0.48794044 1.        ]
 [0.19067405 0.         0.43571351]
 [1.         1.         0.19139157]
 [0.3933518  0.01947089 0.        ]]
```

æ³¨æ„æœ€å¤§å€¼æœ€å°å€¼æ˜¯å˜åŒ–çš„ï¼Œå¦å¤–ï¼Œæœ€å¤§å€¼ä¸æœ€å°å€¼éå¸¸å®¹æ˜“å—å¼‚å¸¸ç‚¹å½±å“ï¼Œæ‰€ä»¥è¿™ç§æ–¹æ³•é²æ£’æ€§è¾ƒå·®ï¼Œåªé€‚åˆä¼ ç»Ÿç²¾ç¡®å°æ•°æ®åœºæ™¯

### 2.4.2 æ ‡å‡†åŒ–

é€šè¿‡å¯¹åŸå§‹æ•°æ®è¿›è¡Œå˜æ¢æŠŠæ•°æ®å˜æ¢åˆ°å‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1çš„èŒƒå›´å†…

ä¼˜åŠ¿

- å¯¹äºå½’ä¸€åŒ–æ¥è¯´ï¼šå¦‚æœå‡ºç°å¼‚å¸¸ç‚¹ï¼Œå½±å“äº†æœ€å¤§å€¼å’Œæœ€å°å€¼ï¼Œé‚£ä¹ˆç»“æœæ˜¾ç„¶ä¼šå‘ç”Ÿæ”¹å˜
- å¯¹äºæ ‡å‡†åŒ–æ¥è¯´ï¼šå¦‚æœå‡ºç°å¼‚å¸¸ç‚¹ï¼Œç”±äºå…·æœ‰ä¸€å®šæ•°æ®é‡ï¼Œå°‘é‡çš„å¼‚å¸¸ç‚¹å¯¹äºå¹³å‡å€¼çš„å½±å“å¹¶ä¸å¤§ï¼Œä»è€Œæ–¹å·®æ”¹å˜è¾ƒå°

#### 1ï¼‰å…¬å¼

$$
X'=\cfrac{x-mean}{\sigma}
$$

> ä½œç”¨äºæ¯ä¸€åˆ—ï¼Œmean ä¸ºå¹³å‡å€¼ï¼ŒÏƒ ä¸ºæ ‡å‡†å·®

#### 2ï¼‰API

***sklearn.preprocessing.StandardScaler( )***

- *StandardScaler.fit_transform(X)*
  - Xï¼šnumpy array æ ¼å¼çš„æ•°æ®[n_samples, n_features]
  - è¿”å›å€¼ï¼šè½¬æ¢åçš„å½¢çŠ¶ç›¸åŒçš„array
- *StandardScaler.mean_*
  - è¿”å›å€¼ï¼šæ¯ä¸€åˆ—ç‰¹å¾çš„å¹³å‡å€¼
- *StandardScaler.var_*
  - è¿”å›å€¼ï¼šæ¯ä¸€åˆ—ç‰¹å¾çš„æ–¹å·®

#### 3ï¼‰ä¾‹

åŒæ ·å¯¹ 2.4.1 çš„æ•°æ®è¿›è¡Œå¤„ç†

- å®ä¾‹åŒ–MinMaxScalar

- é€šè¿‡fit_transformè½¬æ¢

```python
import pandas as pd
from sklearn.preprocessing import StandardScaler

data = pd.read_csv(path)

# 1ã€å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
transfer = StandardScaler() # å€¼éƒ½åœ¨0é™„è¿‘,æ‰€ä»¥æœ‰è´Ÿæ•°æ˜¯æ­£å¸¸çš„
# 2ã€è°ƒç”¨fit_transform
data = transfer.fit_transform(data[['milage','Liters','Consumtime']]) 

print("æ ‡å‡†åŒ–çš„ç»“æœ:\n", data)
print("æ¯ä¸€åˆ—ç‰¹å¾çš„å¹³å‡å€¼ï¼š\n", transfer.mean_)
print("æ¯ä¸€åˆ—ç‰¹å¾çš„æ–¹å·®ï¼š\n", transfer.var_)
```

è¾“å‡º

```python
æ ‡å‡†åŒ–çš„ç»“æœ:
[[ 0.0947602   0.44990013  0.29573441]
 [-1.20166916  0.18312874  1.67200507]
 [-0.63448132 -1.11527928  0.01123265]
 [ 1.77297701  1.54571769 -0.70784025]
 [-0.03158673 -1.06346729 -1.27113187]]
æ¯ä¸€åˆ—ç‰¹å¾çš„å¹³å‡å€¼ï¼š
 [3.8988000e+04 6.3478996e+00 7.9924800e-01]
æ¯ä¸€åˆ—ç‰¹å¾çš„æ–¹å·®ï¼š
 [4.15683072e+08 1.93505309e+01 2.73652475e-01]
```

## 2.5 ç‰¹å¾é™ç»´

ï¼ˆFeature Dimension Reduceï¼‰

é™ç»´æ˜¯æŒ‡åœ¨æŸäº›é™å®šæ¡ä»¶ä¸‹ï¼Œé™ä½éšæœºå˜é‡ï¼ˆç‰¹å¾ï¼‰ä¸ªæ•°ï¼Œå¾—åˆ°ä¸€ç»„â€œä¸ç›¸å…³â€ä¸»å˜é‡çš„è¿‡ç¨‹

ä¸¤ç§æ–¹å¼

- ç‰¹å¾é€‰æ‹©
- ä¸»æˆåˆ†åˆ†æï¼ˆå¯ä»¥ç†è§£ä¸€ç§ç‰¹å¾æå–çš„æ–¹å¼ï¼‰

### 2.5.1 ç‰¹å¾é€‰æ‹©

```python
import sklearn.feature_selection
```

æ•°æ®ä¸­åŒ…å«å†—ä½™æˆ–æ— å…³å˜é‡ï¼ˆæˆ–ç§°ç‰¹å¾ã€å±æ€§ã€æŒ‡æ ‡ç­‰ï¼‰ï¼Œæ—¨åœ¨ä»åŸæœ‰ç‰¹å¾ä¸­æ‰¾å‡ºä¸»è¦ç‰¹å¾

æ–¹æ³•

- è¿‡æ»¤å¼ï¼ˆFilterï¼‰ï¼šä¸»è¦æ¢ç©¶ç‰¹å¾æœ¬èº«ç‰¹ç‚¹ã€ç‰¹å¾ä¸ç‰¹å¾å’Œç›®æ ‡å€¼ä¹‹é—´å…³è”
  - æ–¹å·®é€‰æ‹©æ³•ï¼šä½æ–¹å·®ç‰¹å¾è¿‡æ»¤
  - ç›¸å…³ç³»æ•°
- åµŒå…¥å¼ï¼ˆEmbeddedï¼‰ï¼šç®—æ³•è‡ªåŠ¨é€‰æ‹©ç‰¹å¾ï¼ˆç‰¹å¾ä¸ç›®æ ‡å€¼ä¹‹é—´çš„å…³è”ï¼‰
  - å†³ç­–æ ‘ï¼šä¿¡æ¯ç†µã€ä¿¡æ¯å¢ç›Š
  - æ­£åˆ™åŒ–ï¼šL1ã€L2
  - æ·±åº¦å­¦ä¹ ï¼šå·ç§¯ç­‰

#### 1ï¼‰ä½æ–¹å·®ç‰¹å¾è¿‡æ»¤

åˆ é™¤ä½æ–¹å·®çš„ä¸€äº›ç‰¹å¾

##### API

***sklearn.feature_selection.VarianceThreshold(threshold = 0.0)***

- *Variance.fit_transform(X)*
  - Xï¼šnumpy array æ ¼å¼çš„æ•°æ® [n_samples, n_features]
  - è¿”å›å€¼ï¼šè®­ç»ƒé›†å·®å¼‚ä½äº threshold çš„ç‰¹å¾å°†è¢«åˆ é™¤ã€‚é»˜è®¤å€¼æ˜¯ä¿ç•™æ‰€æœ‰éé›¶æ–¹å·®ç‰¹å¾ï¼Œå³åˆ é™¤æ‰€æœ‰æ ·æœ¬ä¸­å…·æœ‰ç›¸åŒå€¼çš„ç‰¹å¾ã€‚

##### ä¾‹

å¤„ç†ä»¥ä¸‹ä¾‹å­

<img src="https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20210118192214857.png" alt="image-20210118192214857" style="zoom:80%;" />

åˆ†æ

- åˆå§‹åŒ– VarianceThreshold ï¼ŒæŒ‡å®šé˜€å€¼æ–¹å·®

- è°ƒç”¨ fit_transform

```python
from sklearn.feature_selection import VarianceThreshold
import pandas as pd

path="../Data/Dating.txt"
data = pd.read_csv(path)

# 1ã€å®ä¾‹åŒ–ä¸€ä¸ªè½¬æ¢å™¨ç±»
transfer = VarianceThreshold(threshold=1)
# 2ã€è°ƒç”¨fit_transform
data = transfer.fit_transform(data.iloc[:, 0:-1])

print("åˆ é™¤ä½æ–¹å·®ç‰¹å¾çš„ç»“æœï¼š\n", data)
print("å½¢çŠ¶ï¼š\n", data.shape)
```

è¾“å‡º

```python
åˆ é™¤ä½æ–¹å·®ç‰¹å¾çš„ç»“æœï¼š
 [[4.0920000e+04 8.3269760e+00]
 [1.4488000e+04 7.1534690e+00]
 [2.6052000e+04 1.4418710e+00]
 [7.5136000e+04 1.3147394e+01]
 [3.8344000e+04 1.6697880e+00]]
å½¢çŠ¶ï¼š
 (5, 2)
```

#### 2ï¼‰ç›¸å…³ç³»æ•°

å»é™¤ç›¸å…³ç‰¹å¾ï¼ˆcorrelated featureï¼‰çš„å½±å“

ä½¿ç”¨ Scipy å®ç°

```python
from scipy.stats import pearsonr
```

##### åŸç†

çš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆPearson Correlation Coefficientï¼‰ï¼šåæ˜ å˜é‡ä¹‹é—´ç›¸å…³å…³ç³»å¯†åˆ‡ç¨‹åº¦çš„ç»Ÿè®¡æŒ‡æ ‡
$$
r=\cfrac{n\sum xy=\sum x\sum y}{\sqrt{n\sum x^2 -(\sum x)^2}\sqrt{n\sum y^2-(\sum y)^2}}
$$
ç›¸å…³ç³»æ•°çš„å€¼ä»‹äºâ€“1ä¸+1ä¹‹é—´ï¼Œå³â€“1â‰¤ r â‰¤+1ã€‚å…¶æ€§è´¨å¦‚ä¸‹ï¼š

- å½“r > 0æ—¶ï¼Œè¡¨ç¤ºä¸¤å˜é‡æ­£ç›¸å…³ï¼Œr < 0æ—¶ï¼Œä¸¤å˜é‡ä¸ºè´Ÿç›¸å…³
- å½“|r|=1æ—¶ï¼Œè¡¨ç¤ºä¸¤å˜é‡ä¸ºå®Œå…¨ç›¸å…³ï¼Œå½“ r=0 æ—¶ï¼Œè¡¨ç¤ºä¸¤å˜é‡é—´æ— ç›¸å…³å…³ç³»
- å½“0<|r|<1æ—¶ï¼Œè¡¨ç¤ºä¸¤å˜é‡å­˜åœ¨ä¸€å®šç¨‹åº¦çš„ç›¸å…³ã€‚ä¸”|r|è¶Šæ¥è¿‘1ï¼Œä¸¤å˜é‡é—´çº¿æ€§å…³ç³»è¶Šå¯†åˆ‡ï¼›|r|è¶Šæ¥è¿‘äº0ï¼Œè¡¨ç¤ºä¸¤å˜é‡çš„çº¿æ€§ç›¸å…³è¶Šå¼±
- ä¸€èˆ¬å¯æŒ‰ä¸‰çº§åˆ’åˆ†ï¼š|r|<0.4ä¸ºä½åº¦ç›¸å…³ï¼›0.4â‰¤|r|<0.7ä¸ºæ˜¾è‘—æ€§ç›¸å…³ï¼›0.7â‰¤|r|<1ä¸ºé«˜åº¦çº¿æ€§ç›¸å…³

##### API

***pearsonr(X, Y)***

- Xï¼šnumpy array æ ¼å¼çš„æ•°æ®
- Yï¼šnumpy array æ ¼å¼çš„æ•°æ®
- è¿”å›å€¼
  - rï¼šç›¸å…³ç³»æ•° [-1ï¼Œ1] ä¹‹é—´
  - p-valueï¼špå€¼ï¼ˆpå€¼è¶Šå°ï¼Œè¡¨ç¤ºç›¸å…³ç³»æ•°è¶Šæ˜¾è‘—ï¼Œä¸€èˆ¬på€¼åœ¨500ä¸ªæ ·æœ¬ä»¥ä¸Šæ—¶æœ‰è¾ƒé«˜çš„å¯é æ€§ï¼‰



å¦‚æœç›¸å…³æ€§é«˜å¯ç”¨ä»¥ä¸‹æ–¹æ³•:

1. é€‰å–å…¶ä¸­ä¸€ä¸ªç‰¹å¾

2. ä¸¤ä¸ªç‰¹å¾åŠ æƒæ±‚å’Œ

3. ä¸»æˆåˆ†åˆ†æï¼ˆé«˜ç»´æ•°æ®å˜ä½ç»´ï¼Œèˆå¼ƒåŸç”±æ•°æ®ï¼Œåˆ›é€ æ–°æ•°æ®ï¼Œå¦‚ï¼šå‹ç¼©æ•°æ®ç»´æ•°ï¼Œé™ä½åŸæ•°æ®å¤æ‚åº¦ï¼ŒæŸå¤±å°‘äº†ä¿¡æ¯ï¼‰

##### ä¾‹

ä¸¤ä¸¤ç‰¹å¾ä¹‹é—´è¿›è¡Œç›¸å…³æ€§è®¡ç®—

```python
from scipy.stats import pearsonr
import pandas as pd

path="../Data/Dating.txt"
data = pd.read_csv(path)

# çš®å°”é€Šç›¸å…³ç³»æ•°èŒƒå›´[-1,1], å¦‚æœå¤§äº0å°±æ˜¯æ­£ç›¸å…³(è¶Šæ¥è¿‘1å°±è¶Šç›¸å…³), åä¹‹äº¦ç„¶
r = pearsonr(data["milage"], data["Liters"])
print("milageå’ŒLitersçš„ç›¸å…³ç³»æ•°ä¸º:\n", r)

r = pearsonr(data["milage"], data["Consumtime"])
print("milageå’ŒLitersçš„ç›¸å…³ç³»æ•°ä¸º:\n", r)
```

è¾“å‡º

```python
milageå’ŒLitersçš„ç›¸å…³ç³»æ•°ä¸º:
 (0.660861943290103, 0.2246299034335304)
milageå’ŒLitersçš„ç›¸å…³ç³»æ•°ä¸º:
 (-0.6406267138718624, 0.2441916485876286)
```

### 2.5.2 ä¸»æˆåˆ†åˆ†æ

ï¼ˆPCAï¼‰å°†æ•°æ®åˆ†è§£ä¸ºè¾ƒä½ç»´æ•°ç©ºé—´

```python
from sklearn.decomposition import PCA
```

#### 1ï¼‰æ¦‚å¿µ

- å®šä¹‰ï¼šé«˜ç»´æ•°æ®è½¬åŒ–ä¸ºä½ç»´æ•°æ®çš„è¿‡ç¨‹ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­å¯èƒ½ä¼šèˆå¼ƒåŸæœ‰æ•°æ®ã€åˆ›é€ æ–°çš„å˜é‡
- ä½œç”¨ï¼šæ˜¯æ•°æ®ç»´æ•°å‹ç¼©ï¼Œå°½å¯èƒ½é™ä½åŸæ•°æ®çš„ç»´æ•°ï¼ˆå¤æ‚åº¦ï¼‰ï¼ŒæŸå¤±å°‘é‡ä¿¡æ¯
- åº”ç”¨ï¼šå›å½’åˆ†ææˆ–è€…èšç±»åˆ†æå½“ä¸­

#### 2ï¼‰API

***sklearn.decomposition.PCA(n_components=None)***

- å‚æ•°ï¼š*n_components*
  - å°æ•°ï¼šè¡¨ç¤ºä¿ç•™ç™¾åˆ†ä¹‹å¤šå°‘çš„ä¿¡æ¯
  - æ•´æ•°ï¼šå‡å°‘åˆ°å¤šå°‘ç‰¹å¾

- *PCA.fit_transform(X)*
  - Xï¼šnumpy array æ ¼å¼çš„æ•°æ® [n_samples,n_features]
  - è¿”å›å€¼ï¼šè½¬æ¢åæŒ‡å®šç»´åº¦çš„ array

#### 3ï¼‰ä¾‹

```python
from sklearn.decomposition import PCA

data = [[2, 8, 4, 5], [3, 8, 5, 5], [10, 5, 1, 0]]  # 3*4çŸ©é˜µ åŒ…å«å››ä¸ªç‰¹å¾

transfer = PCA(n_components=3) # ä¸ºæ•´æ•°å°±æ˜¯è½¬ä¸ºå¤šå°‘ä¸ªç‰¹å¾  ä¿ç•™çš„è‡³å°‘éƒ½æ¯”åŸç‰¹å¾å€¼å°‘ä¸€ä¸ª
data_new = transfer.fit_transform(data)
print("(ä¸»æˆåˆ†åˆ†æ)PCAé™ç»´:\n", data_new)
```

è¾“å‡º

```python
(ä¸»æˆåˆ†åˆ†æ)PCAé™ç»´:
[[-3.57495904e+00 -6.64748145e-01  1.07947657e-16]
 [-3.17447323e+00  6.91574499e-01  1.07947657e-16]
 [ 6.74943227e+00 -2.68263539e-02  1.07947657e-16]]
```

è‹¥ n_components è®¾ä¸º0.95

```python
(ä¸»æˆåˆ†åˆ†æ)PCAé™ç»´:
[[-3.57495904]
 [-3.17447323]
 [ 6.74943227]]
```

# ä¸‰ã€è½¬æ¢å™¨å’Œä¼°è®¡å™¨

## 3.1 è½¬æ¢å™¨

ï¼ˆtransformerï¼‰

ç‰¹å¾å·¥ç¨‹çš„æ¥å£ç§°ä¹‹ä¸ºè½¬æ¢å™¨ï¼›è½¬æ¢å™¨æ˜¯ç‰¹å¾å·¥ç¨‹çš„çˆ¶ç±»

è°ƒç”¨æ­¥éª¤

1. å®ä¾‹åŒ– (å®ä¾‹åŒ–çš„æ˜¯ä¸€ä¸ªè½¬æ¢å™¨ç±»(Transformer))

2. è°ƒç”¨fit_transform(å¯¹äºæ–‡æ¡£å»ºç«‹åˆ†ç±»è¯é¢‘çŸ©é˜µï¼Œä¸èƒ½åŒæ—¶è°ƒç”¨)

è½¬æ¢å™¨è°ƒç”¨å½¢å¼

- fit_transform()
- fit()
  - æŒ‰å…¬å¼è®¡ç®—
- transform()
  - è¿›è¡Œæœ€ç»ˆçš„è½¬æ¢

## 3.2 ä¼°è®¡å™¨

ï¼ˆestimatorï¼‰

ä¼°è®¡å™¨å®ç°äº†ç®—æ³•çš„APIï¼Œä¼°è®¡å™¨æ˜¯ç®—æ³•çš„çˆ¶ç±»

- ç”¨äºåˆ†ç±»çš„ä¼°è®¡å™¨ï¼š
  - sklearn.neighbors k-è¿‘é‚»ç®—æ³•
  - sklearn.naive_bayes è´å¶æ–¯
  - sklearn.linear_model.LogisticRegression é€»è¾‘å›å½’
  - sklearn.tree å†³ç­–æ ‘ä¸éšæœºæ£®æ—
- ç”¨äºå›å½’çš„ä¼°è®¡å™¨ï¼š
  - sklearn.linear_model.LinearRegression çº¿æ€§å›å½’
  - sklearn.linear_model.Ridge å²­å›å½’
- ç”¨äºæ— ç›‘ç£å­¦ä¹ çš„ä¼°è®¡å™¨
  - sklearn.cluster.KMeans èšç±»

è°ƒç”¨æ­¥éª¤

1. å®ä¾‹åŒ–ä¼°è®¡å™¨ç±»estimator

2. è¿›è¡Œè®­ç»ƒï¼Œä¸€æ—¦è°ƒç”¨å®Œæ¯•ï¼Œæ„å‘³ç€æ¨¡å‹ç”Ÿæˆ

   - *estimator.fit(x_train, y_train)*

3. æ¨¡å‹è¯„ä¼°

   - ç›´æ¥æ¯”å¯¹çœŸå®å€¼å’Œé¢„æµ‹å€¼

     *y_predict = estimator.predict(x_test)*

     *y_test == y_predict*

   - è®¡ç®—å‡†ç¡®ç‡

     *accuracy = estimator.score(x_test, y_test)*

<img src="https://trou.oss-cn-shanghai.aliyuncs.com/img/ä¼°è®¡å™¨å·¥ä½œæµç¨‹.png" alt="ä¼°è®¡å™¨å·¥ä½œæµç¨‹" style="zoom:67%;" />



## 3.3 æ¨¡å‹é€‰æ‹©ä¸è°ƒä¼˜

```python
from sklearn.model_selection import GridSearchCV
```

### 3.3.1 æ¦‚å¿µ

#### 1ï¼‰äº¤å‰éªŒè¯

ï¼ˆcross validationï¼‰

å°†æ‹¿åˆ°çš„è®­ç»ƒæ•°æ®ï¼Œåˆ†ä¸ºè®­ç»ƒå’ŒéªŒè¯é›†

#### 2ï¼‰è¶…å‚æ•°æœç´¢-ç½‘æ ¼æœç´¢

ï¼ˆGrid Searchï¼‰

é€šå¸¸æƒ…å†µä¸‹ï¼Œæœ‰å¾ˆå¤šå‚æ•°æ˜¯éœ€è¦æ‰‹åŠ¨æŒ‡å®šçš„ï¼ˆå¦‚k-è¿‘é‚»ç®—æ³•ä¸­çš„Kå€¼ï¼‰ï¼Œè¿™ç§å«è¶…å‚æ•°ã€‚ä½†æ˜¯æ‰‹åŠ¨è¿‡ç¨‹ç¹æ‚ï¼Œæ‰€ä»¥éœ€è¦å¯¹æ¨¡å‹é¢„è®¾å‡ ç§è¶…å‚æ•°ç»„åˆã€‚æ¯ç»„è¶…å‚æ•°éƒ½é‡‡ç”¨äº¤å‰éªŒè¯æ¥è¿›è¡Œè¯„ä¼°ã€‚æœ€åé€‰å‡ºæœ€ä¼˜å‚æ•°ç»„åˆå»ºç«‹æ¨¡å‹ã€‚

### 3.3.2 API

***sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None)***

å¯¹ä¼°è®¡å™¨çš„æŒ‡å®šå‚æ•°å€¼è¿›è¡Œè¯¦å°½æœç´¢
- å‚æ•°
  - *estimator*ï¼šä¼°è®¡å™¨å¯¹è±¡
  - *param_grid*ï¼šä¼°è®¡å™¨å‚æ•° ` (dict){â€œn_neighborsâ€:[1,3,5]}`
  - *cv*ï¼šæŒ‡å®šå‡ æŠ˜äº¤å‰éªŒè¯
- æ–¹æ³•
  - *fit*ï¼šè¾“å…¥è®­ç»ƒæ•°æ®
  - *score*ï¼šå‡†ç¡®ç‡
- ç»“æœåˆ†æï¼š
  - *best_params_*ï¼šåœ¨äº¤å‰éªŒè¯ä¸­éªŒè¯çš„æœ€å¥½è¶…å‚æ•°
  - *best_score_*ï¼šåœ¨äº¤å‰éªŒè¯ä¸­éªŒè¯çš„æœ€å¥½ç»“æœ
  - *best_estimator_*ï¼šæœ€å¥½çš„å‚æ•°æ¨¡å‹
  - *cv_results_*ï¼šæ¯æ¬¡äº¤å‰éªŒè¯åçš„éªŒè¯é›†å‡†ç¡®ç‡ç»“æœå’Œè®­ç»ƒé›†å‡†ç¡®ç‡ç»“æœ

# å››ã€åˆ†ç±»

## 4.1 KNN ç®—æ³•

> æ ¹æ®é‚»å±…ï¼Œåˆ¤æ–­ç±»åˆ«

```python
from sklearn.neighbors import KNeighborsClassifier
```

ï¼ˆK Nearest Neighborï¼‰å³K - è¿‘é‚»ç®—æ³•

å¦‚æœä¸€ä¸ªæ ·æœ¬åœ¨ç‰¹å¾ç©ºé—´ä¸­çš„**kä¸ªæœ€ç›¸ä¼¼(å³ç‰¹å¾ç©ºé—´ä¸­æœ€é‚»è¿‘)çš„æ ·æœ¬ä¸­çš„å¤§å¤šæ•°å±äºæŸä¸€ä¸ªç±»åˆ«**ï¼Œåˆ™è¯¥æ ·æœ¬ä¹Ÿå±äºè¿™ä¸ªç±»åˆ«ã€‚

- ä¼˜ç‚¹ï¼š
  - ç®€å•ï¼Œæ˜“äºç†è§£ï¼Œæ˜“äºå®ç°ï¼Œæ— éœ€è®­ç»ƒ
- ç¼ºç‚¹ï¼š
  - æ‡’æƒ°ç®—æ³•ï¼Œå¯¹æµ‹è¯•æ ·æœ¬åˆ†ç±»æ—¶çš„è®¡ç®—é‡å¤§ï¼Œå†…å­˜å¼€é”€å¤§
  - å¿…é¡»æŒ‡å®šKå€¼ï¼ŒKå€¼é€‰æ‹©ä¸å½“åˆ™åˆ†ç±»ç²¾åº¦ä¸èƒ½ä¿è¯
- ä½¿ç”¨åœºæ™¯ï¼šå°æ•°æ®åœºæ™¯ï¼Œå‡ åƒï½å‡ ä¸‡æ ·æœ¬ï¼Œå…·ä½“åœºæ™¯å…·ä½“ä¸šåŠ¡å»æµ‹è¯•

### 4.1.1 API

***sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')***

- n_neighborsï¼šintï¼Œå¯é€‰ï¼ˆé»˜è®¤= 5ï¼‰ï¼Œä½¿ç”¨çš„é‚»å±…æ•°
- algorithmï¼š*{â€˜autoâ€™ï¼Œâ€˜ball_treeâ€™ï¼Œâ€˜kd_treeâ€™ï¼Œâ€˜bruteâ€™}*ï¼Œå¯é€‰ç”¨äºè®¡ç®—æœ€è¿‘é‚»å±…çš„ç®—æ³•ï¼Œä¸åŒå®ç°æ–¹å¼å½±å“æ•ˆç‡
  - *â€˜ball_treeâ€™* å°†ä¼šä½¿ç”¨ BallTree
  - *â€˜kd_treeâ€™* å°†ä½¿ç”¨ KDTree
  - *â€˜autoâ€™* å°†å°è¯•æ ¹æ®ä¼ é€’ç»™fitæ–¹æ³•çš„å€¼æ¥å†³å®šæœ€åˆé€‚çš„ç®—æ³•

### 4.1.2 ä¾‹

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn import datasets
import numpy as np
import pandas as pd

# æ•°æ®é›†å¯¼å…¥
iris = datasets.load_iris()

# æ•°æ®é›†åˆ’åˆ†
x_train, x_test, y_train, y_test = train_test_split(
    iris.data, iris.target, random_state=43)

# æ ‡å‡†åŒ–
transfer_std = StandardScaler()
x_train_std = transfer_std.fit_transform(x_train)
x_test_std = transfer_std.transform(x_test)  # æµ‹è¯•é›†ä¸è¦ç”¨fit, å› ä¸ºè¦ä¿æŒå’Œè®­ç»ƒé›†å¤„ç†æ–¹å¼ä¸€è‡´

# KNN
estimator_knn = KNeighborsClassifier(n_neighbors=3)

# è°ƒä¼˜
param_dict = {"n_neighbors": [1, 3, 5, 7, 9, 11]}
estimator_knn = GridSearchCV(
    estimator_knn, param_grid=param_dict, cv=10)  # 10æŠ˜

# è®­ç»ƒæ¨¡å‹
estimator_knn.fit(x_train_std, y_train)
y_pred = estimator_knn.predict(x_test_std)

print("é¢„æµ‹å€¼ä¸º:", y_pred, "\nçœŸå®å€¼ä¸º:", y_test, "\næ¯”è¾ƒç»“æœä¸º:", y_test == y_pred)
print("å‡†ç¡®ç‡ä¸ºï¼š\n", estimator_knn.score(x_test_std, y_test))

print("æœ€ä½³å‚æ•°:\n", estimator_knn.best_params_)
print("æœ€ä½³ç»“æœ:\n", estimator_knn.best_score_)
print("æœ€ä½³ä¼°è®¡å™¨:\n", estimator_knn.best_estimator_)
print("äº¤å‰éªŒè¯ç»“æœ:\n", estimator_knn.cv_results_)

```

è¾“å‡º

```python
é¢„æµ‹å€¼ä¸º: [0 0 2 1 2 0 2 1 1 1 0 1 2 0 1 1 0 0 2 2 0 0 0 1 2 2 0 1 0 0 1 0 1 1 2 2 1
 2] 
çœŸå®å€¼ä¸º: [0 0 2 1 2 0 2 1 1 1 0 1 2 0 1 1 0 0 2 2 0 0 0 2 2 2 0 1 0 0 1 0 1 1 2 2 1
 2] 
æ¯”è¾ƒç»“æœä¸º: [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True False
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True]
å‡†ç¡®ç‡ä¸ºï¼š
 0.9736842105263158
æœ€ä½³å‚æ•°:
 {'n_neighbors': 1}
æœ€ä½³ç»“æœ:
 0.9469696969696969
æœ€ä½³ä¼°è®¡å™¨:
 KNeighborsClassifier(n_neighbors=1)
äº¤å‰éªŒè¯ç»“æœ:
 {'mean_fit_time': array([0.00029657, 0.00039995, 0.00039968, 0.00049977, 0.00029998,
       0.00040131]), 'std_fit_time': array([0.00045309, 0.00048983, 0.00048951, 0.00049977, 0.00045822,
       0.0004915 ]), 'mean_score_time': array([0.00089977, 0.00080023, 0.00110025, 0.00080018, 0.00079889,
       0.00080283]), 'std_score_time': array([0.00029992, 0.0004004 , 0.00030082, 0.00040009, 0.00039965,
       0.00040154]), 'param_n_neighbors': masked_array(data=[1, 3, 5, 7, 9, 11],
             mask=[False, False, False, False, False, False],
       fill_value='?',
            dtype=object), 'params': [{'n_neighbors': 1}, {'n_neighbors': 3}, {'n_neighbors': 5}, {'n_neighbors': 7}, {'n_neighbors': 9}, {'n_neighbors': 11}], 'split0_test_score': array([0.91666667, 0.91666667, 0.91666667, 0.91666667, 0.91666667,
       0.91666667]), 'split1_test_score': array([0.91666667, 0.91666667, 0.83333333, 0.91666667, 0.91666667,
       0.91666667]), 'split2_test_score': array([0.90909091, 0.90909091, 0.90909091, 0.90909091, 0.90909091,
       1.        ]), 'split3_test_score': array([0.90909091, 0.90909091, 0.90909091, 0.90909091, 0.90909091,
       0.81818182]), 'split4_test_score': array([1., 1., 1., 1., 1., 1.]), 'split5_test_score': array([0.90909091, 0.90909091, 1.        , 1.        , 1.        ,
       1.        ]), 'split6_test_score': array([1., 1., 1., 1., 1., 1.]), 'split7_test_score': array([0.90909091, 0.81818182, 0.81818182, 0.81818182, 0.81818182,
       0.81818182]), 'split8_test_score': array([1., 1., 1., 1., 1., 1.]), 'split9_test_score': array([1.        , 0.90909091, 1.        , 1.        , 1.        ,
       0.90909091]), 'mean_test_score': array([0.9469697 , 0.92878788, 0.93863636, 0.9469697 , 0.9469697 ,
       0.93787879]), 'std_test_score': array([0.04338734, 0.05412294, 0.06830376, 0.05945884, 0.05945884,
       0.07048305]), 'rank_test_score': array([1, 6, 4, 1, 1, 5])}
```

## 4.2 æœ´ç´ è´å¶æ–¯ç®—æ³•

ï¼ˆNaive Bayesï¼‰

> ç›¸äº’ç‹¬ç«‹çš„ç‰¹å¾ + è´å¶æ–¯å…¬å¼

```python
from sklearn.naive_bayes import MultinomialNB
```

æœ´ç´ ï¼šç‰¹å¾ä¸ç‰¹å¾ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„

æœ´ç´ è´å¶æ–¯ç®—æ³•ç»å¸¸ç”¨äºæ–‡æœ¬åˆ†ç±», å› ä¸ºæ–‡ç« è½¬æ¢æˆæœºå™¨å­¦ä¹ ç®—æ³•è¯†åˆ«çš„æ•°æ®æ˜¯ä»¥å•è¯ä¸ºç‰¹å¾çš„

- ä¼˜ç‚¹ï¼š
  - æœ´ç´ è´å¶æ–¯æ¨¡å‹å‘æºäºå¤å…¸æ•°å­¦ç†è®ºï¼Œæœ‰ç¨³å®šçš„åˆ†ç±»æ•ˆç‡ã€‚
  - å¯¹ç¼ºå¤±æ•°æ®ä¸å¤ªæ•æ„Ÿï¼Œç®—æ³•ä¹Ÿæ¯”è¾ƒç®€å•ï¼Œå¸¸ç”¨äºæ–‡æœ¬åˆ†ç±»ã€‚
  - åˆ†ç±»å‡†ç¡®åº¦é«˜ï¼Œé€Ÿåº¦å¿«
- ç¼ºç‚¹ï¼š
  - ç”±äºä½¿ç”¨äº†æ ·æœ¬å±æ€§ç‹¬ç«‹æ€§çš„å‡è®¾ï¼Œæ‰€ä»¥å¦‚æœç‰¹å¾å±æ€§æœ‰å…³è”æ—¶å…¶æ•ˆæœä¸å¥½

### 4.2.1 åŸç†

#### 1ï¼‰è´å¶æ–¯å…¬å¼

ä»¥æ–‡æœ¬åˆ†ç±»ä¸ºä¾‹
$$
P(C|F_1,F_2,\ldots)=\cfrac{P(F_1,F_2,\ldots|C)P(C)}{P(F_1,F_2,\ldots)}
$$

- $P(C)$ï¼šæ¯ä¸ªæ–‡æ¡£ç±»åˆ«çš„æ¦‚ç‡(æŸæ–‡æ¡£ç±»åˆ«æ•°ï¼æ€»æ–‡æ¡£æ•°é‡)
- $P(Wâ”‚C)$ï¼šç»™å®šç±»åˆ«ä¸‹ç‰¹å¾ï¼ˆè¢«é¢„æµ‹æ–‡æ¡£ä¸­å‡ºç°çš„è¯ï¼‰çš„æ¦‚ç‡
  - $W$ ä¸ºç»™å®šæ–‡æ¡£çš„ç‰¹å¾å€¼ï¼ˆé¢‘æ•°ç»Ÿè®¡ï¼‰
  - è®¡ç®—æ–¹æ³•ï¼š$P(F_1â”‚C)=N_i/N$ ï¼ˆè®­ç»ƒæ–‡æ¡£ä¸­å»è®¡ç®—ï¼‰
    - $N_i$ï¼šè¯¥ $F_1$ è¯åœ¨ $C$ ç±»åˆ«æ‰€æœ‰æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°
    - $N$ï¼šæ‰€å±ç±»åˆ« $C$ ä¸‹çš„æ–‡æ¡£çš„æ–‡æœ¬æ€»å’Œ
- $P(F_1,F_2,\ldots)$ é¢„æµ‹æ–‡æ¡£ä¸­æ¯ä¸ªè¯çš„æ¦‚ç‡

#### 2ï¼‰æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ç³»æ•°

ç›®çš„ï¼šé˜²æ­¢è®¡ç®—å‡ºçš„åˆ†ç±»æ¦‚ç‡ä¸º0
$$
P(F_1|C)=\cfrac{N_i+\alpha}{N+\alpha m}
$$

- $\alpha$ï¼šé¢„å…ˆæŒ‡å®šçš„ç³»æ•°ï¼Œé»˜è®¤ä¸º 1
- $m$ï¼šè®­ç»ƒæ–‡æ¡£ä¸­ç‰¹å¾è¯çš„ç§ç±»æ•°

```python
# å› ä¸ºæ ·æœ¬æ•°é‡ä¸å¤Ÿï¼Œä¼šå‡ºç°ç‰¹å¾è¯ä¸åœ¨ä¸€ç±»æ–‡æœ¬ä¸­å‡ºç°çš„æƒ…å†µ
P(å¨±ä¹|å½±é™¢,æ”¯ä»˜å®,äº‘è®¡ç®—) = ğ‘ƒ(å½±é™¢,æ”¯ä»˜å®,äº‘è®¡ç®—|å¨±ä¹)âˆ—P(å¨±ä¹)=(56/121)âˆ—(15/121)âˆ—(0/121)âˆ—(60/90) = 0
# æ­¤æ—¶éœ€è¦å®ç”¨åˆ°æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ç³»æ•°
P(å¨±ä¹|å½±é™¢,æ”¯ä»˜å®,äº‘è®¡ç®—) =P(å½±é™¢,æ”¯ä»˜å®,äº‘è®¡ç®—|å¨±ä¹)P(å¨±ä¹)=(56+1/121+4)(15+1/121+4)(0+1/121+1*4)(60/90) = 0.00002
```

### 4.2.2 API

***sklearn.naive_bayes.MultinomialNB(alpha = 1.0)***

- *alpha*ï¼šæ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ç³»æ•°

### 4.2.3 ä¾‹ï¼š20ç±»æ–°é—»åˆ†ç±»

åˆ†æ

- åˆ’åˆ†æ•°æ®é›†
- tfidf è¿›è¡Œçš„ç‰¹å¾æŠ½å–
- æœ´ç´ è´å¶æ–¯é¢„æµ‹

```python
from sklearn.datasets import fetch_20newsgroups, load_files
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
import pandas as pd

data = fetch_20newsgroups(subset="all") 
x_train, x_test, y_train, y_test = \
        train_test_split(data.data, data.target, test_size=0.2, random_state=22)
# æ–‡æœ¬åˆ†ç±»
transfer = TfidfVectorizer()  
x_train = transfer.fit_transform(x_train)
x_test = transfer.transform(x_test)  
# æœ´ç´ è´å¶æ–¯
estimator = MultinomialNB()
estimator.fit(x_train, y_train)
y_predict = estimator.predict(x_test)

print("é¢„æµ‹å€¼ä¸º:", y_predict, "\nçœŸå®å€¼ä¸º:", y_test, "\næ¯”è¾ƒç»“æœä¸º:", y_test == y_predict)
score = estimator.score(x_test, y_test)
print("å‡†ç¡®ç‡ä¸º: ", score)
```

è¾“å‡º

```python
é¢„æµ‹å€¼ä¸º: [15 13 16 ... 13  2 13] 
çœŸå®å€¼ä¸º: [15 13 16 ... 13  2 13] 
æ¯”è¾ƒç»“æœä¸º: [ True  True  True ...  True  True  True]
å‡†ç¡®ç‡ä¸º:  0.8511936339522547
```

## 4.3 å†³ç­–æ ‘

ï¼ˆDecision Treeï¼‰

```python
from sklearn.tree import DecisionTreeClassifier
```

> if - else

### 4.3.1 åŸç†

#### 1ï¼‰ä¿¡æ¯ç†µ

$$
H(X)=-\sum\limits_{i=1}^n P(x_i)log_bP(x_i)
$$

#### 2ï¼‰æ¡ä»¶ä¿¡æ¯ç†µ

$$
H(D|A)=\sum\limits_{i=1}^n \cfrac{|D_i|}{|D|}H(D_i)
$$



#### 3ï¼‰ä¿¡æ¯å¢ç›Š

å†³ç­–æ ‘çš„åˆ’åˆ†ä¾æ®ä¹‹ä¸€

ç‰¹å¾ $A$ å¯¹è®­ç»ƒæ•°æ®é›† $D$ çš„ä¿¡æ¯å¢ç›Š $g(D,A)$,å®šä¹‰ä¸ºé›†åˆ D çš„ä¿¡æ¯ç†µ $H(D)$ ä¸ç‰¹å¾ $A$ ç»™å®šæ¡ä»¶ä¸‹ $D$ çš„ä¿¡æ¯æ¡ä»¶ç†µ $H(D|A)$ ä¹‹å·®
$$
g(D,A)=H(D)=H(D|A)
$$

#### 4ï¼‰ä¸‰ç§ç®—æ³•å®ç°

- ID3
  - ä¿¡æ¯å¢ç›Š æœ€å¤§çš„å‡†åˆ™
- C4.5
  - ä¿¡æ¯å¢ç›Šæ¯” æœ€å¤§çš„å‡†åˆ™
- CART
  - åˆ†ç±»æ ‘: åŸºå°¼ç³»æ•° æœ€å°çš„å‡†åˆ™ åœ¨sklearnä¸­å¯ä»¥é€‰æ‹©åˆ’åˆ†çš„é»˜è®¤åŸåˆ™
  - ä¼˜åŠ¿ï¼šåˆ’åˆ†æ›´åŠ ç»†è‡´ï¼ˆä»åé¢ä¾‹å­çš„æ ‘æ˜¾ç¤ºæ¥ç†è§£ï¼‰

### 4.3.2 API

***class sklearn.tree.DecisionTreeClassifier(criterion=â€™giniâ€™, max_depth=None,random_state=None)***

å†³ç­–æ ‘åˆ†ç±»å™¨

- criterionï¼šé»˜è®¤æ˜¯â€™giniâ€™ç³»æ•°ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©ä¿¡æ¯å¢ç›Šçš„ç†µâ€™entropyâ€™
- max_depthï¼šæ ‘çš„æ·±åº¦å¤§å°
- random_stateï¼šéšæœºæ•°ç§å­

### 4.3.3 ä¿å­˜æ ‘çš„ç»“æ„

```python
from sklearn.tree import export_graphviz
```

***sklearn.tree.export_graphviz()*** 

è¯¥å‡½æ•°èƒ½å¤Ÿå¯¼å‡ºDOTæ ¼å¼

- tree.export_graphviz(estimator, out_file=path, feature_names)

### 4.3.4 ä¾‹

```python
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, export_graphviz

iris = datasets.load_iris()
x_train, x_test, y_train, y_test = train_test_split(
    iris.data, iris.target, random_state=22)
# å†³ç­–æ ‘è®­ç»ƒ
estimator = DecisionTreeClassifier(criterion="entropy")
estimator.fit(x_train, y_train)
# ç”Ÿæˆæ ‘æ–‡ä»¶
export_graphviz(estimator, out_file="tree.dot",
                feature_names=iris.feature_names)

y_pred = estimator.predict(x_test)
print("é¢„æµ‹å€¼ä¸º:", y_pred, "\nçœŸå®å€¼ä¸º:", y_test, "\næ¯”è¾ƒç»“æœä¸º:", y_test == y_pred)
score = estimator.score(x_test, y_test)
print("å‡†ç¡®ç‡ä¸º: ", score)
```

è¾“å‡º

```python
é¢„æµ‹å€¼ä¸º: [0 2 1 2 1 1 1 1 1 0 2 1 2 2 0 2 1 1 1 1 0 2 0 1 2 0 2 2 2 1 0 0 1 1 1 0 0
 0] 
çœŸå®å€¼ä¸º: [0 2 1 2 1 1 1 2 1 0 2 1 2 2 0 2 1 1 2 1 0 2 0 1 2 0 2 2 2 2 0 0 1 1 1 0 0
 0] 
æ¯”è¾ƒç»“æœä¸º: [ True  True  True  True  True  True  True False  True  True  True  True
  True  True  True  True  True  True False  True  True  True  True  True
  True  True  True  True  True False  True  True  True  True  True  True
  True  True]
å‡†ç¡®ç‡ä¸º:  0.9210526315789473
```

# å…­ã€èšç±»

K-meansï¼ˆKå‡å€¼èšç±»ï¼‰

- ç‰¹ç‚¹ï¼šé‡‡ç”¨è¿­ä»£å¼ç®—æ³•ï¼Œç›´è§‚æ˜“æ‡‚å¹¶ä¸”éå¸¸å®ç”¨
- ç¼ºç‚¹ï¼šå®¹æ˜“æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜è§£(å¤šæ¬¡èšç±»)

## 6.1 èšç±»æ­¥éª¤

1. éšæœºè®¾ç½® K ä¸ªç‰¹å¾ç©ºé—´å†…çš„ç‚¹ä½œä¸ºåˆå§‹çš„èšç±»ä¸­å¿ƒ

2. å¯¹äºå…¶ä»–æ¯ä¸ªç‚¹è®¡ç®—åˆ° K ä¸ªä¸­å¿ƒçš„è·ç¦»ï¼ŒæœªçŸ¥çš„ç‚¹é€‰æ‹©æœ€è¿‘çš„ä¸€ä¸ªèšç±»ä¸­å¿ƒç‚¹ä½œä¸ºæ ‡è®°ç±»åˆ«

3. æ¥ç€å¯¹ç€æ ‡è®°çš„èšç±»ä¸­å¿ƒä¹‹åï¼Œé‡æ–°è®¡ç®—å‡ºæ¯ä¸ªèšç±»çš„æ–°ä¸­å¿ƒç‚¹ï¼ˆå¹³å‡å€¼ï¼‰

4. å¦‚æœè®¡ç®—å¾—å‡ºçš„æ–°ä¸­å¿ƒç‚¹ä¸åŸä¸­å¿ƒç‚¹ä¸€æ ·ï¼Œé‚£ä¹ˆç»“æŸï¼Œå¦åˆ™é‡æ–°è¿›è¡Œç¬¬äºŒæ­¥è¿‡ç¨‹

<img src="https://trou.oss-cn-shanghai.aliyuncs.com/img/K-meansè¿‡ç¨‹åˆ†æ.png" alt="K-meansè¿‡ç¨‹åˆ†æ" style="zoom:67%;" />

## 6.2 API

***sklearn.cluster.KMeans(n_clusters=8, init=â€˜k-means++â€™â€¦)***

- n_clustersï¼šå¼€å§‹çš„èšç±»ä¸­å¿ƒæ•°é‡
- initï¼šåˆå§‹åŒ–æ–¹æ³•ï¼Œé»˜è®¤ä¸º'k-means ++â€™

*KMeans.labels_*ï¼šé»˜è®¤æ ‡è®°çš„ç±»å‹ï¼Œå¯ä»¥å’ŒçœŸå®å€¼æ¯”è¾ƒï¼ˆä¸æ˜¯å€¼æ¯”è¾ƒï¼‰

## 6.3  è½®å»“ç³»æ•°

è½®å»“ç³»æ•°ä½œä¸º Kmeans çš„æ€§èƒ½è¯„ä¼°æŒ‡æ ‡

### 6.3.1 å…¬å¼

$$
sc_i=\cfrac{b_i-a_i}{max(b_i,a_i)}
$$

> æ³¨ï¼šå¯¹äºæ¯ä¸ªç‚¹ $i$ ä¸ºå·²èšç±»æ•°æ®ä¸­çš„æ ·æœ¬ ï¼Œ$b_i$ ä¸º $i$ åˆ°å…¶å®ƒæ—ç¾¤çš„æ‰€æœ‰æ ·æœ¬çš„è·ç¦»æœ€å°å€¼ï¼Œ$a_i$ ä¸º $i$ åˆ°æœ¬èº«ç°‡çš„è·ç¦»å¹³å‡å€¼ã€‚æœ€ç»ˆè®¡ç®—å‡ºæ‰€æœ‰çš„æ ·æœ¬ç‚¹çš„è½®å»“ç³»æ•°å¹³å‡å€¼

### 6.3.2 è½®å»“ç³»æ•°å€¼åˆ†æ

<img src="https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20210118223723565.png" alt="image-20210118223723565" style="zoom: 60%;" />

**åˆ†æè¿‡ç¨‹**ï¼ˆä»¥ä¸€ä¸ªè“1ç‚¹ä¸ºä¾‹ï¼‰

1. è®¡ç®—å‡ºè“1ç¦»æœ¬èº«æ—ç¾¤æ‰€æœ‰ç‚¹çš„è·ç¦»çš„å¹³å‡å€¼$a_i$

2. è“1åˆ°å…¶å®ƒä¸¤ä¸ªæ—ç¾¤çš„è·ç¦»è®¡ç®—å‡ºå¹³å‡å€¼çº¢å¹³å‡ï¼Œç»¿å¹³å‡ï¼Œå–æœ€å°çš„é‚£ä¸ªè·ç¦»ä½œä¸º$b_i$

3. æ ¹æ®å…¬å¼ï¼šæç«¯å€¼è€ƒè™‘ï¼šå¦‚æœ$b_i >>a_i$ï¼Œé‚£ä¹ˆå…¬å¼ç»“æœè¶‹è¿‘äº1ï¼›å¦‚æœ$a_i>>>b_i$ï¼Œé‚£ä¹ˆå…¬å¼ç»“æœè¶‹è¿‘äº-1

**ç»“è®º**ï¼šå¦‚æœ$b_i>>a_i$ï¼Œè¶‹è¿‘äº1ï¼Œæ•ˆæœå¥½ï¼Œ $b_i<<a_i$ï¼Œè¶‹è¿‘äº-1ï¼Œæ•ˆæœä¸å¥½ã€‚è½®å»“ç³»æ•°çš„å€¼æ˜¯ä»‹äº [-1,1] ï¼Œè¶Šè¶‹è¿‘äº1ä»£è¡¨å†…èšåº¦å’Œåˆ†ç¦»åº¦éƒ½ç›¸å¯¹è¾ƒä¼˜

### 6.3.3 API

- ***sklearn.metrics.silhouette_score(X, labels)***ï¼šè®¡ç®—æ‰€æœ‰æ ·æœ¬çš„å¹³å‡è½®å»“ç³»æ•°
  - Xï¼šç‰¹å¾å€¼
  - labelsï¼šè¢«èšç±»æ ‡è®°çš„ç›®æ ‡å€¼ï¼ˆèšç±»ç»“æœï¼‰

## 6.4 ä¾‹

åˆ†æ

1. é™ç»´ä¹‹åçš„æ•°æ®

2. k-meansèšç±»

3. èšç±»ç»“æœæ˜¾ç¤º
4. ç”¨æˆ·èšç±»ç»“æœè¯„ä¼°

```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.datasets.samples_generator import make_blobs
# Xä¸ºæ ·æœ¬ç‰¹å¾ï¼ŒYä¸ºæ ·æœ¬ç°‡ç±»åˆ«ï¼Œ å…±1000ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬2ä¸ªç‰¹å¾ï¼Œå…±4ä¸ªç°‡ï¼Œç°‡ä¸­å¿ƒåœ¨[-1,-1], [0,0],[1,1], [2,2]ï¼Œ ç°‡æ–¹å·®åˆ†åˆ«ä¸º[0.4, 0.2, 0.2]
X, y = make_blobs(n_samples=1000, n_features=2, centers=[[-1,-1], [0,0], [1,1], [2,2]], cluster_std=[0.4, 0.2, 0.2, 0.2])
plt.scatter(X[:, 0], X[:, 1], marker='o')
plt.show()
```

æ¡ˆä¾‹æ ·æœ¬

<img src="https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20210119103734104.png" alt="image-20210119103734104" style="zoom:67%;" />

```python
from sklearn.cluster import KMeans

#y_pred = KMeans(n_clusters=4).fit_predict(X)
estimator = KMeans(n_clusters=4, init='k-means++')
estimator.fit(X)
y_pred = estimator.predict(X)

plt.scatter(X[:, 0], X[:, 1], c=y_pred)
plt.show()
```

<img src="https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20210119104047678.png" alt="image-20210119104047678" style="zoom:67%;" />

è¾“å‡º

```python
æ¨¡å‹è½®å»“ç³»æ•°ä¸º(1 æœ€å¥½, -1 æœ€å·®): 0.6634549555891298
```

