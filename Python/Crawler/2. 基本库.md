#  一、urllib

## 1.1 模块介绍

1. request：它是最基本的HTTP请求模块，可以用来模拟发送请求。就像在浏览器里输入网址然后回车一样，只需要给库方法传入URL以及额外的参数，就可以模拟实现这个过程了。

2. error：异常处理模块，如果出现请求错误，我们可以捕获这些异常，然后进行重试或其他操作以保证程序不会意外终止。

3. parse：一个工具模块，提供了许多URL处理方法，比如拆分、解析、合并等。

4. robotparser：主要是用来识别网站的robots.txt文件，然后判断哪些网站可以爬，哪些网站不可以爬，它其实用得比较少

## 1.2 发送请求 

使用urllib.equest模块，我们可以方便地实现请求的发送并得到响应

### 1.2.1 urlopen()

``` python
import urllib.request
response = urllib.request.urlopen("https://www.python.org")
print(response.read().decode('utf-8'))
```

输出：网页源代码

#### 1）利用type()方法输出响应的类型

``` python
print(type(response))
```

输出：`<class 'http.client.HTTPResponse'>`

可以发现，它是一个HTTPResposne类型的对象。它主要包含read()、readinto()、getheader(name)、getheaders()、fileno()等方法，msg、version、status、reason、debuglevel、closed等属性

#### 2）调用方法和属性

```python
print(response.status)
print(response.getheaders())
print(response.getheader('Server'))
```

输出：响应的状态码，响应的头信息，响应头中的Server值 

``` markdown
200
 [('Server', 'nginx'), ('Content-Type', 'text/html; charset=utf-8')……('Strict-Transport-Security', 'max-age=63072000; includeSubDomains')]
 nginx
```

#### 3）urlopen()函数的API

`urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)`

1. data参数

   - 如果传递了这个参数，则它的请求方式就不再是GET方式，而是POST方式

   - 如果它是字节流编码格式（bytes类型），则需要通过bytes()方法转化

   - 例

     ``` python
     import urllib.parse
     import urllib.request
     data = bytes(urllib.parse.urlencode({'name': 'germey'}), encoding='utf-8')
     response = urllib.request.urlopen('https://httpbin.org/post', data=data)
     print(response.read().decode('utf-8'))
     ```

     第一个参数需要是str类型，需要用urllib.parse模块里的urlencode()方法来将参数字典转化为字符串

     第二个参数指定编码格式，这里指定为utf8

     输出：

     ``` markdown
     {
     	"args": {},
     	"data": "",
     	"files": {},
     	"form": {
     			"name": "germey"
     			},
     	"headers": {
     	    "Accept-Encoding": "identity",
     		"Content-Length": "11",
     		"Content-Type": "application/x-www-form-urlencoded",
     		"Host": "httpbin.org",
     		"User-Agent": "Python-urllib/3.7",
     		"X-Amzn-Trace-Id": "Root=1-5f96de5b-6805a3ac7b7b4b151a11fdc6"
     			},
     	"json": null,
     	"origin": "59.79.2.148",
     	"url": "https://httpbin.org/post"
     }
     ```

2. timeout参数

   用于设置超时时间，单位为秒，意思就是如果请求超出了设置的这个时间，还没有得到响应，就会抛出异常。如果不指定该参数，就会使用全局默认时间。它支持HTTP、HTTPS、FTP请求。

   - 例一

     ``` python
     import urllib.request
     response = urllib.request.urlopen('https://httpbin.org/get', timeout=0.1)
     print(response.read())
     ```

     程序1秒过后，服务器依然没有响应，于是抛出了URLError异常。该异常属于urllib.error模块，错误原因是超时

   - 例二

     可以通过设置这个超时时间来控制一个网页如果长时间未响应，就跳过它的抓取。这可以利用try except语句来实现

     ``` python
     import urllib.request
     import socket
     import urllib.error
     try:
     	response = urllib.request.urlopen('http://httpbin.org/get', timeout=0.1)
     except urllib.error.URLError as e:
     	if isinstance(e.reason, socket.timeout):
     		print('TIME OUT')
     ```

     这里我们请求了<http://httpbin.org/get>测试链接，设置超时时间是0.1秒，然后捕获了URLError异常，接着判断异常是socket.timeout类型（意思就是超时异常），从而得出它确实是因为超时而报错，打印输出了TIME OUT

3. 其他参数
   - context参数，它必须是ssl.SSLContext类型，用来指定SSL设置。
   - cafile和capath这两个参数分别指定CA证书和它的路径，这个在请求HTTPS链接时会有用。
   - cadefault参数现在已经弃用了，其默认值为False

### 2.2 Request

如果请求中需要加入Headers等信息，就可以利用更强大的Request类来构建

``` python
import urllib.request
request = urllib.request.Request('https://python.org')
response = urllib.request.urlopen(request)
print(response.read().decode('utf-8'))
```

我们依然是用urlopen()方法来发送这个请求，只不过这次该方法的参数不再是URL，而是一个Request类型的对象。通过构造这个数据结构，一方面我们可以将请求独立成一个对象，另一方面可更加丰富和灵活地配置参数

#### 1） 使用参数来构造Request

`class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)`

1. url用于请求URL，这是必传参数，其他都是可选参数

2. data如果要传，必须传bytes（字节流）类型。如果它是字典，可以先用urllib.parse模块里的urlencode()编码

3. headers是一个字典，它就是请求头，可以在构造请求时通过headers参数直接构造，也可以通过调用请求实例的add_header()方法添加

   添加请求头最常用的用法就是通过修改User-Agent来伪装浏览器，默认的User-Agent是Python-urllib，我们可以通过修改它来伪装浏览器。比如要伪装火狐浏览器，你可以把它设置为`Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11`

4. origin_req_host指的是请求方的host名称或者IP地址。

5. unverifiable表示这个请求是否是无法验证的，默认是False，意思就是说用户有足够权限来选择接收这个请求的结果。

   例如，我们请求一个HTML文档中的图片，但是我们没有自动抓取图像的权限，这时unverifiable的值就是True。

6. method是一个字符串，用来指示请求使用的方法，比如GET、POST和PUT等

   例

   ```python
   from urllib import request, parse
   url = 'https://httpbin.org/post'
   headers = {
   	'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',
   	'Host': 'httpbin.org'
   }
   dict = {'name': 'germey'}
   data = bytes(parse.urlencode(dict), encoding='utf-8')
   req = request.Request(url=url, data=data, headers=headers, method='POST')
   response = request.urlopen(req)
   print(response.read().decode('utf-8'))
   ```

   其中url即请求URL，headers中指定了User-Agent和Host，参数data用urlencode()和bytes()方法转成字节流。另外，指定了请求方式为POST

   输出

   ```markdown
   {
   	"args": {},
   	"data": "",
   	"files": {},
   	"form": {
   			"name": "germey"
   			      },
   	"headers": {
   			"Accept-Encoding": "identity",
   		 	"Content-Length": "11",
   		 	"Content-Type": "application/x-www-form-urlencoded",
   			"Host": "httpbin.org",
   			"User-Agent": "Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)",
   			"X-Amzn-Trace-Id": "Root=1-5f96e6c8-0c429d0c205ce45b3365653e"
   			         },
   	"json": null,
   	"origin": "59.79.2.148",
   	"url": "https://httpbin.org/post"
   }
   
   ```

   另外，headers也可以用add_header()方法来添加

   ```python
   req = request.Request(url=url, data=data, method='POST')
   req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MISE 5.5; Windows NT)')
   ```

### 2.3 高级操作

1. 对于一些更高级的操作（比如Cookies处理、代理设置等），需要使用更强大的工具Handler来实现。我们可以把Handler理解为各种处理器，有专门处理登录验证的，有处理Cookies的，有处理代理设置的。利用它们，我们几乎可以做到HTTP请求中所有的事情

2. 另一个比较重要的类就是`OpenerDirector`（Opener）`urlopen()`实际上就是urllib为我们提供的一个Opener。之前使用的Request和urlopen()相当于类库为你封装好了极其常用的请求方法，利用它们可以完成基本的请求。Opener可以使用`open()`方法，返回的类型和`urlopen()`如出一辙。可以利用Handler来构建Opener。

## 1.3 处理异常

`urllib`的`error`模块定义了由request模块产生的异常。如果出现了问题，`request`模块便会抛出`error`模块中定义的异常

### 1.3.1 URLError

它具有一个属性reason，即返回错误的原因

```python
from urllib import request, error
try:
	response = request.urlopen('https://cuiqingcai.com/404')
except error.URLError as e:
    print(e.reason)
```

打开一个不存在的页面，捕获了URLError这个异常

输出：`Not Found`

### 1.3.2 HTTPError

它是`URLError`的子类，专门用来处理HTTP请求错误

- 属性
  - code：返回HTTP状态码，比如404表示网页不存在，500表示服务器内部错误等。
  - reason：同父类一样，用于返回错误的原因。
  - headers：返回请求头

- 例

  ```python
  from urllib import request, error
  try:
  	response = request.urlopen('https://cuiqingcai.com/404')
  except error.HTTPError as e:
  	print(e.reason, e.code, e.headers, sep='\n')
  ```

  输出

  ```markdown
  Not Found
  404
  Server: nginx/1.4.6 (Ubuntu)
  Date: Wed, 03 Aug 2016 08:54:22 GMT
  Content-Type: text/html; charset=UTF-8
  Transfer-Encoding: chunked
  Connection: close
  X-Powered-By: PHP/5.5.9-1ubuntu4.14
  Vary: Cookie
  Expires: Wed, 11 Jan 1984 05:00:00 GMT
  Cache-Control: no-cache, must-revalidate, max-age=0
  Pragma: no-cache
  Link: <http://cuiqingcai.com/wp-json/>; rel="https://api.w.org/"
  ```

  捕获了`HTTPError`异常，输出了reason、code和headers属性

- 因为`URLError`是`HTTPError`的父类，所以可以先选择捕获子类的错误，再去捕获父类的错误

  所以上述代码更好的写法如下

  ```python
  from urllib import request, error
  try:
  	response = request.urlopen('https://cuiqingcai.com/404')
  except error.HTTPError as e:
  	print(e.reason, e.code, e.headers, sep='\n')
  except error.URLError as e:
  	print(e.reason)
  else:
  	print('Request Successfully')
  ```

  这样就可以做到先捕获`HTTPError`，获取它的错误状态码、原因、headers等信息。如果不是`HTTPError`异常，就会捕获`URLError`异常，输出错误原因。最后，用`else`来处理正常的逻辑。这是一个较好的异常处理写法

- reason属性返回的不一定是字符串，也可能是一个对象

  ```python
  import socket
  import urllib.request
  import urllib.error
  try:
  	response = urllib.request.urlopen('https://www.baidu.com', timeout=0.01)
  except urllib.error.URLError as e:
  	print(type(e.reason))
  	if isinstance(e.reason, socket.timeout):
  		print('TIME OUT')
  ```

  这里我们直接设置超时时间来强制抛出timeout异常。

  输出

  ```markdown
   <class 'socket.timeout'>
   TIME OUT
  ```

可以发现，`reason`属性的结果是`socket.timeout`类。所以，这里我们可以用`isinstance()`方法来判断它的类型，作出更详细的异常判断

## 1.4 解析链接

urllib库里还提供了parse这个模块，它定义了处理URL的标准接口，例如实现URL各部分的抽取、合并以及链接转换

### 1.4.1 urlparse()

该方法可以实现URL的识别和分段

- 输出urlparse的类型与结果

  ```python
  from urllib.parse import urlparse
  result = urlparse('https://www.baidu.com/index.html;user?id=5#comment')
  print(type(result))
  print(result)
  ```

  ```
  <class 'urllib.parse.ParseResult'>
  ParseResult(scheme='http', netloc='www.baidu.com', path='/index.html', params='user', query='id=5', fragment='comment')
  ```

  返回结果是一个ParseResult类型的对象，它包含6部分，分别是scheme、netloc、path、params、query和fragment

- 标准链接格式

  `scheme://netloc/path;parameters?query#fragment`

- API

  `urllib.parse.urlparse(urlstring, scheme='', allow_fragments=True)`

  - 参数

    1. urlstring：这是必填项，即待解析的URL。

    2. scheme：它是默认的协议（比如http或https等）。假如这个链接没有带协议信息，会将这个作为默认的协议。

    3. allow_fragments：即是否忽略fragment。如果它被设置为False，fragment部分就会被忽略，它会被解析为path、parameters或者query的一部分，而fragment部分为空

  - 说明
    1. 可见，scheme参数只有在URL中不包含scheme信息时才生效。如果URL中有scheme信息，就会返回解析出的scheme
    2. 返回结果ParseResult实际上是一个元组，我们可以用索引顺序来获取，也可以用属性名获取。

### 1.4.2 urlunparse()

其接受的参数是一个可迭代对象，但是它的长度必须是6

```python
from urllib.parse import urlparse
result = urlparse('www.baidu.com/index.html;user?id=5#comment', scheme='https')
print(result)
```

这里参数data用了列表类型。当然，你也可以用其他类型，比如元组或者特定的数据结构

输出：`http://www.baidu.com/index.html;user?a=6#comment`

### 1.4.3 urlsplit()

这个方法和`urlparse()`方法非常相似，只不过它不再单独解析params这一部分，只返回5个结果。上面例子中的params会合并到path中

```python
from urllib.parse import urlsplit
result = urlsplit('http://www.baidu.com/index.html;user?id=5#comment')
print(result)
```

输出：`SplitResult(scheme='http', netloc='www.baidu.com', path='/index.html;user', query='id=5', fragment='comment')`

返回结果是`SplitResult`，它其实也是一个元组类型，既可以用属性获取值，也可以用索引来获取

### 1.4.4 urljoin()

提供一个base_url（基础链接）作为第一个参数，将新的链接作为第二个参数，该方法会分析base_url的scheme、netloc和path这3个内容并对新链接缺失的部分进行补充，最后返回结果

```python
from urllib.parse import urljoin
print(urljoin('http://www.baidu.com', 'FAQ.html'))
print(urljoin('http://www.baidu.com', 'https://cuiqingcai.com/FAQ.html'))
print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html'))
print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html question=2'))
print(urljoin('http://www.baidu.com?wd=abc', 'https://cuiqingcai.com/index.php'))
print(urljoin('http://www.baidu.com', '?category=2#comment'))
print(urljoin('www.baidu.com', '?category=2#comment'))
print(urljoin('www.baidu.com#comment', '?category=2'))
```

输出

```
http://www.baidu.com/FAQ.html
https://cuiqingcai.com/FAQ.html
https://cuiqingcai.com/FAQ.html
https://cuiqingcai.com/FAQ.html?question=2
https://cuiqingcai.com/index.php
http://www.baidu.com?category=2#comment
www.baidu.com?category=2#comment
www.baidu.com?category=2
```

base_url提供了三项内容scheme、netloc和path。如果这3项在新的链接里不存在，就予以补充；如果新的链接存在，就使用新的链接的部分。而base_url中的params、query和fragment是不起作用的

### 1.4.5 urlencode()

它在构造GET请求参数的时候非常有用

```python
from urllib.parse import urlencode
params = {
	'name': 'germey',
	'age': 22
}
base_url = 'http://www.baidu.com?'
url = base_url + urlencode(params)
print(url)
```

输出：`http://www.baidu.com?name=germey&age=22`

这里首先声明了一个字典来将参数表示出来，然后调用`urlencode()`方法将其序列化为GET请求参数。

参数就成功地由字典类型转化为GET请求参数了

有时为了更加方便地构造参数，我们会事先用字典来表示。要转化为URL的参数时，只需要调用该方法即可

### 1.4.6 parse_qs()

有一串GET请求参数，利用parse_qs()方法，就可以将它转回字典

```python
from urllib.parse import parse_qs
query = 'name=germey&age=22'
print(parse_qs(query))
```

输出：`{'name': ['germey'], 'age': ['22']}`

### 1.4.7 parse_qsl()

它用于将参数转化为元组组成的列表

```python
from urllib.parse import parse_qsl
query = 'name=germey&age=22'
print(parse_qsl(query))
```

输出：`[('name', 'germey'), ('age', '22')]`

### 1.4.8 quote()

该方法可以将内容转化为URL编码的格式。URL中带有中文参数时，有时可能会导致乱码的问题，此时用这个方法可以将中文字符转化为URL编码

```python
from urllib.parse import quote
keyword = '壁纸'
url = 'https://www.baidu.com/s?wd=' + quote(keyword)
print(url)
```

声明了一个中文的搜索文字，然后用`quote()`方法对其进行URL编码

输出：[`https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8`](https://www.baidu.com/s?wd=壁纸)

### 1.4.9 unquote()

它可以进行URL解码

```python
from urllib.parse import unquote
url = 'https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8'
print(unquote(url))
```

输出：`https://www.baidu.com/s?wd=壁纸`

## 1.5 使用robotparser分析Robot协议

### 1.5.1 说明

该模块提供了一个类RobotFileParser，它可以根据某网站的robots.txt文件来判断一个爬取爬虫是否有权限来爬取这个网页。

### 1.5.2 声明

```python urllib.robotparser.RobotFileParser(url='')```

可以不传入url，使用`set_url()`设置

### 1.5.3 方法

- **set_url()**

  用来设置robots.txt文件的链接。如果在创建RobotFileParser对象时传入了链接，那么就不需要再使用这个方法设置了。

- **read()**

  读取robots.txt文件并进行分析。注意，这个方法执行一个读取和分析操作，如果不调用这个方法，接下来的判断都会为False，所以一定记得调用这个方法。这个方法不会返回任何内容，但是执行了读取操作。

- **parse()**

  用来解析robots.txt文件，传入的参数是robots.txt某些行的内容，它会按照robots.txt的语法规则来分析这些内容。

- **can_fetch()**

  该方法传入两个参数，第一个是User-agent，第二个是要抓取的URL。返回的内容是该搜索引擎是否可以抓取这个URL，返回结果是True或False。

- **mtime()**

  返回的是上次抓取和分析robots.txt的时间，这对于长时间分析和抓取的搜索爬虫是很有必要的，你可能需要定期检查来抓取最新的robots.txt。

- **modified()**

  它同样对长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取和分析robots.txt的时间

### 1.5.4 例

```python
from urllib.robotparser import RobotFileParser
rp = RobotFileParser()
rp.set_url('https://www.baidu.com/robots.txt')
rp.read()
print(rp.can_fetch('Baiduspider', 'https://www.baidu.com'))
print(rp.can_fetch('Baiduspider', 'https://www.baidu.com/homepage/'))
print(rp.can_fetch('Googlebot', 'https://www.baidu.com/homepage/'))
```

步骤

1. 创建RobotFileParser对象

2. 通过set_url()方法设置了robots.txt的链接

3. 利用can_fetch()方法判断了网页是否可以被抓取

### 1.5.5 使用parser()方法执行读取和分析

```python
from urllib.request import urlopen
from urllib.robotparser import RobotFileParser
rp = RobotFileParser()
rp.parse(urlopen('https://www.baidu.com/robots.txt').read().decode('utf-8').split('\n'))
print(rp.can_fetch('Baiduspider', 'https://www.baidu.com'))
print(rp.can_fetch('Baiduspider', 'https://www.baidu.com/homepage/'))
print(rp.can_fetch('Googlebot', 'https://www.baidu.com/homepage/'))
```

# 二、Handler与Opener

## 2.1 说明

- urllib.request模块里的BaseHandler类，它是所有其他Handler的父类，它提供了最基本的方法

- 有各种Handler子类继承BaseHandler类
  1. HTTPDefaultErrorHandler：用于处理HTTP响应错误，错误都会抛出HTTPError类型的异常
  2. HTTPRedirectHandler：用于处理重定向
  3. HTTPCookieProcessor：用于处理Cookies
  4. ProxyHandler：用于设置代理，默认代理为空
  5. HTTPPasswordMgr：用于管理密码，它维护了用户名和密码的表
  6. HTTPBasicAuthHandler：用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题

## 2.2 验证

请求需要身份验证的界面，借助HTTPBasicAuthHandler就可以完成

- 实现

```python
from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener
from urllib.error import URLError
username = 'admin'
password = 'admin'
url = 'https://static3.scrape.center/'
p = HTTPPasswordMgrWithDefaultRealm()
p.add_password(None, url, username, password)
auth_handler = HTTPBasicAuthHandler(p)
opener = build_opener(auth_handler)
try:
	result = opener.open(url)
	html = result.read().decode('utf-8')
    print(html)
except URLError as e:
    print(e.reason)
```

- 步骤

  1. 首先实例化`HTTPBasicAuthHandler`对象，其参数是`HTTPPasswordMgrWithDefaultRealm`对象，它利用`add_password()`添加进去用户名和密码，这样就建立了一个处理验证的Handler。

  2. 利用这个Handler并使用`build_opener()`方法构建一个Opener，这个Opener在发送请求时就相当于已经验证成功了。

  3. 利用Opener的`open()`方法打开链接，就可以完成验证了。这里获取到的结果就是验证后的页面源码内容

## 2.3 代理

如果要添加代理

```python
from urllib.error import URLError
from urllib.request import ProxyHandler, build_opener
proxy_handler = ProxyHandler({
	'http': 'http://127.0.0.1:9743',
	'https': 'https://127.0.0.1:9743'
})
opener = build_opener(proxy_handler)
try:
	response = opener.open('https://www.baidu.com')
	print(response.read().decode('utf-8'))
except URLError as e:
	print(e.reason)
```

- 步骤
  1. 在本地搭建一个代理，它运行在9743端口上。
  2. 使用`ProxyHandler`，其参数是一个字典，键名是协议类型（比如HTTP或者HTTPS等），键值是代理链接，可以添加多个代理。
  3. 利用这个Handler及`build_opener()`方法构造一个Opener，之后发送请求即可。

## 2.4 Cookies

### 2.4.1 获取网站的Cookies

```python
import http.cookiejar, urllib.request
cookie = http.cookiejar.CookieJar()
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('https://www.baidu.com')
for item in cookie:
	print(item.name + "=" + item.value)
```

### 2.4.2 步骤

1. 声明一个CookieJar对象
2. 利用HTTPCookieProcessor来构建一个Handler
3. 利用build_opener()方法构建出Opener，执行open()函数即可

### 2.4.3 将Cookies以文件形式保存

```python
import urllib.request, http.cookiejar
filename = 'cookie.txt'
cookie = http.cookiejar.MozillaCookieJar(filename)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('https://www.baidu.com')
cookie.save(ignore_discard=True, ignore_expires=True)
```

CookieJar需要换成MozillaCookieJar，它在生成文件时会用到，是CookieJar的子类，可以用来处理Cookies和文件相关的事件，比如读取和保存Cookies，可以将Cookies保存成Mozilla型浏览器的Cookies格式

另外，LWPCookieJar同样可以读取和保存Cookies，但是保存的格式和MozillaCookieJar不一样，它会保存成libwww-perl(LWP)格式的Cookies文件，可以改写成`cookie = http.cookiejar.LWPCookieJar(filename)`

### 2.4.4 生成Cookies文件后，从文件中读取并利用的方法

以LWPCookieJar格式为例

```python
import urllib.request, http.cookiejar
cookie = http.cookiejar.LWPCookieJar()
cookie.load('cookie.txt', ignore_discard=True, ignore_expires=True)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('https://www.baidu.com')
print(response.read().decode('utf-8'))
```

这里调用`load()`方法来读取本地的Cookies文件，获取到了Cookies的内容。读取Cookies之后使用同样的方法构建Handler和Opener即可完成操作。运行结果正常的话，会输出百度网页的源代码

# 三、requests

## 3.1 GET请求

### 3.1.1 基本操作

以返回的是JSON形式的字符串的请求链接为例

```python
import requests
r = requests.get('http://httpbin.org/get')
print(r.text)
```

输出

```
{
  "args": {}, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.10.0"
  }, 
  "origin": "122.4.215.33", 
  "url": "http://httpbin.org/get"
  }
```

成功发起了GET请求，返回结果中包含请求头、URL、IP等信息

### 3.1.2 添加额外参数

若要添加params

```python
import requests
data = {
	'name': 'germey',
	'age': 22
}
r = requests.get("http://httpbin.org/get", params=data)
print(r.text)
```

输出

```
{
  "args": {
    "age": "22", 
    "name": "germey"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.10.0"
  }, 
  "origin": "122.4.215.33", 
  "url": "http://httpbin.org/get?age=22&name=germey"
}
```

### 3.1.3 将返回结果是JSON格式的字符串转化为字典

```python
print(r.json())
```

如果返回结果不是JSON格式，便会出现解析错误，抛出`json.decoder.JSONDecodeError`异常

## 3.2 抓取网页

```python
import requests
import re
headers = {
'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'
		}
r = requests.get("https://www.zhihu.com/explore", headers=headers)
pattern = re.compile('explore-feed.*?question_link.*?>(.*?)</a>', re.S)
titles = re.findall(pattern, r.text)
print(titles)
```

- 步骤

  1. 加入了headers信息，其中包含了User-Agent字段信息，也就是浏览器标识信息。如果不加这个，知乎会禁止抓取
  2. 利用正则表达式来匹配出所有的问题内容

- 输出

  ```
  ['\n为什么很多人喜欢提及「拉丁语系」这个词？\n', '\n在没有水的情况下水系宝可梦如何战斗？\n', '\n有哪些经验可以送给 Kindle 新人？\n', '\n谷歌的广告业务是如何赚钱的？\n', '\n程序员该学习什么，能在上学期间挣钱？\n', '\n有哪些原本只是一个小消息，但回看发现是个惊天大新闻的例子？\n', '\n如何评价今敏？\n', '\n源氏是怎么把那么长的刀从背后拔出来的？\n', '\n年轻时得了绝症或大病是怎样的感受？\n', '\n年轻时得了绝症或大病是怎样的感受？\n']
  ```

## 3.3 抓取二进制数据

以GitHub的站点图标为例

```python
import requests
r = requests.get("https://github.com/favicon.ico")
print(r.content)
```

输出一段二进制数据

- 保存

```python
import requests
r = requests.get("https://github.com/favicon.ico")
with open('favicon.ico', 'wb') as f:
    f.write(r.content)
```

## 3.4 添加headers

对于一些网站，如果不传递headers，就不能正常请求

```python
import requests
headers = {
	'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'
		}
r = requests.get("https://www.zhihu.com/explore", headers=headers)
print(r.text)
```

可以在headers这个参数中任意添加其他的字段信息

## 3.5 POST请求

- 例：请求http://httpbin.org/post，该网站可以判断如果请求是POST方式，就把相关请求信息返回

```python
import requests
data = {'name': 'germey', 'age': '22'}
r = requests.post("http://httpbin.org/post", data=data)
print(r.text)
```

输出

```
{
	"args": {},
	"data": "",
	"files": {},
	"form": {
		"age": "22",
		"name": "germey"
	},
	"headers": {
		"Accept": "*/*",
		"Accept-Encoding": "gzip, deflate",
		"Content-Length": "18",
		"Content-Type": "application/x-www-form-urlencoded",
		"Host": "httpbin.org",
		"User-Agent": "python-requests/2.24.0",
		"X-Amzn-Trace-Id": "Root=1-5f9836ad-000ca9c96ac917d419407277"
	},
	"json": null,
	"origin": "59.79.2.148",
	"url": "http://httpbin.org/post"
}
```

其中form部分就是提交的数据，这就证明POST请求成功发送了

## 3.6 响应

### 3.6.1 例

```python
import requests
r = requests.get('https://static1.scrape.center/')
print(type(r.status_code), r.status_code)
print(type(r.headers), r.headers)
print(type(r.cookies), r.cookies)
print(type(r.url), r.url)
print(type(r.history), r.history)
```

### 2.6.2 输出

```
<class 'int'> 500
<class 'requests.structures.CaseInsensitiveDict'> {'Server': 'nginx/1.17.8', 'Date': 'Tue, 27 Oct 2020 15:10:32 GMT', 'Content-Type': 'text/html', 'Content-Length': '145', 'Connection': 'keep-alive', 'X-Frame-Options': 'DENY', 'Vary': 'Cookie', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains'}
<class 'requests.cookies.RequestsCookieJar'> <RequestsCookieJar[]>
<class 'str'> https://static1.scrape.center/
<class 'list'> []
```

输出`status_code`属性得到状态码

输出`headers`属性得到响应头

输出`cookies`属性得到Cookies

输出`url`属性得到URL

输出`history`属性得到请求历史

### 3.6.3 状态码查询对象`requests.codes`

requests还提供了一个内置的状态码查询对象`requests.codes`

```python
import requests
r = requests.get('https://static1.scrape.center/')
exit() if not r.status_code == requests.codes.ok else print('Request Successfully')
```

通过比较返回码和内置的成功的返回码，来保证请求得到了正常响应，输出成功请求的消息，否则程序终止，这里我们用`requests.codes.ok`得到的是成功的状态码200

### 3.6.4 返回码和相应的查询条件

```python
# 信息性状态码
100: ('continue',),
101: ('switching_protocols',),
102: ('processing',),
103: ('checkpoint',),
122: ('uri_too_long', 'request_uri_too_long'),

# 成功状态码
200: ('ok', 'okay', 'all_ok', 'all_okay', 'all_good', '\\o/', '✓'),
201: ('created',),
202: ('accepted',),
203: ('non_authoritative_info', 'non_authoritative_information'),
204: ('no_content',),
205: ('reset_content', 'reset'),
206: ('partial_content', 'partial'),
207: ('multi_status', 'multiple_status', 'multi_stati', 'multiple_stati'),
208: ('already_reported',),
226: ('im_used',),

# 重定向状态码
300: ('multiple_choices',),
301: ('moved_permanently', 'moved', '\\o-'),
302: ('found',),
303: ('see_other', 'other'),
304: ('not_modified',),
305: ('use_proxy',),
306: ('switch_proxy',),
307: ('temporary_redirect', 'temporary_moved', 'temporary'),
308: ('permanent_redirect',
      'resume_incomplete', 'resume',), # These 2 to be removed in 3.0

# 客户端错误状态码
400: ('bad_request', 'bad'),
401: ('unauthorized',),
402: ('payment_required', 'payment'),
403: ('forbidden',),
404: ('not_found', '-o-'),
405: ('method_not_allowed', 'not_allowed'),
406: ('not_acceptable',),
407: ('proxy_authentication_required', 'proxy_auth', 'proxy_authentication'),
408: ('request_timeout', 'timeout'),
409: ('conflict',),
410: ('gone',),
411: ('length_required',),
412: ('precondition_failed', 'precondition'),
413: ('request_entity_too_large',),
414: ('request_uri_too_large',),
415: ('unsupported_media_type', 'unsupported_media', 'media_type'),
416: ('requested_range_not_satisfiable', 'requested_range', 'range_not_satisfiable'),
417: ('expectation_failed',),
418: ('im_a_teapot', 'teapot', 'i_am_a_teapot'),
421: ('misdirected_request',),
422: ('unprocessable_entity', 'unprocessable'),
423: ('locked',),
424: ('failed_dependency', 'dependency'),
425: ('unordered_collection', 'unordered'),
426: ('upgrade_required', 'upgrade'),
428: ('precondition_required', 'precondition'),
429: ('too_many_requests', 'too_many'),
431: ('header_fields_too_large', 'fields_too_large'),
444: ('no_response', 'none'),
449: ('retry_with', 'retry'),
450: ('blocked_by_windows_parental_controls', 'parental_controls'),
451: ('unavailable_for_legal_reasons', 'legal_reasons'),
499: ('client_closed_request',),

# 服务端错误状态码
500: ('internal_server_error', 'server_error', '/o\\', '✗'),
501: ('not_implemented',),
502: ('bad_gateway',),
503: ('service_unavailable', 'unavailable'),
504: ('gateway_timeout',),
505: ('http_version_not_supported', 'http_version'),
506: ('variant_also_negotiates',),
507: ('insufficient_storage',),
509: ('bandwidth_limit_exceeded', 'bandwidth'),
510: ('not_extended',),
511: ('network_authentication_required', 'network_auth', 'network_authentication')
```

## 3.7 文件上传

用favicon.ico来模拟文件上传的过程

```python
import requests
files = {'file': open('favicon.ico', 'rb')}
r = requests.post("http://httpbin.org/post", files=files)
print(r.text)
```

- 输出

```
{
  "args": {}, 
  "data": "", 
  "files": {
    "file": "data:application/octet-stream;base64,AAAAAA...="
  }, 
  "form": {}, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "6665", 
    "Content-Type": "multipart/form-data; boundary=809f80b1a2974132b133ade1a8e8e058", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.10.0"
  }, 
  "json": null, 
  "origin": "60.207.237.16", 
  "url": "http://httpbin.org/post"
}
```

网站会返回响应，里面包含files这个字段，而form字段是空的，这证明文件上传部分会单独有一个files字段来标识

## 3.8 获取 Cookies

### 3.8.1 例

```python
import requests
r = requests.get("https://www.baidu.com")
print(r.cookies)
for key, value in r.cookies.items():
	 print(key + '=' + value)
```

- 输出

```
<RequestsCookieJar[<Cookie BDORZ=27315 for .baidu.com/>]>
BDORZ=27315
```

- 步骤
  1. 调用cookies属性，得到Cookies，可以发现它是RequestCookieJar类型
  2. 用items()方法将其转化为元组组成的列表，遍历输出每一个Cookie的名称和值，实现Cookie的遍历解析

### 3.8.2 用Cookie来维持登录状态

- Headers中的Cookie内容复制下来
- 将Cookie设置到headers中

```python
import requests
headers = {
		'Cookie': '_zap=35740c……76',
		'Host': 'www.zhihu.com',
		'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36',
}
r = requests.get('https://www.zhihu.com', headers=headers)
print(r.text)
```

结果中包含了登录后的结果

- 通过cookies参数来设置（较繁琐）

```python
import requests
cookies = 'q_e=......fdb5763b0'
jar = requests.cookies.RequestsCookieJar()
headers = {
	'Host': 'www.zhihu.com',
	'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'
}
for cookie in cookies.split(';'):
    key, value = cookie.split('=', 1)
	jar.set(key, value)
r = requests.get("http://www.zhihu.com", cookies=jar, headers=headers)
print(r.text)
```

首先新建了一个`RequestCookieJar`对象，然后将复制下来的cookies利用`split()`方法分割，接着利用`set()`方法设置好每个Cookie的key和value，然后通过调用requests的`get()`方法并传递给cookies参数即可

## 3.9 会话维持

如果直接利用`get()`或`post()`等方法的确可以做到模拟网页的请求，但是这实际上是相当于不同的会话，也就是说相当于用了两个浏览器打开了不同的页面

利用Session对象，可以方便地维护一个会话，而且不用担心cookies的问题，它会帮我们自动处理好

```python
import requests
s = requests.Session()
s.get('http://httpbin.org/cookies/set/number/123456789')
r = s.get('http://httpbin.org/cookies')
print(r.text)
```

输出

```
{
  "cookies": {
    "number": "123456789"
  }
}
```

用Session，可以做到模拟同一个会话而不用担心Cookies的问题。它通常用于模拟登录成功之后再进行下一步的操作

## 3.10 SSL证书验证

requests还提供了证书验证的功能。当发送HTTP请求的时候，它会检查SSL证书，我们可以使用verify参数控制是否检查此证书。其实如果不加verify参数的话，默认是True，会自动验证

请求一个HTTPS站点，但是证书验证错误的页面时，把verify参数设置为False即可

```python
import requests
response = requests.get('https://www.12306.cn', verify=False)
print(response.status_code)
```

打印出请求成功的状态码

```
D:\Application\Anaconda\lib\site-packages\urllib3\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.12306.cn'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
InsecureRequestWarning,
200
```

报了一个警告，它建议我们给它指定证书

### 3.10.1 可以通过设置忽略警告的方式来屏蔽这个警告

```python
import requests
from requests.packages import urllib3
urllib3.disable_warnings()
response = requests.get('https://www.12306.cn', verify=False)
print(response.status_code)
```

### 3.10.2 或者通过捕获警告到日志的方式忽略警告

```python
import logging
import requests
logging.captureWarnings(True)
response = requests.get('https://www.12306.cn', verify=False)
print(response.status_code)
```

### 3.10.3 可以指定一个本地证书用作客户端证书

这可以是单个文件（包含密钥和证书）或一个包含两个文件路径的元组

```python
import requests
response = requests.get('https://www.12306.cn',
                        cert=('/path/server.crt', '/path/key'))
print(response.status_code)
```

我们需要有crt和key文件，并且指定它们的路径。注意，本地私有证书的key必须是解密状态，加密状态的key是不支持的

## 3.11 代理设置

**使用proxies参数**

```python
import requests
proxies = {
	"http": "http://10.10.1.10:3128",
	"https": "http://10.10.1.10:1080",
}
requests.get("https://www.taobao.com", proxies=proxies)
```

若代理需要使用HTTP Basic Auth，可以使用类似[http://user](http://user/):password@host:port这样的语法来设置代理

```python
import requests
proxies = {
    "http": "http://user:password@10.10.1.10:3128/",
}
requests.get("https://www.taobao.com", proxies=proxies)
```

使用SOCKS协议代理

```python
import requests
proxies = {
	'http': 'socks5://user:password@host:port',
	'https': 'socks5://user:password@host:port'
}
requests.get("https://www.taobao.com", proxies=proxies)
```

## 3.12 超时设置

使用timeout参数。这个时间的计算是发出请求到服务器返回响应的时间

```python
import requests
r = requests.get("https://www.taobao.com", timeout=1)
print(r.status_code)
```

- 请求分为两个阶段，即连接（connect）和读取（read）。上面设置的timeout将用作连接和读取这二者的timeout总和

- 如果要分别指定，就可以传入一个元组

```python
r = requests.get('https://www.taobao.com', timeout=(5, 11, 30))
```

- 如果想永久等待，可以直接将timeout设置为None，或者不设置直接留空，因为默认是None

## 3.13 身份认证

```python
import requests
from requests.auth import HTTPBasicAuth
r = requests.get('http://localhost:5000',
                 auth=HTTPBasicAuth('username', 'password'))
print(r.status_code)
```

如果用户名和密码正确的话，请求时就会自动认证成功，会返回200状态码，如果认证失败，则返回401状态码

### 3.13.1 直接传一个元组

它会默认使用HTTPBasicAuth这个类来认证

```python
import requests
r = requests.get('http://localhost:5000', auth=('username', 'password'))
print(r.status_code)
```

### 3.13.2 使用OAuth1认证

```python
import requests
from requests_oauthlib import OAuth1
url = 'https://api.twitter.com/1.1/account/verify_credentials.json'
auth = OAuth1('YOUR_APP_KEY', 'YOUR_APP_SECRET',
              'USER_OAUTH_TOKEN', 'USER_OAUTH_TOKEN_SECRET')
requests.get(url, auth=auth)
```

## 3.14 Prepared Request

将请求表示为数据结构，这个数据结构就叫Prepared Request（一个对象）

```python
rom requests import Request, Session
url = 'http://httpbin.org/post'
data = {
    'name': 'germey'
}
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'
}
s = Session()
req = Request('POST', url, data=data, headers=headers)
prepped = s.prepare_request(req)
r = s.send(prepped)
print(r.text)
```

输出

```
{
  "args": {},
  "data": "",
  "files": {},
  "form": {
    "name": "germey"
  },
  "headers": {
    "Accept": "*/*",
    "Accept-Encoding": "gzip, deflate",
    "Content-Length": "11",
    "Content-Type": "application/x-www-form-urlencoded",
    "Host": "httpbin.org",
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36",
    "X-Amzn-Trace-Id": "Root=1-5f9984be-6ac8c46848dbd62c7bd3b022"
  },
  "json": null,
  "origin": "59.79.2.148",
  "url": "http://httpbin.org/post"
}
```

这里我们引入了`Request`，然后用url、data和headers参数构造了一个`Request`对象，这时需要再调用Session的`prepare_request()`方法将其转换为一个`Prepared Request`对象，然后调用`send()`方法发送即可

# 四、正则表达式

## 4.1 概念

正则表达式(regular expression)描述了一种字符串匹配的模式（pattern），可以用来检查一个串是否含有某种子串、将匹配的子串替换或者从某个串中取出符合某个条件的子串等

## 4.2 符号

### 4.2.1 普通字符

普通字符包括没有显式指定为元字符的所有可打印和不可打印字符。这包括所有大写和小写字母、所有数字、所有标点符号和一些其他符号

| **字符** | **描述**                                                     |
| -------- | ------------------------------------------------------------ |
| [ABC]    | 匹配 **[...]** 中的所有字符，例如 **[aeiou]** 匹配字符串 "google runoob  taobao" 中所有的 e o u a 字母。  ![image-20201206192920315](https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20201206192920315.png) |
| \[^ABC]  | 匹配除了 **[...]** 中字符的所有字符，例如 **[^aeiou]** 匹配字符串 "google runoob  taobao" 中除了 e o u a 字母的所有字母  ![image-20201206193003594](https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20201206193003594.png) |
| [A-Z]    | [A-Z]  表示一个区间，匹配所有大写字母，[a-z] 表示所有小写字母             ![image-20201206192825611](https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20201206192825611.png) |
| **.**    | 匹配除换行符（\n、\r）之外的任何单个字符，相等于  \[^\n\r]                          ![image-20201206193042154](https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20201206193042154.png) |
| [\s\S]   | 匹配所有。\s  是匹配所有空白符，包括换行，\S 非空白符，包括换行。    ![image-20201206193101636](https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20201206193101636.png) |
| \w       | 匹配字母、数字、下划线。等价于  [A-Za-z0-9_]                                     ![image-20201206193134832](https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20201206193134832.png) |

### 4.2.2 非打印字符

| **字符** | **描述**                                                     |
| -------- | ------------------------------------------------------------ |
| \cx      | 匹配由x指明的控制字符。例如，  \cM 匹配一个 Control-M 或回车符。x 的值必须为 A-Z 或 a-z 之一。否则，将 c 视为一个原义的 'c' 字符。 |
| \f       | 匹配一个换页符。等价于  \x0c 和 \cL。                        |
| \n       | 匹配一个换行符。等价于  \x0a 和 \cJ。                        |
| \r       | 匹配一个回车符。等价于  \x0d 和 \cM。                        |
| \s       | 匹配任何空白字符，包括空格、制表符、换页符等等。等价于  [ \f\n\r\t\v]。注意 Unicode 正则表达式会匹配全角空格符。 |
| \S       | 匹配任何非空白字符。等价于  [^ \f\n\r\t\v]。                 |
| \t       | 匹配一个制表符。等价于  \x09 和 \cI。                        |
| \v       | 匹配一个垂直制表符。等价于  \x0b 和 \cK。                    |

### 4.2.3 特殊字符

许多元字符要求在试图匹配它们时特别对待。若要匹配这些特殊字符，必须首先使字符"转义"，即，将反斜杠字符**\\** 放在它们前面

| **特别字符** | **描述**                                                     |
| ------------ | ------------------------------------------------------------ |
| $            | 匹配输入字符串的结尾位置。如果设置了  RegExp 对象的 Multiline 属性，则 $ 也匹配 '\n' 或 '\r'。 |
| ( )          | 标记一个子表达式的开始和结束位置。子表达式可以获取供以后使用。 |
| *            | 匹配前面的子表达式零次或多次。                               |
| +            | 匹配前面的子表达式一次或多次。                               |
| .            | 匹配除换行符  \n 之外的任何单字符。                          |
| [            | 标记一个中括号表达式的开始。                                 |
| ?            | 匹配前面的子表达式零次或一次，或指明一个非贪婪限定符。       |
| \            | 将下一个字符标记为或特殊字符、或原义字符、或向后引用、或八进制转义符。例如，  'n' 匹配字符 'n'。'\n' 匹配换行符。序列 '\\' 匹配 "\"，而 '\(' 则匹配 "("。 |
| ^            | 匹配输入字符串的开始位置，除非在方括号表达式中使用，当该符号在方括号表达式中使用时，表示不接受该方括号表达式中的字符集合。 |
| {            | 标记限定符表达式的开始。                                     |
| \|           | 指明两项之间的一个选择。                                     |

### 4.2.4 限定符

限定符用来指定正则表达式的一个给定组件必须要出现多少次才能满足匹配。有 ***** 或 **+** 或 **?** 或 **{n}** 或 **{n,}** 或 **{n,m}** 共6种

| **字符** | **描述**                                                     |
| -------- | ------------------------------------------------------------ |
| *        | 匹配前面的子表达式零次或多次。例如，zo*  能匹配 "z" 以及 "zoo"。* 等价于{0,}。 |
| +        | 匹配前面的子表达式一次或多次。例如，'zo+'  能匹配 "zo" 以及 "zoo"，但不能匹配 "z"。+ 等价于 {1,}。 |
| ?        | 匹配前面的子表达式零次或一次。例如，"do(es)?"  可以匹配 "do" 、 "does" 中的 "does" 、 "doxy"  中的 "do" 。? 等价于 {0,1}。 |
| {n}      | n  是一个非负整数。匹配确定的 n 次。例如，'o{2}' 不能匹配 "Bob" 中的 'o'，但是能匹配  "food" 中的两个 o。 |
| {n,}     | n  是一个非负整数。至少匹配n 次。例如，'o{2,}' 不能匹配 "Bob" 中的 'o'，但能匹配  "foooood" 中的所有 o。'o{1,}' 等价于 'o+'。'o{0,}' 则等价于 'o*'。 |
| {n,m}    | m 和 n  均为非负整数，其中n <= m。最少匹配 n 次且最多匹配 m 次。例如，"o{1,3}" 将匹配  "fooooood" 中的前三个 o。'o{0,1}' 等价于 'o?'。请注意在逗号和两个数之间不能有空格。 |

### 4.2.5 定位符

定位符用来描述字符串或单词的边界，**^** 和 **$** 分别指字符串的开始与结束，**\b** 描述单词的前或后边界，**\B** 表示非单词边界

| **字符** | **描述**                                                     |
| -------- | ------------------------------------------------------------ |
| ^        | 匹配输入字符串开始的位置。如果设置了  RegExp 对象的 Multiline 属性，^ 还会与 \n 或 \r 之后的位置匹配。 |
| $        | 匹配输入字符串结尾的位置。如果设置了  RegExp 对象的 Multiline 属性，$ 还会与 \n 或 \r 之前的位置匹配。 |
| \b       | 匹配一个单词边界，即字与空格间的位置。                       |
| \B       | 非单词边界匹配。                                             |

**注意**

1. 不能将限定符与定位符一起使用。由于在紧靠换行或者单词边界的前面或后面不能有一个以上位置，因此不允许诸如 **^\*** 之类的表达式。
2. 若要匹配一行文本开始处的文本，请在正则表达式的开始使用 **^** 字符。不要将 **^** 的这种用法与中括号表达式内的用法混淆。
3. 若要匹配一行文本的结束处的文本，请在正则表达式的结束处使用 **$** 字符。
4. **\b** 字符的位置是非常重要的。如果它位于要匹配的字符串的开始，它在单词的开始处查找匹配项。如果它位于字符串的结尾，它在单词的结尾处查找匹配项
5. 对于 **\B** 非单词边界运算符，位置并不重要，因为匹配不关心究竟是单词的开头还是结尾

### 4.2.6 选择

用圆括号 **()** 将所有选择项括起来，相邻的选择项之间用 **|** 分隔。

**()** 表示捕获分组，**()** 会把每个分组里的匹配的值保存起来， 多个匹配值可以通过数字 n 来查看(**n** 是一个数字，表示第 n 个捕获组的内容)

## 4.3 python常用方法

### 4.3.1 match()

向它传入要匹配的字符串以及正则表达式，就可以检测这个正则表达式是否匹配字符串

`match()`方法会尝试从字符串的起始位置匹配正则表达式，如果匹配，就返回匹配成功的结果；如果不匹配，就返回None

```python
import re
content = 'Hello 123 4567 World_This is a Regex Demo'
print(len(content))
result = re.match('^Hello\s\d\d\d\s\d{4}\s\w{10}', content)
print(result)
print(result.group())
print(result.span())
```

输出

```
41
<_sre.SRE_Match object; span=(0, 25), match='Hello 123 4567 World_This'>
Hello 123 4567 World_This
(0, 25)
```

步骤

1. 在`match()`方法中，第一个参数传入了正则表达式，第二个参数传入了要匹配的字符串

2. `group()`方法可以输出匹配到的内容，结果是`Hello 123 4567 World_This`

   `span()`方法可以输出匹配的范围，结果是`(0, 25)`

也可这样写

```python
result = re.match('^Hello.*Demo$', content)
```

### 4.3.2 使用()括号将想提取的子字符串括起来

```python
import re
content = 'Hello 1234567 World_This is a Regex Demo'
result = re.match('^Hello\s(\d+)\sWorld', content)
print(result)
print(result.group())
print(result.group(1))
print(result.span())
```

输出

```
<_sre.SRE_Match object; span=(0, 19), match='Hello 1234567 World'>
Hello 1234567 World
1234567
(0, 19)
```

若想把字符串中的1234567提取出来，此时可以将数字部分的正则表达式用()括起来，然后调用了group(1)获取匹配结果

也可这样写

```python
result = re.match('^He.*?(\d+).*Demo$', content)
```

### 4.3.3 修饰符

遇到换行符时，使用修饰符`re.S`进行匹配，即可用“.”匹配换行符

```python
result = re.match('^He.*?(\d+).*?Demo$', content, re.S)
```

其他常用修饰符

![image-20201206194553409](https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20201206194553409.png)

### 4.3.4 search()

`match()`方法是从字符串的开头开始匹配的，一旦开头不匹配，那么整个匹配就失败了

`search()`，它在匹配时会扫描整个字符串，然后返回第一个成功匹配的结果，搜索完了还没有找到，就返回None

为了匹配方便，尽量使用`search()`方法

### 4.3.5 findall()

该方法会搜索整个字符串，然后返回匹配正则表达式的所有内容

### 4.3.6 sub()

替换指定文本

如要将文本中的数字去掉

```python
import re
content = '54aK54yr5oiR54ix5L2g'
content = re.sub('\d+', '', content)
print(content)
```

输出

```
aKyroiRixLg
```

只需要给第一个参数传入\d+来匹配所有的数字，第二个参数为替换成的字符串（如果去掉该参数的话，可以赋值为空），第三个参数是原字符串

### 4.3.7 compile()

这个方法可以将正则字符串编译成正则表达式对象，以便在后面的匹配中复用

```python
import re
content1 = '2016-12-15 12:00'
content2 = '2016-12-17 12:55'
content3 = '2016-12-22 13:21'
pattern = re.compile('\d{2}:\d{2}')
result1 = re.sub(pattern, '', content1)
result2 = re.sub(pattern, '', content2)
result3 = re.sub(pattern, '', content3)
print(result1, result2, result3)
```

输出

```
2016-12-15  2016-12-17  2016-12-22
```

例如，这里有3个日期，我们想分别将3个日期中的时间去掉，这时可以借助`sub()`方法。该方法的第一个参数是正则表达式，但是这里没有必要重复写3个同样的正则表达式，此时可以借助`compile()`方法将正则表达式编译成一个正则表达式对象，以便复用