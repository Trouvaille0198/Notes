# 一、urllib

## 1.1 模块介绍

1. request：它是最基本的HTTP请求模块，可以用来模拟发送请求。就像在浏览器里输入网址然后回车一样，只需要给库方法传入URL以及额外的参数，就可以模拟实现这个过程了。

2. error：异常处理模块，如果出现请求错误，我们可以捕获这些异常，然后进行重试或其他操作以保证程序不会意外终止。

3. parse：一个工具模块，提供了许多URL处理方法，比如拆分、解析、合并等。

4. robotparser：主要是用来识别网站的robots.txt文件，然后判断哪些网站可以爬，哪些网站不可以爬，它其实用得比较少

## 1.2 发送请求

使用urllib.equest模块，我们可以方便地实现请求的发送并得到响应

### 1.2.1 urlopen()

``` python
import urllib.request
response = urllib.request.urlopen("https://www.python.org")
print(response.read().decode('utf-8'))
```

输出：网页源代码

#### 1）利用type()方法输出响应的类型

``` python
print(type(response))
```

输出：`<class 'http.client.HTTPResponse'>`

可以发现，它是一个HTTPResposne类型的对象。它主要包含read()、readinto()、getheader(name)、getheaders()、fileno()等方法，msg、version、status、reason、debuglevel、closed等属性

#### 2）调用方法和属性

```python
print(response.status)
print(response.getheaders())
print(response.getheader('Server'))
```

输出：响应的状态码，响应的头信息，响应头中的Server值

``` markdown
200
 [('Server', 'nginx'), ('Content-Type', 'text/html; charset=utf-8')……('Strict-Transport-Security', 'max-age=63072000; includeSubDomains')]
 nginx
```

#### 3）urlopen()函数的API

`urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)`

1. data参数

   - 如果传递了这个参数，则它的请求方式就不再是GET方式，而是POST方式

   - 如果它是字节流编码格式（bytes类型），则需要通过bytes()方法转化

   - 例

     ``` python
     import urllib.parse
     import urllib.request
     data = bytes(urllib.parse.urlencode({'name': 'germey'}), encoding='utf-8')
     response = urllib.request.urlopen('https://httpbin.org/post', data=data)
     print(response.read().decode('utf-8'))
     ```

     第一个参数需要是str类型，需要用urllib.parse模块里的urlencode()方法来将参数字典转化为字符串

     第二个参数指定编码格式，这里指定为utf8

     输出：

     ``` markdown
     {
     	"args": {},
     	"data": "",
     	"files": {},
     	"form": {
     			"name": "germey"
     			},
     	"headers": {
     	    "Accept-Encoding": "identity",
     		"Content-Length": "11",
     		"Content-Type": "application/x-www-form-urlencoded",
     		"Host": "httpbin.org",
     		"User-Agent": "Python-urllib/3.7",
     		"X-Amzn-Trace-Id": "Root=1-5f96de5b-6805a3ac7b7b4b151a11fdc6"
     			},
     	"json": null,
     	"origin": "59.79.2.148",
     	"url": "https://httpbin.org/post"
     }
     ```

2. timeout参数

   用于设置超时时间，单位为秒，意思就是如果请求超出了设置的这个时间，还没有得到响应，就会抛出异常。如果不指定该参数，就会使用全局默认时间。它支持HTTP、HTTPS、FTP请求。

   - 例一

     ``` python
     import urllib.request
     response = urllib.request.urlopen('https://httpbin.org/get', timeout=0.1)
     print(response.read())
     ```

     程序1秒过后，服务器依然没有响应，于是抛出了URLError异常。该异常属于urllib.error模块，错误原因是超时

   - 例二

     可以通过设置这个超时时间来控制一个网页如果长时间未响应，就跳过它的抓取。这可以利用try except语句来实现

     ``` python
     import urllib.request
     import socket
     import urllib.error
     try:
     	response = urllib.request.urlopen('http://httpbin.org/get', timeout=0.1)
     except urllib.error.URLError as e:
     	if isinstance(e.reason, socket.timeout):
     		print('TIME OUT')
     ```

     这里我们请求了<http://httpbin.org/get>测试链接，设置超时时间是0.1秒，然后捕获了URLError异常，接着判断异常是socket.timeout类型（意思就是超时异常），从而得出它确实是因为超时而报错，打印输出了TIME OUT

3. 其他参数
   - context参数，它必须是ssl.SSLContext类型，用来指定SSL设置。
   - cafile和capath这两个参数分别指定CA证书和它的路径，这个在请求HTTPS链接时会有用。
   - cadefault参数现在已经弃用了，其默认值为False

### 2.2 Request

如果请求中需要加入Headers等信息，就可以利用更强大的Request类来构建

``` python
import urllib.request
request = urllib.request.Request('https://python.org')
response = urllib.request.urlopen(request)
print(response.read().decode('utf-8'))
```

我们依然是用urlopen()方法来发送这个请求，只不过这次该方法的参数不再是URL，而是一个Request类型的对象。通过构造这个数据结构，一方面我们可以将请求独立成一个对象，另一方面可更加丰富和灵活地配置参数

#### 1） 使用参数来构造Request

`class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)`

1. url用于请求URL，这是必传参数，其他都是可选参数

2. data如果要传，必须传bytes（字节流）类型。如果它是字典，可以先用urllib.parse模块里的urlencode()编码

3. headers是一个字典，它就是请求头，可以在构造请求时通过headers参数直接构造，也可以通过调用请求实例的add_header()方法添加

   添加请求头最常用的用法就是通过修改User-Agent来伪装浏览器，默认的User-Agent是Python-urllib，我们可以通过修改它来伪装浏览器。比如要伪装火狐浏览器，你可以把它设置为`Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11`

4. origin_req_host指的是请求方的host名称或者IP地址。

5. unverifiable表示这个请求是否是无法验证的，默认是False，意思就是说用户有足够权限来选择接收这个请求的结果。

   例如，我们请求一个HTML文档中的图片，但是我们没有自动抓取图像的权限，这时unverifiable的值就是True。

6. method是一个字符串，用来指示请求使用的方法，比如GET、POST和PUT等

   例

   ```python
   from urllib import request, parse
   url = 'https://httpbin.org/post'
   headers = {
   	'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',
   	'Host': 'httpbin.org'
   }
   dict = {'name': 'germey'}
   data = bytes(parse.urlencode(dict), encoding='utf-8')
   req = request.Request(url=url, data=data, headers=headers, method='POST')
   response = request.urlopen(req)
   print(response.read().decode('utf-8'))
   ```

   其中url即请求URL，headers中指定了User-Agent和Host，参数data用urlencode()和bytes()方法转成字节流。另外，指定了请求方式为POST

   输出

   ```markdown
   {
   	"args": {},
   	"data": "",
   	"files": {},
   	"form": {
   			"name": "germey"
   			      },
   	"headers": {
   			"Accept-Encoding": "identity",
   		 	"Content-Length": "11",
   		 	"Content-Type": "application/x-www-form-urlencoded",
   			"Host": "httpbin.org",
   			"User-Agent": "Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)",
   			"X-Amzn-Trace-Id": "Root=1-5f96e6c8-0c429d0c205ce45b3365653e"
   			         },
   	"json": null,
   	"origin": "59.79.2.148",
   	"url": "https://httpbin.org/post"
   }
   
   ```

   另外，headers也可以用add_header()方法来添加

   ```python
   req = request.Request(url=url, data=data, method='POST')
   req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MISE 5.5; Windows NT)')
   ```

### 2.3 高级操作

1. 对于一些更高级的操作（比如Cookies处理、代理设置等），需要使用更强大的工具Handler来实现。我们可以把Handler理解为各种处理器，有专门处理登录验证的，有处理Cookies的，有处理代理设置的。利用它们，我们几乎可以做到HTTP请求中所有的事情

2. 另一个比较重要的类就是`OpenerDirector`（Opener）`urlopen()`实际上就是urllib为我们提供的一个Opener。之前使用的Request和urlopen()相当于类库为你封装好了极其常用的请求方法，利用它们可以完成基本的请求。Opener可以使用`open()`方法，返回的类型和`urlopen()`如出一辙。可以利用Handler来构建Opener。

## 1.3 处理异常

`urllib`的`error`模块定义了由request模块产生的异常。如果出现了问题，`request`模块便会抛出`error`模块中定义的异常

### 1.3.1 URLError

它具有一个属性reason，即返回错误的原因

```python
from urllib import request, error
try:
	response = request.urlopen('https://cuiqingcai.com/404')
except error.URLError as e:
    print(e.reason)
```

打开一个不存在的页面，捕获了URLError这个异常

输出：`Not Found`

### 1.3.2 HTTPError

它是`URLError`的子类，专门用来处理HTTP请求错误

- 属性
  - code：返回HTTP状态码，比如404表示网页不存在，500表示服务器内部错误等。
  - reason：同父类一样，用于返回错误的原因。
  - headers：返回请求头

- 例

  ```python
  from urllib import request, error
  try:
  	response = request.urlopen('https://cuiqingcai.com/404')
  except error.HTTPError as e:
  	print(e.reason, e.code, e.headers, sep='\n')
  ```

  输出

  ```markdown
  Not Found
  404
  Server: nginx/1.4.6 (Ubuntu)
  Date: Wed, 03 Aug 2016 08:54:22 GMT
  Content-Type: text/html; charset=UTF-8
  Transfer-Encoding: chunked
  Connection: close
  X-Powered-By: PHP/5.5.9-1ubuntu4.14
  Vary: Cookie
  Expires: Wed, 11 Jan 1984 05:00:00 GMT
  Cache-Control: no-cache, must-revalidate, max-age=0
  Pragma: no-cache
  Link: <http://cuiqingcai.com/wp-json/>; rel="https://api.w.org/"
  ```

  捕获了`HTTPError`异常，输出了reason、code和headers属性

- 因为`URLError`是`HTTPError`的父类，所以可以先选择捕获子类的错误，再去捕获父类的错误

  所以上述代码更好的写法如下

  ```python
  from urllib import request, error
  try:
  	response = request.urlopen('https://cuiqingcai.com/404')
  except error.HTTPError as e:
  	print(e.reason, e.code, e.headers, sep='\n')
  except error.URLError as e:
  	print(e.reason)
  else:
  	print('Request Successfully')
  ```

  这样就可以做到先捕获`HTTPError`，获取它的错误状态码、原因、headers等信息。如果不是`HTTPError`异常，就会捕获`URLError`异常，输出错误原因。最后，用`else`来处理正常的逻辑。这是一个较好的异常处理写法

- reason属性返回的不一定是字符串，也可能是一个对象

  ```python
  import socket
  import urllib.request
  import urllib.error
  try:
  	response = urllib.request.urlopen('https://www.baidu.com', timeout=0.01)
  except urllib.error.URLError as e:
  	print(type(e.reason))
  	if isinstance(e.reason, socket.timeout):
  		print('TIME OUT')
  ```

  这里我们直接设置超时时间来强制抛出timeout异常。

  输出

  ```markdown
   <class 'socket.timeout'>
   TIME OUT
  ```

可以发现，`reason`属性的结果是`socket.timeout`类。所以，这里我们可以用`isinstance()`方法来判断它的类型，作出更详细的异常判断

## 1.4 解析链接

urllib库里还提供了parse这个模块，它定义了处理URL的标准接口，例如实现URL各部分的抽取、合并以及链接转换

### 1.4.1 urlparse()

该方法可以实现URL的识别和分段

- 输出urlparse的类型与结果

  ```python
  from urllib.parse import urlparse
  result = urlparse('https://www.baidu.com/index.html;user?id=5#comment')
  print(type(result))
  print(result)
  ```

  ```
  <class 'urllib.parse.ParseResult'>
  ParseResult(scheme='http', netloc='www.baidu.com', path='/index.html', params='user', query='id=5', fragment='comment')
  ```

  返回结果是一个ParseResult类型的对象，它包含6部分，分别是scheme、netloc、path、params、query和fragment

- 标准链接格式

  `scheme://netloc/path;parameters?query#fragment`

- API

  `urllib.parse.urlparse(urlstring, scheme='', allow_fragments=True)`

  - 参数

    1. urlstring：这是必填项，即待解析的URL。

    2. scheme：它是默认的协议（比如http或https等）。假如这个链接没有带协议信息，会将这个作为默认的协议。

    3. allow_fragments：即是否忽略fragment。如果它被设置为False，fragment部分就会被忽略，它会被解析为path、parameters或者query的一部分，而fragment部分为空

  - 说明
    1. 可见，scheme参数只有在URL中不包含scheme信息时才生效。如果URL中有scheme信息，就会返回解析出的scheme
    2. 返回结果ParseResult实际上是一个元组，我们可以用索引顺序来获取，也可以用属性名获取。

### 1.4.2 urlunparse()

其接受的参数是一个可迭代对象，但是它的长度必须是6

```python
from urllib.parse import urlparse
result = urlparse('www.baidu.com/index.html;user?id=5#comment', scheme='https')
print(result)
```

这里参数data用了列表类型。当然，你也可以用其他类型，比如元组或者特定的数据结构

输出：`http://www.baidu.com/index.html;user?a=6#comment`

### 1.4.3 urlsplit()

这个方法和`urlparse()`方法非常相似，只不过它不再单独解析params这一部分，只返回5个结果。上面例子中的params会合并到path中

```python
from urllib.parse import urlsplit
result = urlsplit('http://www.baidu.com/index.html;user?id=5#comment')
print(result)
```

输出：`SplitResult(scheme='http', netloc='www.baidu.com', path='/index.html;user', query='id=5', fragment='comment')`

返回结果是`SplitResult`，它其实也是一个元组类型，既可以用属性获取值，也可以用索引来获取

### 1.4.4 urljoin()

提供一个base_url（基础链接）作为第一个参数，将新的链接作为第二个参数，该方法会分析base_url的scheme、netloc和path这3个内容并对新链接缺失的部分进行补充，最后返回结果

```python
from urllib.parse import urljoin
print(urljoin('http://www.baidu.com', 'FAQ.html'))
print(urljoin('http://www.baidu.com', 'https://cuiqingcai.com/FAQ.html'))
print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html'))
print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html question=2'))
print(urljoin('http://www.baidu.com?wd=abc', 'https://cuiqingcai.com/index.php'))
print(urljoin('http://www.baidu.com', '?category=2#comment'))
print(urljoin('www.baidu.com', '?category=2#comment'))
print(urljoin('www.baidu.com#comment', '?category=2'))
```

输出

```
http://www.baidu.com/FAQ.html
https://cuiqingcai.com/FAQ.html
https://cuiqingcai.com/FAQ.html
https://cuiqingcai.com/FAQ.html?question=2
https://cuiqingcai.com/index.php
http://www.baidu.com?category=2#comment
www.baidu.com?category=2#comment
www.baidu.com?category=2
```

base_url提供了三项内容scheme、netloc和path。如果这3项在新的链接里不存在，就予以补充；如果新的链接存在，就使用新的链接的部分。而base_url中的params、query和fragment是不起作用的

### 1.4.5 urlencode()

它在构造GET请求参数的时候非常有用

```python
from urllib.parse import urlencode
params = {
	'name': 'germey',
	'age': 22
}
base_url = 'http://www.baidu.com?'
url = base_url + urlencode(params)
print(url)
```

输出：`http://www.baidu.com?name=germey&age=22`

这里首先声明了一个字典来将参数表示出来，然后调用`urlencode()`方法将其序列化为GET请求参数。

参数就成功地由字典类型转化为GET请求参数了

有时为了更加方便地构造参数，我们会事先用字典来表示。要转化为URL的参数时，只需要调用该方法即可

### 1.4.6 parse_qs()

有一串GET请求参数，利用parse_qs()方法，就可以将它转回字典

```python
from urllib.parse import parse_qs
query = 'name=germey&age=22'
print(parse_qs(query))
```

输出：`{'name': ['germey'], 'age': ['22']}`

### 1.4.7 parse_qsl()

它用于将参数转化为元组组成的列表

```python
from urllib.parse import parse_qsl
query = 'name=germey&age=22'
print(parse_qsl(query))
```

输出：`[('name', 'germey'), ('age', '22')]`

### 1.4.8 quote()

该方法可以将内容转化为URL编码的格式。URL中带有中文参数时，有时可能会导致乱码的问题，此时用这个方法可以将中文字符转化为URL编码

```python
from urllib.parse import quote
keyword = '壁纸'
url = 'https://www.baidu.com/s?wd=' + quote(keyword)
print(url)
```

声明了一个中文的搜索文字，然后用`quote()`方法对其进行URL编码

输出：[`https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8`](https://www.baidu.com/s?wd=壁纸)

### 1.4.9 unquote()

它可以进行URL解码

```python
from urllib.parse import unquote
url = 'https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8'
print(unquote(url))
```

输出：`https://www.baidu.com/s?wd=壁纸`

## 1.5 使用robotparser分析Robot协议

### 1.5.1 说明

该模块提供了一个类RobotFileParser，它可以根据某网站的robots.txt文件来判断一个爬取爬虫是否有权限来爬取这个网页。

### 1.5.2 声明

```python urllib.robotparser.RobotFileParser(url='')```

可以不传入url，使用`set_url()`设置

### 1.5.3 方法

- **set_url()**

  用来设置robots.txt文件的链接。如果在创建RobotFileParser对象时传入了链接，那么就不需要再使用这个方法设置了。

- **read()**

  读取robots.txt文件并进行分析。注意，这个方法执行一个读取和分析操作，如果不调用这个方法，接下来的判断都会为False，所以一定记得调用这个方法。这个方法不会返回任何内容，但是执行了读取操作。

- **parse()**

  用来解析robots.txt文件，传入的参数是robots.txt某些行的内容，它会按照robots.txt的语法规则来分析这些内容。

- **can_fetch()**

  该方法传入两个参数，第一个是User-agent，第二个是要抓取的URL。返回的内容是该搜索引擎是否可以抓取这个URL，返回结果是True或False。

- **mtime()**

  返回的是上次抓取和分析robots.txt的时间，这对于长时间分析和抓取的搜索爬虫是很有必要的，你可能需要定期检查来抓取最新的robots.txt。

- **modified()**

  它同样对长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取和分析robots.txt的时间

### 1.5.4 例

```python
from urllib.robotparser import RobotFileParser
rp = RobotFileParser()
rp.set_url('https://www.baidu.com/robots.txt')
rp.read()
print(rp.can_fetch('Baiduspider', 'https://www.baidu.com'))
print(rp.can_fetch('Baiduspider', 'https://www.baidu.com/homepage/'))
print(rp.can_fetch('Googlebot', 'https://www.baidu.com/homepage/'))
```

步骤

1. 创建RobotFileParser对象

2. 通过set_url()方法设置了robots.txt的链接

3. 利用can_fetch()方法判断了网页是否可以被抓取

### 1.5.5 使用parser()方法执行读取和分析

```python
from urllib.request import urlopen
from urllib.robotparser import RobotFileParser
rp = RobotFileParser()
rp.parse(urlopen('https://www.baidu.com/robots.txt').read().decode('utf-8').split('\n'))
print(rp.can_fetch('Baiduspider', 'https://www.baidu.com'))
print(rp.can_fetch('Baiduspider', 'https://www.baidu.com/homepage/'))
print(rp.can_fetch('Googlebot', 'https://www.baidu.com/homepage/'))
```

# 二、Handler与Opener

## 2.1 说明

- urllib.request模块里的BaseHandler类，它是所有其他Handler的父类，它提供了最基本的方法

- 有各种Handler子类继承BaseHandler类
  1. HTTPDefaultErrorHandler：用于处理HTTP响应错误，错误都会抛出HTTPError类型的异常
  2. HTTPRedirectHandler：用于处理重定向
  3. HTTPCookieProcessor：用于处理Cookies
  4. ProxyHandler：用于设置代理，默认代理为空
  5. HTTPPasswordMgr：用于管理密码，它维护了用户名和密码的表
  6. HTTPBasicAuthHandler：用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题

## 2.2 验证

请求需要身份验证的界面，借助HTTPBasicAuthHandler就可以完成

- 实现

```python
from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener
from urllib.error import URLError
username = 'admin'
password = 'admin'
url = 'https://static3.scrape.center/'
p = HTTPPasswordMgrWithDefaultRealm()
p.add_password(None, url, username, password)
auth_handler = HTTPBasicAuthHandler(p)
opener = build_opener(auth_handler)
try:
	result = opener.open(url)
	html = result.read().decode('utf-8')
    print(html)
except URLError as e:
    print(e.reason)
```

- 步骤

  1. 首先实例化`HTTPBasicAuthHandler`对象，其参数是`HTTPPasswordMgrWithDefaultRealm`对象，它利用`add_password()`添加进去用户名和密码，这样就建立了一个处理验证的Handler。

  2. 利用这个Handler并使用`build_opener()`方法构建一个Opener，这个Opener在发送请求时就相当于已经验证成功了。

  3. 利用Opener的`open()`方法打开链接，就可以完成验证了。这里获取到的结果就是验证后的页面源码内容

## 2.3 代理

如果要添加代理

```python
from urllib.error import URLError
from urllib.request import ProxyHandler, build_opener
proxy_handler = ProxyHandler({
	'http': 'http://127.0.0.1:9743',
	'https': 'https://127.0.0.1:9743'
})
opener = build_opener(proxy_handler)
try:
	response = opener.open('https://www.baidu.com')
	print(response.read().decode('utf-8'))
except URLError as e:
	print(e.reason)
```

- 步骤
  1. 在本地搭建一个代理，它运行在9743端口上。
  2. 使用`ProxyHandler`，其参数是一个字典，键名是协议类型（比如HTTP或者HTTPS等），键值是代理链接，可以添加多个代理。
  3. 利用这个Handler及`build_opener()`方法构造一个Opener，之后发送请求即可。

## 2.4 Cookies

### 2.4.1 获取网站的Cookies

```python
import http.cookiejar, urllib.request
cookie = http.cookiejar.CookieJar()
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('https://www.baidu.com')
for item in cookie:
	print(item.name + "=" + item.value)
```

### 2.4.2 步骤

1. 声明一个CookieJar对象
2. 利用HTTPCookieProcessor来构建一个Handler
3. 利用build_opener()方法构建出Opener，执行open()函数即可

### 2.4.3 将Cookies以文件形式保存

```python
import urllib.request, http.cookiejar
filename = 'cookie.txt'
cookie = http.cookiejar.MozillaCookieJar(filename)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('https://www.baidu.com')
cookie.save(ignore_discard=True, ignore_expires=True)
```

CookieJar需要换成MozillaCookieJar，它在生成文件时会用到，是CookieJar的子类，可以用来处理Cookies和文件相关的事件，比如读取和保存Cookies，可以将Cookies保存成Mozilla型浏览器的Cookies格式

另外，LWPCookieJar同样可以读取和保存Cookies，但是保存的格式和MozillaCookieJar不一样，它会保存成libwww-perl(LWP)格式的Cookies文件，可以改写成`cookie = http.cookiejar.LWPCookieJar(filename)`

### 2.4.4 生成Cookies文件后，从文件中读取并利用的方法

以LWPCookieJar格式为例

```python
import urllib.request, http.cookiejar
cookie = http.cookiejar.LWPCookieJar()
cookie.load('cookie.txt', ignore_discard=True, ignore_expires=True)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('https://www.baidu.com')
print(response.read().decode('utf-8'))
```

这里调用`load()`方法来读取本地的Cookies文件，获取到了Cookies的内容。读取Cookies之后使用同样的方法构建Handler和Opener即可完成操作。运行结果正常的话，会输出百度网页的源代码

# 三、requests

## 3.1 GET请求

### 3.1.1 基本操作

以返回的是JSON形式的字符串的请求链接为例

```python
import requests
r = requests.get('http://httpbin.org/get')
print(r.text)
```

输出

```
{
  "args": {}, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.10.0"
  }, 
  "origin": "122.4.215.33", 
  "url": "http://httpbin.org/get"
  }
```

成功发起了GET请求，返回结果中包含请求头、URL、IP等信息

### 3.1.2 添加额外参数

若要添加params

```python
import requests
data = {
	'name': 'germey',
	'age': 22
}
r = requests.get("http://httpbin.org/get", params=data)
print(r.text)
```

输出

```
{
  "args": {
    "age": "22", 
    "name": "germey"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.10.0"
  }, 
  "origin": "122.4.215.33", 
  "url": "http://httpbin.org/get?age=22&name=germey"
}
```

### 3.1.3 将返回结果是JSON格式的字符串转化为字典

```python
print(r.json())
```

如果返回结果不是JSON格式，便会出现解析错误，抛出`json.decoder.JSONDecodeError`异常

## 3.2 抓取网页

```python
import requests
import re
headers = {
'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'
		}
r = requests.get("https://www.zhihu.com/explore", headers=headers)
pattern = re.compile('explore-feed.*?question_link.*?>(.*?)</a>', re.S)
titles = re.findall(pattern, r.text)
print(titles)
```

- 步骤

  1. 加入了headers信息，其中包含了User-Agent字段信息，也就是浏览器标识信息。如果不加这个，知乎会禁止抓取
  2. 利用正则表达式来匹配出所有的问题内容

- 输出

  ```
  ['\n为什么很多人喜欢提及「拉丁语系」这个词？\n', '\n在没有水的情况下水系宝可梦如何战斗？\n', '\n有哪些经验可以送给 Kindle 新人？\n', '\n谷歌的广告业务是如何赚钱的？\n', '\n程序员该学习什么，能在上学期间挣钱？\n', '\n有哪些原本只是一个小消息，但回看发现是个惊天大新闻的例子？\n', '\n如何评价今敏？\n', '\n源氏是怎么把那么长的刀从背后拔出来的？\n', '\n年轻时得了绝症或大病是怎样的感受？\n', '\n年轻时得了绝症或大病是怎样的感受？\n']
  ```

  ## 3.3 抓取二进制数据

  以GitHub的站点图标为例

```python
import requests
r = requests.get("https://github.com/favicon.ico")
print(r.content)
```

输出一段二进制数据

- 保存

```python
import requests
r = requests.get("https://github.com/favicon.ico")
with open('favicon.ico', 'wb') as f:
    f.write(r.content)
```

## 3.4 添加headers

对于一些网站，如果不传递headers，就不能正常请求

```python
import requests
headers = {
	'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'
		}
r = requests.get("https://www.zhihu.com/explore", headers=headers)
print(r.text)
```

可以在headers这个参数中任意添加其他的字段信息

## 3.5 POST请求

- 例：请求http://httpbin.org/post，该网站可以判断如果请求是POST方式，就把相关请求信息返回

```python
import requests
data = {'name': 'germey', 'age': '22'}
r = requests.post("http://httpbin.org/post", data=data)
print(r.text)
```

输出

```
{
	"args": {},
	"data": "",
	"files": {},
	"form": {
		"age": "22",
		"name": "germey"
	},
	"headers": {
		"Accept": "*/*",
		"Accept-Encoding": "gzip, deflate",
		"Content-Length": "18",
		"Content-Type": "application/x-www-form-urlencoded",
		"Host": "httpbin.org",
		"User-Agent": "python-requests/2.24.0",
		"X-Amzn-Trace-Id": "Root=1-5f9836ad-000ca9c96ac917d419407277"
	},
	"json": null,
	"origin": "59.79.2.148",
	"url": "http://httpbin.org/post"
}
```

其中form部分就是提交的数据，这就证明POST请求成功发送了

## 3.6 响应

### 3.6.1 例

```python
import requests
r = requests.get('https://static1.scrape.center/')
print(type(r.status_code), r.status_code)
print(type(r.headers), r.headers)
print(type(r.cookies), r.cookies)
print(type(r.url), r.url)
print(type(r.history), r.history)
```

### 2.6.2 输出

```
<class 'int'> 500
<class 'requests.structures.CaseInsensitiveDict'> {'Server': 'nginx/1.17.8', 'Date': 'Tue, 27 Oct 2020 15:10:32 GMT', 'Content-Type': 'text/html', 'Content-Length': '145', 'Connection': 'keep-alive', 'X-Frame-Options': 'DENY', 'Vary': 'Cookie', 'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains'}
<class 'requests.cookies.RequestsCookieJar'> <RequestsCookieJar[]>
<class 'str'> https://static1.scrape.center/
<class 'list'> []
```

输出`status_code`属性得到状态码

输出`headers`属性得到响应头

输出`cookies`属性得到Cookies

输出`url`属性得到URL

输出`history`属性得到请求历史

### 3.6.3 requests还提供了一个内置的状态码查询对象`requests.codes`

```python
import requests
r = requests.get('https://static1.scrape.center/')
exit() if not r.status_code == requests.codes.ok else print('Request Successfully')
```

通过比较返回码和内置的成功的返回码，来保证请求得到了正常响应，输出成功请求的消息，否则程序终止，这里我们用`requests.codes.ok`得到的是成功的状态码200

### 3.6.4 返回码和相应的查询条件

```python
# 信息性状态码
100: ('continue',),
101: ('switching_protocols',),
102: ('processing',),
103: ('checkpoint',),
122: ('uri_too_long', 'request_uri_too_long'),

# 成功状态码
200: ('ok', 'okay', 'all_ok', 'all_okay', 'all_good', '\\o/', '✓'),
201: ('created',),
202: ('accepted',),
203: ('non_authoritative_info', 'non_authoritative_information'),
204: ('no_content',),
205: ('reset_content', 'reset'),
206: ('partial_content', 'partial'),
207: ('multi_status', 'multiple_status', 'multi_stati', 'multiple_stati'),
208: ('already_reported',),
226: ('im_used',),

# 重定向状态码
300: ('multiple_choices',),
301: ('moved_permanently', 'moved', '\\o-'),
302: ('found',),
303: ('see_other', 'other'),
304: ('not_modified',),
305: ('use_proxy',),
306: ('switch_proxy',),
307: ('temporary_redirect', 'temporary_moved', 'temporary'),
308: ('permanent_redirect',
      'resume_incomplete', 'resume',), # These 2 to be removed in 3.0

# 客户端错误状态码
400: ('bad_request', 'bad'),
401: ('unauthorized',),
402: ('payment_required', 'payment'),
403: ('forbidden',),
404: ('not_found', '-o-'),
405: ('method_not_allowed', 'not_allowed'),
406: ('not_acceptable',),
407: ('proxy_authentication_required', 'proxy_auth', 'proxy_authentication'),
408: ('request_timeout', 'timeout'),
409: ('conflict',),
410: ('gone',),
411: ('length_required',),
412: ('precondition_failed', 'precondition'),
413: ('request_entity_too_large',),
414: ('request_uri_too_large',),
415: ('unsupported_media_type', 'unsupported_media', 'media_type'),
416: ('requested_range_not_satisfiable', 'requested_range', 'range_not_satisfiable'),
417: ('expectation_failed',),
418: ('im_a_teapot', 'teapot', 'i_am_a_teapot'),
421: ('misdirected_request',),
422: ('unprocessable_entity', 'unprocessable'),
423: ('locked',),
424: ('failed_dependency', 'dependency'),
425: ('unordered_collection', 'unordered'),
426: ('upgrade_required', 'upgrade'),
428: ('precondition_required', 'precondition'),
429: ('too_many_requests', 'too_many'),
431: ('header_fields_too_large', 'fields_too_large'),
444: ('no_response', 'none'),
449: ('retry_with', 'retry'),
450: ('blocked_by_windows_parental_controls', 'parental_controls'),
451: ('unavailable_for_legal_reasons', 'legal_reasons'),
499: ('client_closed_request',),

# 服务端错误状态码
500: ('internal_server_error', 'server_error', '/o\\', '✗'),
501: ('not_implemented',),
502: ('bad_gateway',),
503: ('service_unavailable', 'unavailable'),
504: ('gateway_timeout',),
505: ('http_version_not_supported', 'http_version'),
506: ('variant_also_negotiates',),
507: ('insufficient_storage',),
509: ('bandwidth_limit_exceeded', 'bandwidth'),
510: ('not_extended',),
511: ('network_authentication_required', 'network_auth', 'network_authentication')
```

## 3.7 文件上传

用favicon.ico来模拟文件上传的过程

```python
import requests
files = {'file': open('favicon.ico', 'rb')}
r = requests.post("http://httpbin.org/post", files=files)
print(r.text)
```

- 输出

```
{
  "args": {}, 
  "data": "", 
  "files": {
    "file": "data:application/octet-stream;base64,AAAAAA...="
  }, 
  "form": {}, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "6665", 
    "Content-Type": "multipart/form-data; boundary=809f80b1a2974132b133ade1a8e8e058", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.10.0"
  }, 
  "json": null, 
  "origin": "60.207.237.16", 
  "url": "http://httpbin.org/post"
}
```

网站会返回响应，里面包含files这个字段，而form字段是空的，这证明文件上传部分会单独有一个files字段来标识

## 3.8 获取 Cookies

### 3.8.1 例

```python
import requests
r = requests.get("https://www.baidu.com")
print(r.cookies)
for key, value in r.cookies.items():
	 print(key + '=' + value)
```

- 输出

```
<RequestsCookieJar[<Cookie BDORZ=27315 for .baidu.com/>]>
BDORZ=27315
```

- 步骤
  1. 调用cookies属性，得到Cookies，可以发现它是RequestCookieJar类型
  2. 用items()方法将其转化为元组组成的列表，遍历输出每一个Cookie的名称和值，实现Cookie的遍历解析

### 3.8.2 用Cookie来维持登录状态

- Headers中的Cookie内容复制下来
- 将Cookie设置到headers中

```python
import requests
headers = {
		'Cookie': '_zap=35740c……76',
		'Host': 'www.zhihu.com',
		'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36',
}
r = requests.get('https://www.zhihu.com', headers=headers)
print(r.text)
```

结果中包含了登录后的结果

- 通过cookies参数来设置（较繁琐）

```python
import requests
cookies = 'q_e=......fdb5763b0'
jar = requests.cookies.RequestsCookieJar()
headers = {
	'Host': 'www.zhihu.com',
	'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'
}
for cookie in cookies.split(';'):
    key, value = cookie.split('=', 1)
	jar.set(key, value)
r = requests.get("http://www.zhihu.com", cookies=jar, headers=headers)
print(r.text)
```

首先新建了一个`RequestCookieJar`对象，然后将复制下来的cookies利用`split()`方法分割，接着利用`set()`方法设置好每个Cookie的key和value，然后通过调用requests的`get()`方法并传递给cookies参数即可

## 3.9 会话维持

如果直接利用`get()`或`post()`等方法的确可以做到模拟网页的请求，但是这实际上是相当于不同的会话，也就是说相当于用了两个浏览器打开了不同的页面

利用Session对象，可以方便地维护一个会话，而且不用担心cookies的问题，它会帮我们自动处理好

```python
import requests
s = requests.Session()
s.get('http://httpbin.org/cookies/set/number/123456789')
r = s.get('http://httpbin.org/cookies')
print(r.text)
```

输出

```
{
  "cookies": {
    "number": "123456789"
  }
}
```

用Session，可以做到模拟同一个会话而不用担心Cookies的问题。它通常用于模拟登录成功之后再进行下一步的操作

## 3.10 SSL证书验证

requests还提供了证书验证的功能。当发送HTTP请求的时候，它会检查SSL证书，我们可以使用verify参数控制是否检查此证书。其实如果不加verify参数的话，默认是True，会自动验证

请求一个HTTPS站点，但是证书验证错误的页面时，把verify参数设置为False即可

```python
import requests
response = requests.get('https://www.12306.cn', verify=False)
print(response.status_code)
```

打印出请求成功的状态码

```
D:\Application\Anaconda\lib\site-packages\urllib3\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.12306.cn'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
InsecureRequestWarning,
200
```

报了一个警告，它建议我们给它指定证书

### 3.10.1 可以通过设置忽略警告的方式来屏蔽这个警告

```python
import requests
from requests.packages import urllib3
urllib3.disable_warnings()
response = requests.get('https://www.12306.cn', verify=False)
print(response.status_code)
```

### 3.10.2 或者通过捕获警告到日志的方式忽略警告

```python
import logging
import requests
logging.captureWarnings(True)
response = requests.get('https://www.12306.cn', verify=False)
print(response.status_code)
```

### 3.10.3 可以指定一个本地证书用作客户端证书

这可以是单个文件（包含密钥和证书）或一个包含两个文件路径的元组

```python
import requests
response = requests.get('https://www.12306.cn',
                        cert=('/path/server.crt', '/path/key'))
print(response.status_code)
```

我们需要有crt和key文件，并且指定它们的路径。注意，本地私有证书的key必须是解密状态，加密状态的key是不支持的

## 3.11 代理设置

**使用proxies参数**

```python
import requests
proxies = {
	"http": "http://10.10.1.10:3128",
	"https": "http://10.10.1.10:1080",
}
requests.get("https://www.taobao.com", proxies=proxies)
```

若代理需要使用HTTP Basic Auth，可以使用类似[http://user](http://user/):password@host:port这样的语法来设置代理

```python
import requests
proxies = {
    "http": "http://user:password@10.10.1.10:3128/",
}
requests.get("https://www.taobao.com", proxies=proxies)
```

使用SOCKS协议代理

```python
import requests
proxies = {
	'http': 'socks5://user:password@host:port',
	'https': 'socks5://user:password@host:port'
}
requests.get("https://www.taobao.com", proxies=proxies)
```

## 3.12 超时设置

使用timeout参数。这个时间的计算是发出请求到服务器返回响应的时间

```python
import requests
r = requests.get("https://www.taobao.com", timeout=1)
print(r.status_code)
```

- 请求分为两个阶段，即连接（connect）和读取（read）。上面设置的timeout将用作连接和读取这二者的timeout总和

- 如果要分别指定，就可以传入一个元组

```python
r = requests.get('https://www.taobao.com', timeout=(5, 11, 30))
```

- 如果想永久等待，可以直接将timeout设置为None，或者不设置直接留空，因为默认是None

## 3.13 身份认证

```python
import requests
from requests.auth import HTTPBasicAuth
r = requests.get('http://localhost:5000',
                 auth=HTTPBasicAuth('username', 'password'))
print(r.status_code)
```

如果用户名和密码正确的话，请求时就会自动认证成功，会返回200状态码，如果认证失败，则返回401状态码