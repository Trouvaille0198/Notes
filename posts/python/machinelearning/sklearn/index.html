<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><meta http-equiv=x-ua-compatible content="IE=edge, chrome=1"><title>sklearn - 伤心肠粉的酱油碟子</title><meta name=Description content><meta property="og:title" content="sklearn"><meta property="og:description" content="sklearn 特征工程 (Feature Engineering) 特征工程是使用专业背景知识和技巧处理数据**，**使得特征能在机器"><meta property="og:type" content="article"><meta property="og:url" content="https://trouvaille0198.github.io/Notes/posts/python/machinelearning/sklearn/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-10-23T00:00:00+00:00"><meta property="article:modified_time" content="2022-03-14T14:33:49+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="sklearn"><meta name=twitter:description content="sklearn 特征工程 (Feature Engineering) 特征工程是使用专业背景知识和技巧处理数据**，**使得特征能在机器"><meta name=application-name content="伤心肠粉的酱油碟子"><meta name=apple-mobile-web-app-title content="伤心肠粉的酱油碟子"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://trouvaille0198.github.io/Notes/posts/python/machinelearning/sklearn/><link rel=prev href=https://trouvaille0198.github.io/Notes/posts/%E5%88%B7%E9%A2%98/array/209.-%E9%95%BF%E5%BA%A6%E6%9C%80%E5%B0%8F%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84/><link rel=next href=https://trouvaille0198.github.io/Notes/posts/python/dataanalysis/pandas/><link rel=stylesheet href=/Notes/lib/normalize/normalize.min.css><link rel=stylesheet href=/Notes/css/style.min.css><link rel=stylesheet href=/Notes/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/Notes/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"sklearn","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/trouvaille0198.github.io\/Notes\/posts\/python\/machinelearning\/sklearn\/"},"genre":"posts","keywords":"Python, ml","wordcount":19186,"url":"https:\/\/trouvaille0198.github.io\/Notes\/posts\/python\/machinelearning\/sklearn\/","datePublished":"2021-10-23T00:00:00+00:00","dateModified":"2022-03-14T14:33:49+00:00","publisher":{"@type":"Organization","name":"MelonCholi"},"author":{"@type":"Person","name":"MelonCholi"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":''==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:''==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/Notes/ title=伤心肠粉的酱油碟子>伤心肠粉</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/Notes/categories/>分类 </a><a class=menu-item href=/Notes/tags/>标签 </a><a class=menu-item href=/Notes/posts/>文章 </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=搜索文章标题或内容... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fas fa-search fa-fw"></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fas fa-times-circle fa-fw"></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin"></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/Notes/ title=伤心肠粉的酱油碟子>伤心肠粉</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fas fa-search fa-fw"></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fas fa-times-circle fa-fw"></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin"></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></div><a class=menu-item href=/Notes/categories/ title>分类</a><a class=menu-item href=/Notes/tags/ title>标签</a><a class=menu-item href=/Notes/posts/ title>文章</a><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw"></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>目录</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animated flipInX">sklearn</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/Notes/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw"></i>MelonCholi</a></span>&nbsp;<span class=post-category>收录于 <a href=/Notes/categories/python/><i class="far fa-folder fa-fw"></i>Python</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2021-10-23>2021-10-23</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 19186 字&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 39 分钟&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#特征工程>特征工程</a><ul><li><a href=#数据集>数据集</a><ul><li><a href=#小规模-load_>小规模 load_*()</a></li><li><a href=#大规模-fetch_>大规模 fetch_*()</a></li><li><a href=#返回值>返回值</a></li></ul></li><li><a href=#数据集划分>数据集划分</a><ul><li><a href=#简单划分-train_test_split>简单划分 train_test_split</a></li><li><a href=#k-折交叉验证>K 折交叉验证</a></li></ul></li><li><a href=#特征抽取>特征抽取</a><ul><li><a href=#字典特征提取-dictvectorizer>字典特征提取 DictVectorizer</a><ul><li><a href=#api>API</a></li><li><a href=#例>例</a></li></ul></li><li><a href=#文本词频特征提取-textcountvectorizer>文本词频特征提取 text.CountVectorizer</a><ul><li><a href=#api-1>API</a></li><li><a href=#例-1>例</a></li><li><a href=#中文处理>中文处理</a></li></ul></li><li><a href=#tf-idf-文本特征提取-texttfidfvectorizer>Tf-idf 文本特征提取 text.TfidfVectorizer</a><ul><li><a href=#公式>公式</a></li><li><a href=#api-2>API</a></li><li><a href=#例-2>例</a></li></ul></li></ul></li><li><a href=#特征预处理>特征预处理</a><ul><li><a href=#归一化>归一化</a><ul><li><a href=#公式-1>公式</a></li><li><a href=#api-3>API</a></li><li><a href=#例-3>例</a></li></ul></li><li><a href=#标准化>标准化</a><ul><li><a href=#公式-2>公式</a></li><li><a href=#api-4>API</a></li><li><a href=#例-4>例</a></li></ul></li></ul></li><li><a href=#特征降维>特征降维</a><ul><li><a href=#特征选择>特征选择</a><ul><li><a href=#低方差特征过滤>低方差特征过滤</a><ul><li><a href=#api-5>API</a></li><li><a href=#例-5>例</a></li></ul></li><li><a href=#相关系数>相关系数</a><ul><li><a href=#原理>原理</a></li><li><a href=#api-6>API</a></li><li><a href=#例-6>例</a></li></ul></li></ul></li><li><a href=#主成分分析>主成分分析</a><ul><li><a href=#概念>概念</a></li><li><a href=#api-7>API</a></li><li><a href=#例-7>例</a></li></ul></li></ul></li></ul></li><li><a href=#转换器和估计器>转换器和估计器</a><ul><li><a href=#转换器>转换器</a></li><li><a href=#估计器>估计器</a></li><li><a href=#模型选择与调优>模型选择与调优</a><ul><li><a href=#概念-1>概念</a><ul><li><a href=#交叉验证>交叉验证</a></li><li><a href=#超参数搜索-网格搜索>超参数搜索-网格搜索</a></li></ul></li><li><a href=#api-8>API</a></li></ul></li><li><a href=#分类的评估方法>分类的评估方法</a><ul><li><a href=#分类评估报告-api>分类评估报告 API</a></li><li><a href=#roc-曲线与-auc-指标>ROC 曲线与 AUC 指标</a><ul><li><a href=#tpr-与-fpr>TPR 与 FPR</a></li><li><a href=#roc-曲线>ROC 曲线</a></li><li><a href=#auc-指标>AUC 指标</a></li><li><a href=#api-9>API</a></li></ul></li></ul></li><li><a href=#模型保存和加载>模型保存和加载</a></li></ul></li><li><a href=#分类>分类</a><ul><li><a href=#knn-算法>KNN 算法</a><ul><li><a href=#api-10>API</a></li><li><a href=#例-8>例</a></li></ul></li><li><a href=#朴素贝叶斯算法>朴素贝叶斯算法</a><ul><li><a href=#原理-1>原理</a><ul><li><a href=#贝叶斯公式>贝叶斯公式</a></li><li><a href=#拉普拉斯平滑系数>拉普拉斯平滑系数</a></li></ul></li><li><a href=#api-11>API</a></li><li><a href=#例20类新闻分类>例：20类新闻分类</a></li></ul></li><li><a href=#决策树>决策树</a><ul><li><a href=#原理-2>原理</a><ul><li><a href=#信息熵>信息熵</a></li><li><a href=#条件信息熵>条件信息熵</a></li><li><a href=#信息增益>信息增益</a></li><li><a href=#三种算法实现>三种算法实现</a></li></ul></li><li><a href=#api-12>API</a></li><li><a href=#保存树的结构>保存树的结构</a></li><li><a href=#例-9>例</a></li></ul></li><li><a href=#随机森林>随机森林</a><ul><li><a href=#原理-3>原理</a></li><li><a href=#api-13>API</a><ul><li><a href=#例-10>例</a></li></ul></li></ul></li><li><a href=#逻辑回归>逻辑回归</a><ul><li><a href=#原理-4>原理</a><ul><li><a href=#输入>输入</a></li><li><a href=#激活函数>激活函数</a></li><li><a href=#损失函数>损失函数</a></li><li><a href=#优化>优化</a></li></ul></li><li><a href=#api-14>API</a></li><li><a href=#例-11>例</a><ul><li><a href=#初始化>初始化</a></li><li><a href=#训练>训练</a></li><li><a href=#查看精确率召回率f1-score>查看精确率，召回率，F1-score</a></li><li><a href=#查看-roc-曲线和-auc-指标>查看 ROC 曲线和 AUC 指标</a></li></ul></li></ul></li></ul></li><li><a href=#回归>回归</a><ul><li><a href=#线性回归>线性回归</a><ul><li><a href=#api-15>API</a><ul><li><a href=#正规方程>正规方程</a></li><li><a href=#梯度下降>梯度下降</a></li><li><a href=#对比>对比</a></li></ul></li><li><a href=#回归性能评估>回归性能评估</a><ul><li><a href=#均方误差-mse>均方误差 MSE</a></li><li><a href=#平均绝对误差-mae>平均绝对误差 MAE</a></li></ul></li><li><a href=#例-12>例</a><ul><li><a href=#初始化-1>初始化</a></li><li><a href=#正规方程-1>正规方程</a></li><li><a href=#梯度下降-1>梯度下降</a></li></ul></li></ul></li><li><a href=#岭回归>岭回归</a><ul><li><a href=#原理-5>原理</a></li><li><a href=#api-16>API</a><ul><li><a href=#常规岭回归>常规岭回归</a></li><li><a href=#交叉验证岭回归>交叉验证岭回归</a></li></ul></li><li><a href=#例-13>例</a></li></ul></li></ul></li><li><a href=#聚类>聚类</a><ul><li><a href=#聚类步骤>聚类步骤</a></li><li><a href=#api-17>API</a></li><li><a href=#轮廓系数>轮廓系数</a><ul><li><a href=#公式-3>公式</a></li><li><a href=#轮廓系数值分析>轮廓系数值分析</a></li><li><a href=#api-18>API</a></li></ul></li><li><a href=#例-14>例</a></li></ul></li></ul></nav></div></div><div class=content id=content><h1 id=sklearn>sklearn</h1><h2 id=特征工程>特征工程</h2><p>(Feature Engineering)</p><p>特征工程是使用专业背景知识和技巧处理数据**，**使得特征能在机器学习算法上发挥更好的作用的过程</p><h3 id=数据集>数据集</h3><p><em>scikit-learn</em> 提供了一些标准数据集</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>datasets</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=小规模-load_>小规模 load_*()</h4><p><em><strong>datasets.load_*()</strong></em></p><p>获取小规模数据集，数据包含在datasets里</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>data1</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>load_iris</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>data2</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>load_boston</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=大规模-fetch_>大规模 fetch_*()</h4><p><em><strong>datasets.fetch_*(data_home, subset)</strong></em></p><p>获取大规模数据集，需要从网络上下载</p><ul><li><em>data_home</em>：数据集下载的目录，默认是 ~/scikit_learn_data/</li><li><em>subset</em>：选择要加载的数据集。&rsquo;train&rsquo;或者&rsquo;test&rsquo;，&lsquo;all&rsquo;，可选</li></ul><h4 id=返回值>返回值</h4><p>load 和 fetch 返回的数据类型为 <em>datasets.base.Bunch</em> (字典继承)</p><ul><li><em>data</em>：特征数据数组，是二维 numpy.ndarray 数组</li><li>*target：*标签数组，是 n_samples 的一维 numpy.ndarray 数组</li><li><em>DESCR</em>：数据描述</li><li><em>feature_names</em>：特征名,新闻数据，手写数字、回归数据集没有</li><li><em>target_names</em>：标签名</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>load_iris</span>
</span></span><span class=line><span class=cl><span class=c1># 获取鸢尾花数据集</span>
</span></span><span class=line><span class=cl><span class=n>iris</span> <span class=o>=</span> <span class=n>load_iris</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;鸢尾花数据集的返回值：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>iris</span><span class=p>)</span> <span class=c1># 将所有参数全部返回，返回值是一个继承自字典的Bench</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;鸢尾花的特征值:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>iris</span><span class=o>.</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;鸢尾花的目标值：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>iris</span><span class=o>.</span><span class=n>target</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;鸢尾花特征的名字：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>iris</span><span class=o>.</span><span class=n>feature_names</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;鸢尾花目标值的名字：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>iris</span><span class=o>.</span><span class=n>target_names</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;鸢尾花的描述：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>iris</span><span class=o>.</span><span class=n>DESCR</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 同样可以写作诸如iris[&#39;data&#39;]的格式</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl> 鸢尾花的特征值:
</span></span><span class=line><span class=cl> [[5.1 3.5 1.4 0.2]
</span></span><span class=line><span class=cl> [4.9 3.  1.4 0.2]
</span></span><span class=line><span class=cl> [4.7 3.2 1.3 0.2]
</span></span><span class=line><span class=cl> [4.6 3.1 1.5 0.2]
</span></span><span class=line><span class=cl> ...
</span></span><span class=line><span class=cl> [6.3 2.5 5.  1.9]
</span></span><span class=line><span class=cl> [6.5 3.  5.2 2. ]
</span></span><span class=line><span class=cl> [6.2 3.4 5.4 2.3]
</span></span><span class=line><span class=cl> [5.9 3.  5.1 1.8]]
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl> 鸢尾花的目标值：
</span></span><span class=line><span class=cl> [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
</span></span><span class=line><span class=cl> 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
</span></span><span class=line><span class=cl> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
</span></span><span class=line><span class=cl> 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
</span></span><span class=line><span class=cl> 2 2]
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl> 鸢尾花特征的名字：
</span></span><span class=line><span class=cl> [&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;]
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl> 鸢尾花目标值的名字：
</span></span><span class=line><span class=cl> [&#39;setosa&#39; &#39;versicolor&#39; &#39;virginica&#39;]
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl> 鸢尾花的描述：
</span></span><span class=line><span class=cl> .. _iris_dataset:
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Iris plants dataset
</span></span><span class=line><span class=cl>--------------------
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>**Data Set Characteristics:**
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    :Number of Instances: 150 (50 in each of three classes)
</span></span><span class=line><span class=cl> ...
</span></span><span class=line><span class=cl>   - Many, many more ...
</span></span></code></pre></td></tr></table></div></div><h3 id=数据集划分>数据集划分</h3><h4 id=简单划分-train_test_split>简单划分 train_test_split</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>
</span></span></code></pre></td></tr></table></div></div><p><em><strong>train_test_split ( arrays, *options )</strong></em></p><ul><li>x：数据集的特征值</li><li>y：数据集的标签值</li><li>test_size：测试集的大小，一般为 float</li><li>random_state：随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。</li></ul><p>返回：测试集特征值，测试集标签，训练集特征值，训练集标签（默认随机取）</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>x_train</span><span class=p>,</span> <span class=n>x_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>iris</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>iris</span><span class=o>.</span><span class=n>target</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>22</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=k-折交叉验证>K 折交叉验证</h4><p>（KFold Cross Validation）</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>KFold</span>
</span></span></code></pre></td></tr></table></div></div><p><em><strong>KFold ( n_splits=5, *, shuffle=False, random_state=None )</strong></em></p><p>返回 k 折对象</p><p>参数</p><ul><li><em>n_splits</em>：K 子集个数，int, default=5</li><li><em>shuffle</em>：是否要洗牌（打乱数据），bool, default=False</li><li><em>random_state</em>：int or RandomState instance, default=None</li></ul><p>方法</p><ul><li><em>get_n_splits([X, y, groups])</em>：返回迭代次数，Returns the number of splitting iterations in the cross-validator</li><li><em>split(X)</em>：生成器，返回训练和测试集的索引值，Generate indices to split data into training and test set.</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>KFold</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>datasets</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 数据集导入</span>
</span></span><span class=line><span class=cl><span class=n>iris</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>load_iris</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>iris</span><span class=o>.</span><span class=n>data</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>iris</span><span class=o>.</span><span class=n>target</span>
</span></span><span class=line><span class=cl><span class=c1># KFold</span>
</span></span><span class=line><span class=cl><span class=n>kf</span> <span class=o>=</span> <span class=n>KFold</span><span class=p>(</span><span class=n>n_splits</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 输出划分数</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>kf</span><span class=o>.</span><span class=n>get_n_splits</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># 划分数据集</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>train_index</span><span class=p>,</span> <span class=n>test_index</span> <span class=ow>in</span> <span class=n>kf</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;TRAIN:&#34;</span><span class=p>,</span> <span class=n>train_index</span><span class=p>,</span> <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>TEST:&#34;</span><span class=p>,</span> <span class=n>test_index</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x_train</span><span class=p>,</span> <span class=n>x_test</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=n>train_index</span><span class=p>],</span> <span class=n>x</span><span class=p>[</span><span class=n>test_index</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>y</span><span class=p>[</span><span class=n>train_index</span><span class=p>],</span> <span class=n>y</span><span class=p>[</span><span class=n>test_index</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=mi>5</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>TRAIN</span><span class=p>:</span> <span class=p>[</span> <span class=mi>30</span>  <span class=mi>31</span>  <span class=mi>32</span>  <span class=mi>33</span>  <span class=mi>34</span>  <span class=mi>35</span>  <span class=mi>36</span>  <span class=mi>37</span>  <span class=mi>38</span>  <span class=mi>39</span>  <span class=mi>40</span>  <span class=mi>41</span>  <span class=mi>42</span>  <span class=mi>43</span>  <span class=mi>44</span>  <span class=mi>45</span>  <span class=mi>46</span>  <span class=mi>47</span>
</span></span><span class=line><span class=cl>  <span class=mi>48</span>  <span class=mi>49</span>  <span class=mi>50</span>  <span class=mi>51</span>  <span class=mi>52</span>  <span class=mi>53</span>  <span class=mi>54</span>  <span class=mi>55</span>  <span class=mi>56</span>  <span class=mi>57</span>  <span class=mi>58</span>  <span class=mi>59</span>  <span class=mi>60</span>  <span class=mi>61</span>  <span class=mi>62</span>  <span class=mi>63</span>  <span class=mi>64</span>  <span class=mi>65</span>
</span></span><span class=line><span class=cl>  <span class=mi>66</span>  <span class=mi>67</span>  <span class=mi>68</span>  <span class=mi>69</span>  <span class=mi>70</span>  <span class=mi>71</span>  <span class=mi>72</span>  <span class=mi>73</span>  <span class=mi>74</span>  <span class=mi>75</span>  <span class=mi>76</span>  <span class=mi>77</span>  <span class=mi>78</span>  <span class=mi>79</span>  <span class=mi>80</span>  <span class=mi>81</span>  <span class=mi>82</span>  <span class=mi>83</span>
</span></span><span class=line><span class=cl>  <span class=mi>84</span>  <span class=mi>85</span>  <span class=mi>86</span>  <span class=mi>87</span>  <span class=mi>88</span>  <span class=mi>89</span>  <span class=mi>90</span>  <span class=mi>91</span>  <span class=mi>92</span>  <span class=mi>93</span>  <span class=mi>94</span>  <span class=mi>95</span>  <span class=mi>96</span>  <span class=mi>97</span>  <span class=mi>98</span>  <span class=mi>99</span> <span class=mi>100</span> <span class=mi>101</span>
</span></span><span class=line><span class=cl> <span class=mi>102</span> <span class=mi>103</span> <span class=mi>104</span> <span class=mi>105</span> <span class=mi>106</span> <span class=mi>107</span> <span class=mi>108</span> <span class=mi>109</span> <span class=mi>110</span> <span class=mi>111</span> <span class=mi>112</span> <span class=mi>113</span> <span class=mi>114</span> <span class=mi>115</span> <span class=mi>116</span> <span class=mi>117</span> <span class=mi>118</span> <span class=mi>119</span>
</span></span><span class=line><span class=cl> <span class=mi>120</span> <span class=mi>121</span> <span class=mi>122</span> <span class=mi>123</span> <span class=mi>124</span> <span class=mi>125</span> <span class=mi>126</span> <span class=mi>127</span> <span class=mi>128</span> <span class=mi>129</span> <span class=mi>130</span> <span class=mi>131</span> <span class=mi>132</span> <span class=mi>133</span> <span class=mi>134</span> <span class=mi>135</span> <span class=mi>136</span> <span class=mi>137</span>
</span></span><span class=line><span class=cl> <span class=mi>138</span> <span class=mi>139</span> <span class=mi>140</span> <span class=mi>141</span> <span class=mi>142</span> <span class=mi>143</span> <span class=mi>144</span> <span class=mi>145</span> <span class=mi>146</span> <span class=mi>147</span> <span class=mi>148</span> <span class=mi>149</span><span class=p>]</span> 
</span></span><span class=line><span class=cl><span class=n>TEST</span><span class=p>:</span> <span class=p>[</span> <span class=mi>0</span>  <span class=mi>1</span>  <span class=mi>2</span>  <span class=mi>3</span>  <span class=mi>4</span>  <span class=mi>5</span>  <span class=mi>6</span>  <span class=mi>7</span>  <span class=mi>8</span>  <span class=mi>9</span> <span class=mi>10</span> <span class=mi>11</span> <span class=mi>12</span> <span class=mi>13</span> <span class=mi>14</span> <span class=mi>15</span> <span class=mi>16</span> <span class=mi>17</span> <span class=mi>18</span> <span class=mi>19</span> <span class=mi>20</span> <span class=mi>21</span> <span class=mi>22</span> <span class=mi>23</span>
</span></span><span class=line><span class=cl> <span class=mi>24</span> <span class=mi>25</span> <span class=mi>26</span> <span class=mi>27</span> <span class=mi>28</span> <span class=mi>29</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>TRAIN</span><span class=p>:</span> <span class=p>[</span>  <span class=mi>0</span>   <span class=mi>1</span>   <span class=mi>2</span>   <span class=mi>3</span>   <span class=mi>4</span>   <span class=mi>5</span>   <span class=mi>6</span>   <span class=mi>7</span>   <span class=mi>8</span>   <span class=mi>9</span>  <span class=mi>10</span>  <span class=mi>11</span>  <span class=mi>12</span>  <span class=mi>13</span>  <span class=mi>14</span>  <span class=mi>15</span>  <span class=mi>16</span>  <span class=mi>17</span>
</span></span><span class=line><span class=cl>  <span class=mi>18</span>  <span class=mi>19</span>  <span class=mi>20</span>  <span class=mi>21</span>  <span class=mi>22</span>  <span class=mi>23</span>  <span class=mi>24</span>  <span class=mi>25</span>  <span class=mi>26</span>  <span class=mi>27</span>  <span class=mi>28</span>  <span class=mi>29</span>  <span class=mi>60</span>  <span class=mi>61</span>  <span class=mi>62</span>  <span class=mi>63</span>  <span class=mi>64</span>  <span class=mi>65</span>
</span></span><span class=line><span class=cl>  <span class=mi>66</span>  <span class=mi>67</span>  <span class=mi>68</span>  <span class=mi>69</span>  <span class=mi>70</span>  <span class=mi>71</span>  <span class=mi>72</span>  <span class=mi>73</span>  <span class=mi>74</span>  <span class=mi>75</span>  <span class=mi>76</span>  <span class=mi>77</span>  <span class=mi>78</span>  <span class=mi>79</span>  <span class=mi>80</span>  <span class=mi>81</span>  <span class=mi>82</span>  <span class=mi>83</span>
</span></span><span class=line><span class=cl>  <span class=mi>84</span>  <span class=mi>85</span>  <span class=mi>86</span>  <span class=mi>87</span>  <span class=mi>88</span>  <span class=mi>89</span>  <span class=mi>90</span>  <span class=mi>91</span>  <span class=mi>92</span>  <span class=mi>93</span>  <span class=mi>94</span>  <span class=mi>95</span>  <span class=mi>96</span>  <span class=mi>97</span>  <span class=mi>98</span>  <span class=mi>99</span> <span class=mi>100</span> <span class=mi>101</span>
</span></span><span class=line><span class=cl> <span class=mi>102</span> <span class=mi>103</span> <span class=mi>104</span> <span class=mi>105</span> <span class=mi>106</span> <span class=mi>107</span> <span class=mi>108</span> <span class=mi>109</span> <span class=mi>110</span> <span class=mi>111</span> <span class=mi>112</span> <span class=mi>113</span> <span class=mi>114</span> <span class=mi>115</span> <span class=mi>116</span> <span class=mi>117</span> <span class=mi>118</span> <span class=mi>119</span>
</span></span><span class=line><span class=cl> <span class=mi>120</span> <span class=mi>121</span> <span class=mi>122</span> <span class=mi>123</span> <span class=mi>124</span> <span class=mi>125</span> <span class=mi>126</span> <span class=mi>127</span> <span class=mi>128</span> <span class=mi>129</span> <span class=mi>130</span> <span class=mi>131</span> <span class=mi>132</span> <span class=mi>133</span> <span class=mi>134</span> <span class=mi>135</span> <span class=mi>136</span> <span class=mi>137</span>
</span></span><span class=line><span class=cl> <span class=mi>138</span> <span class=mi>139</span> <span class=mi>140</span> <span class=mi>141</span> <span class=mi>142</span> <span class=mi>143</span> <span class=mi>144</span> <span class=mi>145</span> <span class=mi>146</span> <span class=mi>147</span> <span class=mi>148</span> <span class=mi>149</span><span class=p>]</span> 
</span></span><span class=line><span class=cl><span class=n>TEST</span><span class=p>:</span> <span class=p>[</span><span class=mi>30</span> <span class=mi>31</span> <span class=mi>32</span> <span class=mi>33</span> <span class=mi>34</span> <span class=mi>35</span> <span class=mi>36</span> <span class=mi>37</span> <span class=mi>38</span> <span class=mi>39</span> <span class=mi>40</span> <span class=mi>41</span> <span class=mi>42</span> <span class=mi>43</span> <span class=mi>44</span> <span class=mi>45</span> <span class=mi>46</span> <span class=mi>47</span> <span class=mi>48</span> <span class=mi>49</span> <span class=mi>50</span> <span class=mi>51</span> <span class=mi>52</span> <span class=mi>53</span>
</span></span><span class=line><span class=cl> <span class=mi>54</span> <span class=mi>55</span> <span class=mi>56</span> <span class=mi>57</span> <span class=mi>58</span> <span class=mi>59</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>TRAIN</span><span class=p>:</span> <span class=p>[</span>  <span class=mi>0</span>   <span class=mi>1</span>   <span class=mi>2</span>   <span class=mi>3</span>   <span class=mi>4</span>   <span class=mi>5</span>   <span class=mi>6</span>   <span class=mi>7</span>   <span class=mi>8</span>   <span class=mi>9</span>  <span class=mi>10</span>  <span class=mi>11</span>  <span class=mi>12</span>  <span class=mi>13</span>  <span class=mi>14</span>  <span class=mi>15</span>  <span class=mi>16</span>  <span class=mi>17</span>
</span></span><span class=line><span class=cl>  <span class=mi>18</span>  <span class=mi>19</span>  <span class=mi>20</span>  <span class=mi>21</span>  <span class=mi>22</span>  <span class=mi>23</span>  <span class=mi>24</span>  <span class=mi>25</span>  <span class=mi>26</span>  <span class=mi>27</span>  <span class=mi>28</span>  <span class=mi>29</span>  <span class=mi>30</span>  <span class=mi>31</span>  <span class=mi>32</span>  <span class=mi>33</span>  <span class=mi>34</span>  <span class=mi>35</span>
</span></span><span class=line><span class=cl>  <span class=mi>36</span>  <span class=mi>37</span>  <span class=mi>38</span>  <span class=mi>39</span>  <span class=mi>40</span>  <span class=mi>41</span>  <span class=mi>42</span>  <span class=mi>43</span>  <span class=mi>44</span>  <span class=mi>45</span>  <span class=mi>46</span>  <span class=mi>47</span>  <span class=mi>48</span>  <span class=mi>49</span>  <span class=mi>50</span>  <span class=mi>51</span>  <span class=mi>52</span>  <span class=mi>53</span>
</span></span><span class=line><span class=cl>  <span class=mi>54</span>  <span class=mi>55</span>  <span class=mi>56</span>  <span class=mi>57</span>  <span class=mi>58</span>  <span class=mi>59</span>  <span class=mi>90</span>  <span class=mi>91</span>  <span class=mi>92</span>  <span class=mi>93</span>  <span class=mi>94</span>  <span class=mi>95</span>  <span class=mi>96</span>  <span class=mi>97</span>  <span class=mi>98</span>  <span class=mi>99</span> <span class=mi>100</span> <span class=mi>101</span>
</span></span><span class=line><span class=cl> <span class=mi>102</span> <span class=mi>103</span> <span class=mi>104</span> <span class=mi>105</span> <span class=mi>106</span> <span class=mi>107</span> <span class=mi>108</span> <span class=mi>109</span> <span class=mi>110</span> <span class=mi>111</span> <span class=mi>112</span> <span class=mi>113</span> <span class=mi>114</span> <span class=mi>115</span> <span class=mi>116</span> <span class=mi>117</span> <span class=mi>118</span> <span class=mi>119</span>
</span></span><span class=line><span class=cl> <span class=mi>120</span> <span class=mi>121</span> <span class=mi>122</span> <span class=mi>123</span> <span class=mi>124</span> <span class=mi>125</span> <span class=mi>126</span> <span class=mi>127</span> <span class=mi>128</span> <span class=mi>129</span> <span class=mi>130</span> <span class=mi>131</span> <span class=mi>132</span> <span class=mi>133</span> <span class=mi>134</span> <span class=mi>135</span> <span class=mi>136</span> <span class=mi>137</span>
</span></span><span class=line><span class=cl> <span class=mi>138</span> <span class=mi>139</span> <span class=mi>140</span> <span class=mi>141</span> <span class=mi>142</span> <span class=mi>143</span> <span class=mi>144</span> <span class=mi>145</span> <span class=mi>146</span> <span class=mi>147</span> <span class=mi>148</span> <span class=mi>149</span><span class=p>]</span> 
</span></span><span class=line><span class=cl><span class=n>TEST</span><span class=p>:</span> <span class=p>[</span><span class=mi>60</span> <span class=mi>61</span> <span class=mi>62</span> <span class=mi>63</span> <span class=mi>64</span> <span class=mi>65</span> <span class=mi>66</span> <span class=mi>67</span> <span class=mi>68</span> <span class=mi>69</span> <span class=mi>70</span> <span class=mi>71</span> <span class=mi>72</span> <span class=mi>73</span> <span class=mi>74</span> <span class=mi>75</span> <span class=mi>76</span> <span class=mi>77</span> <span class=mi>78</span> <span class=mi>79</span> <span class=mi>80</span> <span class=mi>81</span> <span class=mi>82</span> <span class=mi>83</span>
</span></span><span class=line><span class=cl> <span class=mi>84</span> <span class=mi>85</span> <span class=mi>86</span> <span class=mi>87</span> <span class=mi>88</span> <span class=mi>89</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>TRAIN</span><span class=p>:</span> <span class=p>[</span>  <span class=mi>0</span>   <span class=mi>1</span>   <span class=mi>2</span>   <span class=mi>3</span>   <span class=mi>4</span>   <span class=mi>5</span>   <span class=mi>6</span>   <span class=mi>7</span>   <span class=mi>8</span>   <span class=mi>9</span>  <span class=mi>10</span>  <span class=mi>11</span>  <span class=mi>12</span>  <span class=mi>13</span>  <span class=mi>14</span>  <span class=mi>15</span>  <span class=mi>16</span>  <span class=mi>17</span>
</span></span><span class=line><span class=cl>  <span class=mi>18</span>  <span class=mi>19</span>  <span class=mi>20</span>  <span class=mi>21</span>  <span class=mi>22</span>  <span class=mi>23</span>  <span class=mi>24</span>  <span class=mi>25</span>  <span class=mi>26</span>  <span class=mi>27</span>  <span class=mi>28</span>  <span class=mi>29</span>  <span class=mi>30</span>  <span class=mi>31</span>  <span class=mi>32</span>  <span class=mi>33</span>  <span class=mi>34</span>  <span class=mi>35</span>
</span></span><span class=line><span class=cl>  <span class=mi>36</span>  <span class=mi>37</span>  <span class=mi>38</span>  <span class=mi>39</span>  <span class=mi>40</span>  <span class=mi>41</span>  <span class=mi>42</span>  <span class=mi>43</span>  <span class=mi>44</span>  <span class=mi>45</span>  <span class=mi>46</span>  <span class=mi>47</span>  <span class=mi>48</span>  <span class=mi>49</span>  <span class=mi>50</span>  <span class=mi>51</span>  <span class=mi>52</span>  <span class=mi>53</span>
</span></span><span class=line><span class=cl>  <span class=mi>54</span>  <span class=mi>55</span>  <span class=mi>56</span>  <span class=mi>57</span>  <span class=mi>58</span>  <span class=mi>59</span>  <span class=mi>60</span>  <span class=mi>61</span>  <span class=mi>62</span>  <span class=mi>63</span>  <span class=mi>64</span>  <span class=mi>65</span>  <span class=mi>66</span>  <span class=mi>67</span>  <span class=mi>68</span>  <span class=mi>69</span>  <span class=mi>70</span>  <span class=mi>71</span>
</span></span><span class=line><span class=cl>  <span class=mi>72</span>  <span class=mi>73</span>  <span class=mi>74</span>  <span class=mi>75</span>  <span class=mi>76</span>  <span class=mi>77</span>  <span class=mi>78</span>  <span class=mi>79</span>  <span class=mi>80</span>  <span class=mi>81</span>  <span class=mi>82</span>  <span class=mi>83</span>  <span class=mi>84</span>  <span class=mi>85</span>  <span class=mi>86</span>  <span class=mi>87</span>  <span class=mi>88</span>  <span class=mi>89</span>
</span></span><span class=line><span class=cl> <span class=mi>120</span> <span class=mi>121</span> <span class=mi>122</span> <span class=mi>123</span> <span class=mi>124</span> <span class=mi>125</span> <span class=mi>126</span> <span class=mi>127</span> <span class=mi>128</span> <span class=mi>129</span> <span class=mi>130</span> <span class=mi>131</span> <span class=mi>132</span> <span class=mi>133</span> <span class=mi>134</span> <span class=mi>135</span> <span class=mi>136</span> <span class=mi>137</span>
</span></span><span class=line><span class=cl> <span class=mi>138</span> <span class=mi>139</span> <span class=mi>140</span> <span class=mi>141</span> <span class=mi>142</span> <span class=mi>143</span> <span class=mi>144</span> <span class=mi>145</span> <span class=mi>146</span> <span class=mi>147</span> <span class=mi>148</span> <span class=mi>149</span><span class=p>]</span> 
</span></span><span class=line><span class=cl><span class=n>TEST</span><span class=p>:</span> <span class=p>[</span> <span class=mi>90</span>  <span class=mi>91</span>  <span class=mi>92</span>  <span class=mi>93</span>  <span class=mi>94</span>  <span class=mi>95</span>  <span class=mi>96</span>  <span class=mi>97</span>  <span class=mi>98</span>  <span class=mi>99</span> <span class=mi>100</span> <span class=mi>101</span> <span class=mi>102</span> <span class=mi>103</span> <span class=mi>104</span> <span class=mi>105</span> <span class=mi>106</span> <span class=mi>107</span>
</span></span><span class=line><span class=cl> <span class=mi>108</span> <span class=mi>109</span> <span class=mi>110</span> <span class=mi>111</span> <span class=mi>112</span> <span class=mi>113</span> <span class=mi>114</span> <span class=mi>115</span> <span class=mi>116</span> <span class=mi>117</span> <span class=mi>118</span> <span class=mi>119</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>TRAIN</span><span class=p>:</span> <span class=p>[</span>  <span class=mi>0</span>   <span class=mi>1</span>   <span class=mi>2</span>   <span class=mi>3</span>   <span class=mi>4</span>   <span class=mi>5</span>   <span class=mi>6</span>   <span class=mi>7</span>   <span class=mi>8</span>   <span class=mi>9</span>  <span class=mi>10</span>  <span class=mi>11</span>  <span class=mi>12</span>  <span class=mi>13</span>  <span class=mi>14</span>  <span class=mi>15</span>  <span class=mi>16</span>  <span class=mi>17</span>
</span></span><span class=line><span class=cl>  <span class=mi>18</span>  <span class=mi>19</span>  <span class=mi>20</span>  <span class=mi>21</span>  <span class=mi>22</span>  <span class=mi>23</span>  <span class=mi>24</span>  <span class=mi>25</span>  <span class=mi>26</span>  <span class=mi>27</span>  <span class=mi>28</span>  <span class=mi>29</span>  <span class=mi>30</span>  <span class=mi>31</span>  <span class=mi>32</span>  <span class=mi>33</span>  <span class=mi>34</span>  <span class=mi>35</span>
</span></span><span class=line><span class=cl>  <span class=mi>36</span>  <span class=mi>37</span>  <span class=mi>38</span>  <span class=mi>39</span>  <span class=mi>40</span>  <span class=mi>41</span>  <span class=mi>42</span>  <span class=mi>43</span>  <span class=mi>44</span>  <span class=mi>45</span>  <span class=mi>46</span>  <span class=mi>47</span>  <span class=mi>48</span>  <span class=mi>49</span>  <span class=mi>50</span>  <span class=mi>51</span>  <span class=mi>52</span>  <span class=mi>53</span>
</span></span><span class=line><span class=cl>  <span class=mi>54</span>  <span class=mi>55</span>  <span class=mi>56</span>  <span class=mi>57</span>  <span class=mi>58</span>  <span class=mi>59</span>  <span class=mi>60</span>  <span class=mi>61</span>  <span class=mi>62</span>  <span class=mi>63</span>  <span class=mi>64</span>  <span class=mi>65</span>  <span class=mi>66</span>  <span class=mi>67</span>  <span class=mi>68</span>  <span class=mi>69</span>  <span class=mi>70</span>  <span class=mi>71</span>
</span></span><span class=line><span class=cl>  <span class=mi>72</span>  <span class=mi>73</span>  <span class=mi>74</span>  <span class=mi>75</span>  <span class=mi>76</span>  <span class=mi>77</span>  <span class=mi>78</span>  <span class=mi>79</span>  <span class=mi>80</span>  <span class=mi>81</span>  <span class=mi>82</span>  <span class=mi>83</span>  <span class=mi>84</span>  <span class=mi>85</span>  <span class=mi>86</span>  <span class=mi>87</span>  <span class=mi>88</span>  <span class=mi>89</span>
</span></span><span class=line><span class=cl>  <span class=mi>90</span>  <span class=mi>91</span>  <span class=mi>92</span>  <span class=mi>93</span>  <span class=mi>94</span>  <span class=mi>95</span>  <span class=mi>96</span>  <span class=mi>97</span>  <span class=mi>98</span>  <span class=mi>99</span> <span class=mi>100</span> <span class=mi>101</span> <span class=mi>102</span> <span class=mi>103</span> <span class=mi>104</span> <span class=mi>105</span> <span class=mi>106</span> <span class=mi>107</span>
</span></span><span class=line><span class=cl> <span class=mi>108</span> <span class=mi>109</span> <span class=mi>110</span> <span class=mi>111</span> <span class=mi>112</span> <span class=mi>113</span> <span class=mi>114</span> <span class=mi>115</span> <span class=mi>116</span> <span class=mi>117</span> <span class=mi>118</span> <span class=mi>119</span><span class=p>]</span> 
</span></span><span class=line><span class=cl><span class=n>TEST</span><span class=p>:</span> <span class=p>[</span><span class=mi>120</span> <span class=mi>121</span> <span class=mi>122</span> <span class=mi>123</span> <span class=mi>124</span> <span class=mi>125</span> <span class=mi>126</span> <span class=mi>127</span> <span class=mi>128</span> <span class=mi>129</span> <span class=mi>130</span> <span class=mi>131</span> <span class=mi>132</span> <span class=mi>133</span> <span class=mi>134</span> <span class=mi>135</span> <span class=mi>136</span> <span class=mi>137</span>
</span></span><span class=line><span class=cl> <span class=mi>138</span> <span class=mi>139</span> <span class=mi>140</span> <span class=mi>141</span> <span class=mi>142</span> <span class=mi>143</span> <span class=mi>144</span> <span class=mi>145</span> <span class=mi>146</span> <span class=mi>147</span> <span class=mi>148</span> <span class=mi>149</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=特征抽取>特征抽取</h3><p>（feature extraction）</p><p>将任意数据（如文本或图像）转换为可用于机器学习的数字特征</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>sklearn.feature_extraction</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=字典特征提取-dictvectorizer>字典特征提取 DictVectorizer</h4><p>它是一个转换器，应用时需要进行实例化</p><h5 id=api>API</h5><p><em><strong>sklearn.feature_extraction.DictVectorizer(sparse=True,…)</strong></em></p><ul><li><em>DictVectorizer.fit_transform(X)</em><ul><li>X：字典或者包含字典的迭代器</li><li>返回 sparse 矩阵或 array 数组</li></ul></li><li><em>DictVectorizer.inverse_transform(X)</em><ul><li>X：array 数组或者 sparse 矩阵</li><li>返回转换之前数据格式</li></ul></li><li><em>DictVectorizer.get_feature_names()</em><ul><li>返回类别名称</li></ul></li></ul><h5 id=例>例</h5><p>流程分析</p><ul><li>实例化类 DictVectorizer</li><li>调用 fit_transform 方法输入数据并转换（注意返回格式）</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.feature_extraction</span> <span class=kn>import</span> <span class=n>DictVectorizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=p>[{</span><span class=s1>&#39;city&#39;</span><span class=p>:</span> <span class=s1>&#39;北京&#39;</span><span class=p>,</span><span class=s1>&#39;temperature&#39;</span><span class=p>:</span><span class=mi>100</span><span class=p>},</span> <span class=p>{</span><span class=s1>&#39;city&#39;</span><span class=p>:</span> <span class=s1>&#39;上海&#39;</span><span class=p>,</span><span class=s1>&#39;temperature&#39;</span><span class=p>:</span><span class=mi>60</span><span class=p>},</span> <span class=p>{</span><span class=s1>&#39;city&#39;</span><span class=p>:</span> <span class=s1>&#39;深圳&#39;</span><span class=p>,</span><span class=s1>&#39;temperature&#39;</span><span class=p>:</span><span class=mi>30</span><span class=p>}]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1、实例化一个转换器类</span>
</span></span><span class=line><span class=cl><span class=n>transfer</span> <span class=o>=</span> <span class=n>DictVectorizer</span><span class=p>(</span><span class=n>sparse</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span><span class=c1># 拒绝返回稀疏矩阵</span>
</span></span><span class=line><span class=cl><span class=c1># 2、调用fit_transform</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;返回的结果:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;特征名字：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>transfer</span><span class=o>.</span><span class=n>get_feature_names</span><span class=p>())</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>返回的结果</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=p>[[</span>  <span class=mf>0.</span>   <span class=mf>1.</span>   <span class=mf>0.</span> <span class=mf>100.</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span>  <span class=mf>1.</span>   <span class=mf>0.</span>   <span class=mf>0.</span>  <span class=mf>60.</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span>  <span class=mf>0.</span>   <span class=mf>0.</span>   <span class=mf>1.</span>  <span class=mf>30.</span><span class=p>]]</span>
</span></span><span class=line><span class=cl><span class=n>特征名字</span><span class=err>：</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=s1>&#39;city=上海&#39;</span><span class=p>,</span> <span class=s1>&#39;city=北京&#39;</span><span class=p>,</span> <span class=s1>&#39;city=深圳&#39;</span><span class=p>,</span> <span class=s1>&#39;temperature&#39;</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><p>若返回稀疏矩阵，改sparse=True</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>返回的结果</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>	<span class=mf>1.0</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>	<span class=mf>100.0</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>	<span class=mf>1.0</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>	<span class=mf>60.0</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>	<span class=mf>1.0</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>	<span class=mf>30.0</span>
</span></span><span class=line><span class=cl><span class=n>特征名字</span><span class=err>：</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=s1>&#39;city=上海&#39;</span><span class=p>,</span> <span class=s1>&#39;city=北京&#39;</span><span class=p>,</span> <span class=s1>&#39;city=深圳&#39;</span><span class=p>,</span> <span class=s1>&#39;temperature&#39;</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><p>这个处理数据的技巧叫做 <em>one-hot</em> 编码</p><h4 id=文本词频特征提取-textcountvectorizer>文本词频特征提取 text.CountVectorizer</h4><p>对文本数据进行词频特征值化</p><p>它是一个转换器，应用时需要进行实例化</p><h5 id=api-1>API</h5><p><em><strong>CountVectorizer(stop_words=[])</strong></em></p><ul><li>CountVectorizer.fit_transform(X)<ul><li>X：文本或者包含文本字符串的可迭代对象</li><li>返回 sparse 矩阵</li></ul></li><li>CountVectorizer.inverse_transform(X)<ul><li>X：array 数组或者 sparse 矩阵</li><li>返回转换之前数据格</li></ul></li><li>CountVectorizer.get_feature_names()<ul><li>返回值单词列表</li></ul></li></ul><h5 id=例-1>例</h5><p>流程分析</p><ul><li>实例化类 CountVectorizer</li><li>调用 fit_transform 方法输入数据并转换 （注意返回格式，利用 toarray() 进行 sparse 矩阵转换 array 数组）</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.feature_extraction.text</span> <span class=kn>import</span> <span class=n>CountVectorizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;life is short,i like like python&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;life is too long,i dislike python&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1、实例化一个转换器类</span>
</span></span><span class=line><span class=cl><span class=n>transfer</span> <span class=o>=</span> <span class=n>CountVectorizer</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># 2、调用fit_transform</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;文本特征抽取的结果：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>data</span><span class=o>.</span><span class=n>toarray</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;返回特征名字：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>transfer</span><span class=o>.</span><span class=n>get_feature_names</span><span class=p>())</span>
</span></span></code></pre></td></tr></table></div></div><p>输出（因为没有 sparse 参数，若要转换成二维数组形式，需要利用 <code>toarray()</code>）</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>文本特征抽取的结果</span><span class=err>：</span>
</span></span><span class=line><span class=cl> <span class=p>[[</span><span class=mi>0</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>1</span><span class=p>]]</span>
</span></span><span class=line><span class=cl><span class=n>返回特征名字</span><span class=err>：</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=s1>&#39;dislike&#39;</span><span class=p>,</span> <span class=s1>&#39;is&#39;</span><span class=p>,</span> <span class=s1>&#39;life&#39;</span><span class=p>,</span> <span class=s1>&#39;like&#39;</span><span class=p>,</span> <span class=s1>&#39;long&#39;</span><span class=p>,</span> <span class=s1>&#39;python&#39;</span><span class=p>,</span> <span class=s1>&#39;short&#39;</span><span class=p>,</span> <span class=s1>&#39;too&#39;</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><p>若直接返回稀疏矩阵</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>文本特征抽取的结果</span><span class=err>：</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>	<span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>	<span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>6</span><span class=p>)</span>	<span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>	<span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span>	<span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>	<span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>	<span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span>	<span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>7</span><span class=p>)</span>	<span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>	<span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>	<span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>返回特征名字</span><span class=err>：</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=s1>&#39;dislike&#39;</span><span class=p>,</span> <span class=s1>&#39;is&#39;</span><span class=p>,</span> <span class=s1>&#39;life&#39;</span><span class=p>,</span> <span class=s1>&#39;like&#39;</span><span class=p>,</span> <span class=s1>&#39;long&#39;</span><span class=p>,</span> <span class=s1>&#39;python&#39;</span><span class=p>,</span> <span class=s1>&#39;short&#39;</span><span class=p>,</span> <span class=s1>&#39;too&#39;</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><h5 id=中文处理>中文处理</h5><p>使用 jieba 分词库</p><p><em><strong>jieba.cut()</strong></em></p><ul><li>返回词语组成的生成器</li></ul><p>分析</p><ul><li>准备句子，利用jieba.cut进行分词</li><li>实例化CountVectorizer</li><li>将分词结果变成字符串当作fit_transform的输入值</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.feature_extraction.text</span> <span class=kn>import</span> <span class=n>CountVectorizer</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>jieba</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>cut_word</span><span class=p>(</span><span class=n>text</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 用结巴对中文字符串进行分词</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=s2>&#34; &#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=n>jieba</span><span class=o>.</span><span class=n>cut</span><span class=p>(</span><span class=n>text</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>text</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 将原始数据转换成分好词的形式</span>
</span></span><span class=line><span class=cl><span class=n>text_list</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>sent</span> <span class=ow>in</span> <span class=n>data</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>text_list</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>cut_word</span><span class=p>(</span><span class=n>sent</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>text_list</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1、实例化一个转换器类</span>
</span></span><span class=line><span class=cl><span class=n>transfer</span> <span class=o>=</span> <span class=n>CountVectorizer</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># 2、调用fit_transform</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>text_list</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;文本特征抽取的结果：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>data</span><span class=o>.</span><span class=n>toarray</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;返回特征名字：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>transfer</span><span class=o>.</span><span class=n>get_feature_names</span><span class=p>())</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=s1>&#39;一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。&#39;</span><span class=p>,</span> <span class=s1>&#39;我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。&#39;</span><span class=p>,</span> <span class=s1>&#39;如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>文本特征抽取的结果</span><span class=err>：</span>
</span></span><span class=line><span class=cl> <span class=p>[[</span><span class=mi>2</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>  <span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>3</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>  <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>4</span> <span class=mi>3</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>  <span class=mi>0</span><span class=p>]]</span>
</span></span><span class=line><span class=cl><span class=n>返回特征名字</span><span class=err>：</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=s1>&#39;一种&#39;</span><span class=p>,</span> <span class=s1>&#39;不会&#39;</span><span class=p>,</span> <span class=s1>&#39;不要&#39;</span><span class=p>,</span> <span class=s1>&#39;之前&#39;</span><span class=p>,</span> <span class=s1>&#39;了解&#39;</span><span class=p>,</span> <span class=s1>&#39;事物&#39;</span><span class=p>,</span> <span class=s1>&#39;今天&#39;</span><span class=p>,</span> <span class=s1>&#39;光是在&#39;</span><span class=p>,</span> <span class=s1>&#39;几百万年&#39;</span><span class=p>,</span> <span class=s1>&#39;发出&#39;</span><span class=p>,</span> <span class=s1>&#39;取决于&#39;</span><span class=p>,</span> <span class=s1>&#39;只用&#39;</span><span class=p>,</span> <span class=s1>&#39;后天&#39;</span><span class=p>,</span> <span class=s1>&#39;含义&#39;</span><span class=p>,</span> <span class=s1>&#39;大部分&#39;</span><span class=p>,</span> <span class=s1>&#39;如何&#39;</span><span class=p>,</span> <span class=s1>&#39;如果&#39;</span><span class=p>,</span> <span class=s1>&#39;宇宙&#39;</span><span class=p>,</span> <span class=s1>&#39;我们&#39;</span><span class=p>,</span> <span class=s1>&#39;所以&#39;</span><span class=p>,</span> <span class=s1>&#39;放弃&#39;</span><span class=p>,</span> <span class=s1>&#39;方式&#39;</span><span class=p>,</span> <span class=s1>&#39;明天&#39;</span><span class=p>,</span> <span class=s1>&#39;星系&#39;</span><span class=p>,</span> <span class=s1>&#39;晚上&#39;</span><span class=p>,</span> <span class=s1>&#39;某样&#39;</span><span class=p>,</span> <span class=s1>&#39;残酷&#39;</span><span class=p>,</span> <span class=s1>&#39;每个&#39;</span><span class=p>,</span> <span class=s1>&#39;看到&#39;</span><span class=p>,</span> <span class=s1>&#39;真正&#39;</span><span class=p>,</span> <span class=s1>&#39;秘密&#39;</span><span class=p>,</span> <span class=s1>&#39;绝对&#39;</span><span class=p>,</span> <span class=s1>&#39;美好&#39;</span><span class=p>,</span> <span class=s1>&#39;联系&#39;</span><span class=p>,</span> <span class=s1>&#39;过去&#39;</span><span class=p>,</span> <span class=s1>&#39;还是&#39;</span><span class=p>,</span> <span class=s1>&#39;这样&#39;</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=tf-idf-文本特征提取-texttfidfvectorizer>Tf-idf 文本特征提取 text.TfidfVectorizer</h4><p>TF-IDF 的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。</p><p>TF-IDF 作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。</p><h5 id=公式>公式</h5><ul><li>词频（term frequency，tf）指的是某一个给定的词语在该文件中出现的频率</li><li>逆向文档频率（inverse document frequency，idf）是一个词语普遍重要性的度量。由总文件数目除以包含该词语之文件的数目，再将得到的商取以 10 为底的对数得到</li></ul><p>$$
tfidf_{i,j}=tf_{i,j}\times idf_i
$$</p><blockquote><p>假如一篇文件的总词语数是 100 个，而词语"非常"出现了 5 次，那么"非常"一词在该文件中的词频就是5/100=0.05。而计算文件频率（IDF）的方法是以文件集的文件总数，除以出现"非常"一词的文件数。所以，如果"非常"一词在1,000份文件出现过，而文件总数是 10,000,000 份的话，其逆向文件频率就是 lg（10,000,000 / 1,0000）=3。最后"非常"对于这篇文档的tf-idf的分数为0.05 * 3=0.15</p></blockquote><h5 id=api-2>API</h5><p><em><strong>TfidfVectorizer(stop_words=[])</strong></em></p><ul><li>TfidfVectorizer.fit_transform(X)<ul><li>X：文本或者包含文本字符串的可迭代对象</li><li>返回sparse矩阵</li></ul></li><li>TfidfVectorizer.inverse_transform(X)<ul><li>X：array数组或者sparse矩阵</li><li>返回转换之前数据格</li></ul></li><li>TfidfVectorizer.get_feature_names()<ul><li>返回值单词列表</li></ul></li></ul><h5 id=例-2>例</h5><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.feature_extraction.text</span> <span class=kn>import</span> <span class=n>TfidfVectorizer</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>jieba</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>cut_word</span><span class=p>(</span><span class=n>text</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 用结巴对中文字符串进行分词</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=s2>&#34; &#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=n>jieba</span><span class=o>.</span><span class=n>cut</span><span class=p>(</span><span class=n>text</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>text</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 将原始数据转换成分好词的形式</span>
</span></span><span class=line><span class=cl><span class=n>text_list</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>sent</span> <span class=ow>in</span> <span class=n>data</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>text_list</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>cut_word</span><span class=p>(</span><span class=n>sent</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>text_list</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1、实例化一个转换器类</span>
</span></span><span class=line><span class=cl><span class=n>transfer</span> <span class=o>=</span> <span class=n>TfidfVectorizer</span><span class=p>(</span><span class=n>stop_words</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;一种&#39;</span><span class=p>,</span> <span class=s1>&#39;不会&#39;</span><span class=p>,</span> <span class=s1>&#39;不要&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=c1># 2、调用fit_transform</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>text_list</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;文本特征抽取的结果：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>data</span><span class=o>.</span><span class=n>toarray</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;返回特征名字：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>transfer</span><span class=o>.</span><span class=n>get_feature_names</span><span class=p>())</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=p>[</span><span class=s1>&#39;一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。&#39;</span><span class=p>,</span> <span class=s1>&#39;我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。&#39;</span><span class=p>,</span> <span class=s1>&#39;如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>文本特征抽取的结果</span><span class=err>：</span>
</span></span><span class=line><span class=cl> <span class=p>[[</span><span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.43643578</span> <span class=mf>0.</span>         <span class=mf>0.</span>
</span></span><span class=line><span class=cl>  <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.21821789</span> <span class=mf>0.</span>         <span class=mf>0.21821789</span>
</span></span><span class=line><span class=cl>  <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.21821789</span> <span class=mf>0.21821789</span>
</span></span><span class=line><span class=cl>  <span class=mf>0.</span>         <span class=mf>0.43643578</span> <span class=mf>0.</span>         <span class=mf>0.21821789</span> <span class=mf>0.</span>         <span class=mf>0.43643578</span>
</span></span><span class=line><span class=cl>  <span class=mf>0.21821789</span> <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.21821789</span> <span class=mf>0.21821789</span>
</span></span><span class=line><span class=cl>  <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.21821789</span> <span class=mf>0.</span>        <span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.2410822</span>  <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.2410822</span>  <span class=mf>0.2410822</span>
</span></span><span class=line><span class=cl>  <span class=mf>0.2410822</span>  <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.</span>
</span></span><span class=line><span class=cl>  <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.2410822</span>  <span class=mf>0.55004769</span> <span class=mf>0.</span>         <span class=mf>0.</span>
</span></span><span class=line><span class=cl>  <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.2410822</span>  <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.</span>
</span></span><span class=line><span class=cl>  <span class=mf>0.</span>         <span class=mf>0.48216441</span> <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.</span>
</span></span><span class=line><span class=cl>  <span class=mf>0.</span>         <span class=mf>0.2410822</span>  <span class=mf>0.</span>         <span class=mf>0.2410822</span> <span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.</span>         <span class=mf>0.644003</span>   <span class=mf>0.48300225</span> <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.</span>
</span></span><span class=line><span class=cl>  <span class=mf>0.</span>         <span class=mf>0.16100075</span> <span class=mf>0.16100075</span> <span class=mf>0.</span>         <span class=mf>0.16100075</span> <span class=mf>0.</span>
</span></span><span class=line><span class=cl>  <span class=mf>0.16100075</span> <span class=mf>0.16100075</span> <span class=mf>0.</span>         <span class=mf>0.12244522</span> <span class=mf>0.</span>         <span class=mf>0.</span>
</span></span><span class=line><span class=cl>  <span class=mf>0.16100075</span> <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.16100075</span> <span class=mf>0.</span>
</span></span><span class=line><span class=cl>  <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.3220015</span>  <span class=mf>0.16100075</span> <span class=mf>0.</span>         <span class=mf>0.</span>
</span></span><span class=line><span class=cl>  <span class=mf>0.16100075</span> <span class=mf>0.</span>         <span class=mf>0.</span>         <span class=mf>0.</span>        <span class=p>]]</span>
</span></span><span class=line><span class=cl><span class=n>返回特征名字</span><span class=err>：</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=s1>&#39;之前&#39;</span><span class=p>,</span> <span class=s1>&#39;了解&#39;</span><span class=p>,</span> <span class=s1>&#39;事物&#39;</span><span class=p>,</span> <span class=s1>&#39;今天&#39;</span><span class=p>,</span> <span class=s1>&#39;光是在&#39;</span><span class=p>,</span> <span class=s1>&#39;几百万年&#39;</span><span class=p>,</span> <span class=s1>&#39;发出&#39;</span><span class=p>,</span> <span class=s1>&#39;取决于&#39;</span><span class=p>,</span> <span class=s1>&#39;只用&#39;</span><span class=p>,</span> <span class=s1>&#39;后天&#39;</span><span class=p>,</span> <span class=s1>&#39;含义&#39;</span><span class=p>,</span> <span class=s1>&#39;大部分&#39;</span><span class=p>,</span> <span class=s1>&#39;如何&#39;</span><span class=p>,</span> <span class=s1>&#39;如果&#39;</span><span class=p>,</span> <span class=s1>&#39;宇宙&#39;</span><span class=p>,</span> <span class=s1>&#39;我们&#39;</span><span class=p>,</span> <span class=s1>&#39;所以&#39;</span><span class=p>,</span> <span class=s1>&#39;放弃&#39;</span><span class=p>,</span> <span class=s1>&#39;方式&#39;</span><span class=p>,</span> <span class=s1>&#39;明天&#39;</span><span class=p>,</span> <span class=s1>&#39;星系&#39;</span><span class=p>,</span> <span class=s1>&#39;晚上&#39;</span><span class=p>,</span> <span class=s1>&#39;某样&#39;</span><span class=p>,</span> <span class=s1>&#39;残酷&#39;</span><span class=p>,</span> <span class=s1>&#39;每个&#39;</span><span class=p>,</span> <span class=s1>&#39;看到&#39;</span><span class=p>,</span> <span class=s1>&#39;真正&#39;</span><span class=p>,</span> <span class=s1>&#39;秘密&#39;</span><span class=p>,</span> <span class=s1>&#39;绝对&#39;</span><span class=p>,</span> <span class=s1>&#39;美好&#39;</span><span class=p>,</span> <span class=s1>&#39;联系&#39;</span><span class=p>,</span> <span class=s1>&#39;过去&#39;</span><span class=p>,</span> <span class=s1>&#39;还是&#39;</span><span class=p>,</span> <span class=s1>&#39;这样&#39;</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=特征预处理>特征预处理</h3><p>（feature preprocessing）</p><p>通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程</p><p>数据的无量纲处理：<strong>使不同规格的数据转换到同一规格</strong></p><ul><li>归一化</li><li>标准化</li></ul><p>特征的单位或者大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级**，**容易影响（支配）目标结果，使得一些算法无法学习到其它的特征，所以要进行归一化/标准化。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>sklearn.preprocessing</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=归一化>归一化</h4><p>通过对原始数据进行变换把数据映射到（默认为）[0,1] 之间</p><h5 id=公式-1>公式</h5><p>$$
X&rsquo;=\cfrac{x-min}{max-min}\
X&rsquo;&rsquo;=X&rsquo;*(mx-mi)+mi
$$</p><blockquote><p>作用于每一列，max 为一列的最大值，min 为一列的最小值，X’’为最终结果，mx，mi分别为指定区间值，默认mx为1，mi为0</p></blockquote><h5 id=api-3>API</h5><p><em><strong>MinMaxScaler (feature_range=(0,1)… )</strong></em></p><ul><li><em>MinMaxScalar.fit_transform(X)</em><ul><li>X：numpy array格式的数据 [n_samples,n_features]</li><li>返回值：转换后的形状相同的array</li></ul></li></ul><h5 id=例-3>例</h5><p>以下为数据实例</p><img src=https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20210118192214857.png alt=image-20210118192214857 style=zoom:80%><ul><li><p>实例化 MinMaxScalar</p></li><li><p>通过 fit_transform 转换</p></li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>MinMaxScaler</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>path</span> <span class=o>=</span> <span class=s2>&#34;../Data/Dating.txt&#34;</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=n>path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1、实例化一个转换器类</span>
</span></span><span class=line><span class=cl><span class=n>transfer</span> <span class=o>=</span> <span class=n>MinMaxScaler</span><span class=p>(</span><span class=n>feature_range</span><span class=o>=</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span> <span class=c1># 默认 MIN=0, MAX=1</span>
</span></span><span class=line><span class=cl><span class=c1># 2、调用fit_transform</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>data</span><span class=p>[[</span><span class=s1>&#39;milage&#39;</span><span class=p>,</span><span class=s1>&#39;Liters&#39;</span><span class=p>,</span><span class=s1>&#39;Consumtime&#39;</span><span class=p>]])</span> <span class=c1># 需要传numpy array格式, 返回array</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;最小值最大值归一化处理的结果：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>data</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>最小值最大值归一化处理的结果</span><span class=err>：</span>
</span></span><span class=line><span class=cl> <span class=p>[[</span><span class=mf>0.43582641</span> <span class=mf>0.58819286</span> <span class=mf>0.53237967</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.</span>         <span class=mf>0.48794044</span> <span class=mf>1.</span>        <span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.19067405</span> <span class=mf>0.</span>         <span class=mf>0.43571351</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>1.</span>         <span class=mf>1.</span>         <span class=mf>0.19139157</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>0.3933518</span>  <span class=mf>0.01947089</span> <span class=mf>0.</span>        <span class=p>]]</span>
</span></span></code></pre></td></tr></table></div></div><p>注意最大值最小值是变化的，另外，最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景</p><h4 id=标准化>标准化</h4><p>通过对原始数据进行变换把数据变换到均值为0，标准差为1的范围内</p><p>优势</p><ul><li>对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变</li><li>对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小</li></ul><h5 id=公式-2>公式</h5><p>$$
X&rsquo;=\cfrac{x-mean}{\sigma}
$$</p><blockquote><p>作用于每一列，mean 为平均值，σ 为标准差</p></blockquote><h5 id=api-4>API</h5><p><em><strong>StandardScaler( )</strong></em></p><ul><li><em>StandardScaler.fit_transform(X)</em><ul><li>X：numpy array 格式的数据[n_samples, n_features]</li><li>返回值：转换后的形状相同的array</li></ul></li><li><em>StandardScaler.mean_</em><ul><li>返回值：每一列特征的平均值</li></ul></li><li><em>StandardScaler.var_</em><ul><li>返回值：每一列特征的方差</li></ul></li></ul><h5 id=例-4>例</h5><p>同样对 2.4.1 的数据进行处理</p><ul><li><p>实例化 StandardScaler</p></li><li><p>通过fit_transform转换</p></li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>StandardScaler</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=n>path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1、实例化一个转换器类</span>
</span></span><span class=line><span class=cl><span class=n>transfer</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span> <span class=c1># 值都在0附近,所以有负数是正常的</span>
</span></span><span class=line><span class=cl><span class=c1># 2、调用fit_transform</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>data</span><span class=p>[[</span><span class=s1>&#39;milage&#39;</span><span class=p>,</span><span class=s1>&#39;Liters&#39;</span><span class=p>,</span><span class=s1>&#39;Consumtime&#39;</span><span class=p>]])</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;标准化的结果:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;每一列特征的平均值：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>transfer</span><span class=o>.</span><span class=n>mean_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;每一列特征的方差：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>transfer</span><span class=o>.</span><span class=n>var_</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>标准化的结果</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=p>[[</span> <span class=mf>0.0947602</span>   <span class=mf>0.44990013</span>  <span class=mf>0.29573441</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=o>-</span><span class=mf>1.20166916</span>  <span class=mf>0.18312874</span>  <span class=mf>1.67200507</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=o>-</span><span class=mf>0.63448132</span> <span class=o>-</span><span class=mf>1.11527928</span>  <span class=mf>0.01123265</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span> <span class=mf>1.77297701</span>  <span class=mf>1.54571769</span> <span class=o>-</span><span class=mf>0.70784025</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=o>-</span><span class=mf>0.03158673</span> <span class=o>-</span><span class=mf>1.06346729</span> <span class=o>-</span><span class=mf>1.27113187</span><span class=p>]]</span>
</span></span><span class=line><span class=cl><span class=n>每一列特征的平均值</span><span class=err>：</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>3.8988000e+04</span> <span class=mf>6.3478996e+00</span> <span class=mf>7.9924800e-01</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>每一列特征的方差</span><span class=err>：</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>4.15683072e+08</span> <span class=mf>1.93505309e+01</span> <span class=mf>2.73652475e-01</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=特征降维>特征降维</h3><p>（Feature Dimension Reduce）</p><p>降维是指在某些限定条件下，降低随机变量（特征）个数，得到一组“不相关”主变量的过程</p><p>两种方式</p><ul><li>特征选择</li><li>主成分分析（可以理解一种特征提取的方式）</li></ul><h4 id=特征选择>特征选择</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>sklearn.feature_selection</span>
</span></span></code></pre></td></tr></table></div></div><p>数据中包含冗余或无关变量（或称特征、属性、指标等），旨在从原有特征中找出主要特征</p><p>方法</p><ul><li>过滤式（Filter）：主要探究特征本身特点、特征与特征和目标值之间关联<ul><li>方差选择法：低方差特征过滤</li><li>相关系数</li></ul></li><li>嵌入式（Embedded）：算法自动选择特征（特征与目标值之间的关联）<ul><li>决策树：信息熵、信息增益</li><li>正则化：L1、L2</li><li>深度学习：卷积等</li></ul></li></ul><h5 id=低方差特征过滤>低方差特征过滤</h5><p>删除低方差的一些特征</p><h6 id=api-5>API</h6><p><em><strong>VarianceThreshold(threshold = 0.0)</strong></em></p><ul><li><em>Variance.fit_transform(X)</em><ul><li>X：numpy array 格式的数据 [n_samples, n_features]</li><li>返回值：训练集差异低于 threshold 的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。</li></ul></li></ul><h6 id=例-5>例</h6><p>处理以下例子</p><img src=https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20210118192214857.png alt=image-20210118192214857 style=zoom:80%><p>分析</p><ul><li><p>初始化 VarianceThreshold ，指定阀值方差</p></li><li><p>调用 fit_transform</p></li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.feature_selection</span> <span class=kn>import</span> <span class=n>VarianceThreshold</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>path</span><span class=o>=</span><span class=s2>&#34;../Data/Dating.txt&#34;</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=n>path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1、实例化一个转换器类</span>
</span></span><span class=line><span class=cl><span class=n>transfer</span> <span class=o>=</span> <span class=n>VarianceThreshold</span><span class=p>(</span><span class=n>threshold</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 2、调用fit_transform</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>data</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;删除低方差特征的结果：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;形状：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>data</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>删除低方差特征的结果</span><span class=err>：</span>
</span></span><span class=line><span class=cl> <span class=p>[[</span><span class=mf>4.0920000e+04</span> <span class=mf>8.3269760e+00</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>1.4488000e+04</span> <span class=mf>7.1534690e+00</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>2.6052000e+04</span> <span class=mf>1.4418710e+00</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>7.5136000e+04</span> <span class=mf>1.3147394e+01</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=mf>3.8344000e+04</span> <span class=mf>1.6697880e+00</span><span class=p>]]</span>
</span></span><span class=line><span class=cl><span class=n>形状</span><span class=err>：</span>
</span></span><span class=line><span class=cl> <span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h5 id=相关系数>相关系数</h5><p>去除相关特征（correlated feature）的影响</p><p>使用 Scipy 实现</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>scipy.stats</span> <span class=kn>import</span> <span class=n>pearsonr</span>
</span></span></code></pre></td></tr></table></div></div><h6 id=原理>原理</h6><p>皮尔逊相关系数（Pearson Correlation Coefficient）：反映变量之间相关关系密切程度的统计指标
$$
r=\cfrac{n\sum xy=\sum x\sum y}{\sqrt{n\sum x^2 -(\sum x)^2}\sqrt{n\sum y^2-(\sum y)^2}}
$$
相关系数的值介于–1与+1之间，即–1≤ r ≤+1。其性质如下：</p><ul><li>当r > 0时，表示两变量正相关，r &lt; 0时，两变量为负相关</li><li>当|r|=1时，表示两变量为完全相关，当 r=0 时，表示两变量间无相关关系</li><li>当0&lt;|r|&lt;1时，表示两变量存在一定程度的相关。且|r|越接近1，两变量间线性关系越密切；|r|越接近于0，表示两变量的线性相关越弱</li><li>一般可按三级划分：|r|&lt;0.4为低度相关；0.4≤|r|&lt;0.7为显著性相关；0.7≤|r|&lt;1为高度线性相关</li></ul><h6 id=api-6>API</h6><p><em><strong>pearsonr(X, Y)</strong></em></p><ul><li>X：numpy array 格式的数据</li><li>Y：numpy array 格式的数据</li><li>返回值<ul><li>r：相关系数 [-1，1] 之间</li><li>p-value：p值（p值越小，表示相关系数越显著，一般p值在500个样本以上时有较高的可靠性）</li></ul></li></ul><p>如果相关性高可用以下方法:</p><ol><li><p>选取其中一个特征</p></li><li><p>两个特征加权求和</p></li><li><p>主成分分析（高维数据变低维，舍弃原由数据，创造新数据，如：压缩数据维数，降低原数据复杂度，损失少了信息）</p></li></ol><h6 id=例-6>例</h6><p>两两特征之间进行相关性计算</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>scipy.stats</span> <span class=kn>import</span> <span class=n>pearsonr</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>path</span><span class=o>=</span><span class=s2>&#34;../Data/Dating.txt&#34;</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=n>path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 皮尔逊相关系数范围[-1,1], 如果大于0就是正相关(越接近1就越相关), 反之亦然</span>
</span></span><span class=line><span class=cl><span class=n>r</span> <span class=o>=</span> <span class=n>pearsonr</span><span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=s2>&#34;milage&#34;</span><span class=p>],</span> <span class=n>data</span><span class=p>[</span><span class=s2>&#34;Liters&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;milage和Liters的相关系数为:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>r</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>r</span> <span class=o>=</span> <span class=n>pearsonr</span><span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=s2>&#34;milage&#34;</span><span class=p>],</span> <span class=n>data</span><span class=p>[</span><span class=s2>&#34;Consumtime&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;milage和Liters的相关系数为:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>r</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>milage和Liters的相关系数为</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=mf>0.660861943290103</span><span class=p>,</span> <span class=mf>0.2246299034335304</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>milage和Liters的相关系数为</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=o>-</span><span class=mf>0.6406267138718624</span><span class=p>,</span> <span class=mf>0.2441916485876286</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=主成分分析>主成分分析</h4><p>（PCA）将数据分解为较低维数空间</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.decomposition</span> <span class=kn>import</span> <span class=n>PCA</span>
</span></span></code></pre></td></tr></table></div></div><h5 id=概念>概念</h5><ul><li>定义：高维数据转化为低维数据的过程，在此过程中可能会舍弃原有数据、创造新的变量</li><li>作用：是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息</li><li>应用：回归分析或者聚类分析当中</li></ul><h5 id=api-7>API</h5><p><em><strong>PCA(n_components=None)</strong></em></p><ul><li><p>参数：<em>n_components</em></p><ul><li>小数：表示保留百分之多少的信息</li><li>整数：减少到多少特征</li></ul></li><li><p><em>PCA.fit_transform(X)</em></p><ul><li>X：numpy array 格式的数据 [n_samples,n_features]</li><li>返回值：转换后指定维度的 array</li></ul></li></ul><h5 id=例-7>例</h5><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.decomposition</span> <span class=kn>import</span> <span class=n>PCA</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=p>[[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>],</span> <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>5</span><span class=p>],</span> <span class=p>[</span><span class=mi>10</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>]]</span>  <span class=c1># 3*4矩阵 包含四个特征</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>transfer</span> <span class=o>=</span> <span class=n>PCA</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span> <span class=c1># 为整数就是转为多少个特征  保留的至少都比原特征值少一个</span>
</span></span><span class=line><span class=cl><span class=n>data_new</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;(主成分分析)PCA降维:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>data_new</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=p>(</span><span class=n>主成分分析</span><span class=p>)</span><span class=n>PCA降维</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=p>[[</span><span class=o>-</span><span class=mf>3.57495904e+00</span> <span class=o>-</span><span class=mf>6.64748145e-01</span>  <span class=mf>1.07947657e-16</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=o>-</span><span class=mf>3.17447323e+00</span>  <span class=mf>6.91574499e-01</span>  <span class=mf>1.07947657e-16</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span> <span class=mf>6.74943227e+00</span> <span class=o>-</span><span class=mf>2.68263539e-02</span>  <span class=mf>1.07947657e-16</span><span class=p>]]</span>
</span></span></code></pre></td></tr></table></div></div><p>若 n_components 设为0.95</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=p>(</span><span class=n>主成分分析</span><span class=p>)</span><span class=n>PCA降维</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=p>[[</span><span class=o>-</span><span class=mf>3.57495904</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span><span class=o>-</span><span class=mf>3.17447323</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>[</span> <span class=mf>6.74943227</span><span class=p>]]</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=转换器和估计器>转换器和估计器</h2><h3 id=转换器>转换器</h3><p>（transformer）</p><p>特征工程的接口称之为转换器；转换器是特征工程的父类</p><p>调用步骤</p><ol><li><p>实例化 (实例化的是一个转换器类(Transformer))</p></li><li><p>调用fit_transform(对于文档建立分类词频矩阵，不能同时调用)</p></li></ol><p>转换器调用形式</p><ul><li>fit_transform()</li><li>fit()<ul><li>按公式计算</li></ul></li><li>transform()<ul><li>进行最终的转换</li></ul></li></ul><h3 id=估计器>估计器</h3><p>（estimator）</p><p>估计器实现了算法的API，估计器是算法的父类</p><ul><li>用于分类的估计器：<ul><li>sklearn.neighbors k-近邻算法</li><li>sklearn.naive_bayes 贝叶斯</li><li>sklearn.linear_model.LogisticRegression 逻辑回归</li><li>sklearn.tree 决策树与随机森林</li></ul></li><li>用于回归的估计器：<ul><li>sklearn.linear_model.LinearRegression 线性回归</li><li>sklearn.linear_model.Ridge 岭回归</li></ul></li><li>用于无监督学习的估计器<ul><li>sklearn.cluster.KMeans 聚类</li></ul></li></ul><p>调用步骤</p><ol><li><p>实例化估计器类estimator</p></li><li><p>进行训练，一旦调用完毕，意味着模型生成</p><ul><li><em>estimator.fit(x_train, y_train)</em></li></ul></li><li><p>模型评估</p><ul><li><p>直接比对真实值和预测值</p><p><em>y_predict = estimator.predict(x_test)</em></p><p><em>y_test == y_predict</em></p></li><li><p>计算准确率</p><p><em>accuracy = estimator.score(x_test, y_test)</em></p></li></ul></li></ol><img src=https://trou.oss-cn-shanghai.aliyuncs.com/img/估计器工作流程.png alt=估计器工作流程 style=zoom:67%><h3 id=模型选择与调优>模型选择与调优</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>GridSearchCV</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=概念-1>概念</h4><h5 id=交叉验证>交叉验证</h5><p>（cross validation）</p><p>将拿到的训练数据，分为训练和验证集</p><h5 id=超参数搜索-网格搜索>超参数搜索-网格搜索</h5><p>（Grid Search）</p><p>通常情况下，有很多参数是需要手动指定的（如k-近邻算法中的K值），这种叫超参数。但是手动过程繁杂，所以需要对模型预设几种超参数组合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。</p><h4 id=api-8>API</h4><p><em><strong>GridSearchCV(estimator, param_grid=None,cv=None)</strong></em></p><p>对估计器的指定参数值进行详尽搜索</p><ul><li>参数<ul><li><em>estimator</em>：估计器对象</li><li><em>param_grid</em>：估计器参数 <code>(dict){“n_neighbors”:[1,3,5]}</code></li><li><em>cv</em>：指定几折交叉验证</li></ul></li><li>方法<ul><li><em>fit</em>：输入训练数据</li><li><em>score</em>：准确率</li></ul></li><li>结果分析：<ul><li><em>best_params_</em>：在交叉验证中验证的最好超参数</li><li><em>best_score_</em>：在交叉验证中验证的最好结果</li><li><em>best_estimator_</em>：最好的参数模型</li><li><em>cv_results_</em>：每次交叉验证后的验证集准确率结果和训练集准确率结果</li></ul></li></ul><h3 id=分类的评估方法>分类的评估方法</h3><h4 id=分类评估报告-api>分类评估报告 API</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>classification_report</span>
</span></span></code></pre></td></tr></table></div></div><p><em><strong>classification_report(y_true, y_pred, labels=[], target_names=None )</strong></em></p><ul><li><em>y_true</em>：真实目标值</li><li><em>y_pred</em>：估计器预测目标值</li><li><em>labels</em>：指定类别对应的数字</li><li><em>target_names</em>：目标类别名称</li><li><em>return</em>：每个类别精确率与召回率</li></ul><h4 id=roc-曲线与-auc-指标>ROC 曲线与 AUC 指标</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>roc_auc_score</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>AUC 只能用来评价二分类</li><li>AUC 非常适合评价样本不平衡中的分类器性能</li></ul><h5 id=tpr-与-fpr>TPR 与 FPR</h5><ul><li>TPR = TP / (TP + FN)<ul><li>所有真实类别为1的样本中，预测类别为1的比例</li><li>即为召回率（查全率）</li></ul></li><li>FPR = FP / (FP + FN)<ul><li>所有真实类别为0的样本中，预测类别为1的比例</li></ul></li></ul><h5 id=roc-曲线>ROC 曲线</h5><ul><li>ROC 曲线的横轴就是 FPRate，纵轴就是 TPRate，当二者相等时，表示的意义则是：对于不论真实类别是 1 还是 0 的样本，分类器预测为 1 的概率是相等的，此时 AUC 为 0.5</li></ul><img src=https://trou.oss-cn-shanghai.aliyuncs.com/img/ROC.png alt=ROC style=zoom:67%><h5 id=auc-指标>AUC 指标</h5><ul><li>AUC 的概率意义是随机取一对正负样本，正样本得分大于负样本的概率</li><li>AUC 的最小值为 0.5，最大值为 1，取值越高越好</li><li>AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。</li><li>0.5&lt;AUC&lt;1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。</li></ul><blockquote><p><strong>最终AUC的范围在 [0.5, 1] 之间，并且越接近1越好</strong></p></blockquote><h5 id=api-9>API</h5><p><em><strong>roc_auc_score(y_true, y_score)</strong></em></p><ul><li>计算 ROC 曲线面积，即 AUC 值</li><li><em>y_true</em>：每个样本的真实类别，必须为 0 (反例), 1 (正例)标记</li><li><em>y_score</em>：每个样本预测的概率值</li></ul><h3 id=模型保存和加载>模型保存和加载</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>joblib</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>保存：joblib.dump( estimator, &lsquo;XXX.pkl&rsquo; )</li><li>加载：estimator = joblib.load( &lsquo;XXX.pkl&rsquo; )</li></ul><p>例</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>joblib</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>store_model</span><span class=p>(</span><span class=n>estimator</span><span class=p>,</span> <span class=n>name</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>joblib</span><span class=o>.</span><span class=n>dump</span><span class=p>(</span><span class=n>estimator</span><span class=p>,</span> <span class=s2>&#34;../../models/&#34;</span><span class=o>+</span><span class=n>name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=s2>&#34;SUCCESS&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>load_model</span><span class=p>(</span><span class=n>name</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>joblib</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>&#34;../../models/&#34;</span><span class=o>+</span><span class=n>name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>model</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=分类>分类</h2><h3 id=knn-算法>KNN 算法</h3><blockquote><p>根据邻居，判断类别</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.neighbors</span> <span class=kn>import</span> <span class=n>KNeighborsClassifier</span>
</span></span></code></pre></td></tr></table></div></div><p>（K Nearest Neighbor）即K - 近邻算法</p><p>如果一个样本在特征空间中的 <strong>k 个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别</strong>，则该样本也属于这个类别。</p><ul><li>优点：<ul><li>简单，易于理解，易于实现，无需训练</li></ul></li><li>缺点：<ul><li>懒惰算法，对测试样本分类时的计算量大，内存开销大</li><li>必须指定 K 值，K 值选择不当则分类精度不能保证</li></ul></li><li>使用场景：小数据场景，几千～几万样本，具体场景具体业务去测试</li></ul><h4 id=api-10>API</h4><p><em><strong>KNeighborsClassifier(n_neighbors=5,algorithm=&lsquo;auto&rsquo;)</strong></em></p><ul><li>n_neighbors：int，可选（默认= 5），使用的邻居数</li><li>algorithm：<em>{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}</em>，可选用于计算最近邻居的算法，不同实现方式影响效率<ul><li><em>‘ball_tree’</em> 将会使用 BallTree</li><li><em>‘kd_tree’</em> 将使用 KDTree</li><li><em>‘auto’</em> 将尝试根据传递给fit方法的值来决定最合适的算法</li></ul></li></ul><h4 id=例-8>例</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.neighbors</span> <span class=kn>import</span> <span class=n>KNeighborsClassifier</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>StandardScaler</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span><span class=p>,</span> <span class=n>GridSearchCV</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>datasets</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 数据集导入</span>
</span></span><span class=line><span class=cl><span class=n>iris</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>load_iris</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 数据集划分</span>
</span></span><span class=line><span class=cl><span class=n>x_train</span><span class=p>,</span> <span class=n>x_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>iris</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>iris</span><span class=o>.</span><span class=n>target</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>43</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 标准化</span>
</span></span><span class=line><span class=cl><span class=n>transfer_std</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>x_train_std</span> <span class=o>=</span> <span class=n>transfer_std</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>x_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x_test_std</span> <span class=o>=</span> <span class=n>transfer_std</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>x_test</span><span class=p>)</span>  <span class=c1># 测试集不要用fit, 因为要保持和训练集处理方式一致</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># KNN</span>
</span></span><span class=line><span class=cl><span class=n>estimator_knn</span> <span class=o>=</span> <span class=n>KNeighborsClassifier</span><span class=p>(</span><span class=n>n_neighbors</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 调优</span>
</span></span><span class=line><span class=cl><span class=n>param_dict</span> <span class=o>=</span> <span class=p>{</span><span class=s2>&#34;n_neighbors&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>7</span><span class=p>,</span> <span class=mi>9</span><span class=p>,</span> <span class=mi>11</span><span class=p>]}</span>
</span></span><span class=line><span class=cl><span class=n>estimator_knn</span> <span class=o>=</span> <span class=n>GridSearchCV</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>estimator_knn</span><span class=p>,</span> <span class=n>param_grid</span><span class=o>=</span><span class=n>param_dict</span><span class=p>,</span> <span class=n>cv</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>  <span class=c1># 10折</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 训练模型</span>
</span></span><span class=line><span class=cl><span class=n>estimator_knn</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>x_train_std</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y_pred</span> <span class=o>=</span> <span class=n>estimator_knn</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>x_test_std</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;预测值为:&#34;</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>,</span> <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>真实值为:&#34;</span><span class=p>,</span> <span class=n>y_test</span><span class=p>,</span> <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>比较结果为:&#34;</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>==</span> <span class=n>y_pred</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;准确率为：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>estimator_knn</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>x_test_std</span><span class=p>,</span> <span class=n>y_test</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;最佳参数:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>estimator_knn</span><span class=o>.</span><span class=n>best_params_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;最佳结果:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>estimator_knn</span><span class=o>.</span><span class=n>best_score_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;最佳估计器:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>estimator_knn</span><span class=o>.</span><span class=n>best_estimator_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;交叉验证结果:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>estimator_knn</span><span class=o>.</span><span class=n>cv_results_</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>预测值为</span><span class=p>:</span> <span class=p>[</span><span class=mi>0</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl> <span class=mi>2</span><span class=p>]</span> 
</span></span><span class=line><span class=cl><span class=n>真实值为</span><span class=p>:</span> <span class=p>[</span><span class=mi>0</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl> <span class=mi>2</span><span class=p>]</span> 
</span></span><span class=line><span class=cl><span class=n>比较结果为</span><span class=p>:</span> <span class=p>[</span> <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>
</span></span><span class=line><span class=cl>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>
</span></span><span class=line><span class=cl>  <span class=kc>True</span>  <span class=kc>True</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>准确率为</span><span class=err>：</span>
</span></span><span class=line><span class=cl> <span class=mf>0.9736842105263158</span>
</span></span><span class=line><span class=cl><span class=n>最佳参数</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s1>&#39;n_neighbors&#39;</span><span class=p>:</span> <span class=mi>1</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>最佳结果</span><span class=p>:</span>
</span></span><span class=line><span class=cl> <span class=mf>0.9469696969696969</span>
</span></span><span class=line><span class=cl><span class=n>最佳估计器</span><span class=p>:</span>
</span></span><span class=line><span class=cl> <span class=n>KNeighborsClassifier</span><span class=p>(</span><span class=n>n_neighbors</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>交叉验证结果</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s1>&#39;mean_fit_time&#39;</span><span class=p>:</span> <span class=n>array</span><span class=p>([</span><span class=mf>0.00029657</span><span class=p>,</span> <span class=mf>0.00039995</span><span class=p>,</span> <span class=mf>0.00039968</span><span class=p>,</span> <span class=mf>0.00049977</span><span class=p>,</span> <span class=mf>0.00029998</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>0.00040131</span><span class=p>]),</span> <span class=s1>&#39;std_fit_time&#39;</span><span class=p>:</span> <span class=n>array</span><span class=p>([</span><span class=mf>0.00045309</span><span class=p>,</span> <span class=mf>0.00048983</span><span class=p>,</span> <span class=mf>0.00048951</span><span class=p>,</span> <span class=mf>0.00049977</span><span class=p>,</span> <span class=mf>0.00045822</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>0.0004915</span> <span class=p>]),</span> <span class=s1>&#39;mean_score_time&#39;</span><span class=p>:</span> <span class=n>array</span><span class=p>([</span><span class=mf>0.00089977</span><span class=p>,</span> <span class=mf>0.00080023</span><span class=p>,</span> <span class=mf>0.00110025</span><span class=p>,</span> <span class=mf>0.00080018</span><span class=p>,</span> <span class=mf>0.00079889</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>0.00080283</span><span class=p>]),</span> <span class=s1>&#39;std_score_time&#39;</span><span class=p>:</span> <span class=n>array</span><span class=p>([</span><span class=mf>0.00029992</span><span class=p>,</span> <span class=mf>0.0004004</span> <span class=p>,</span> <span class=mf>0.00030082</span><span class=p>,</span> <span class=mf>0.00040009</span><span class=p>,</span> <span class=mf>0.00039965</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>0.00040154</span><span class=p>]),</span> <span class=s1>&#39;param_n_neighbors&#39;</span><span class=p>:</span> <span class=n>masked_array</span><span class=p>(</span><span class=n>data</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>7</span><span class=p>,</span> <span class=mi>9</span><span class=p>,</span> <span class=mi>11</span><span class=p>],</span>
</span></span><span class=line><span class=cl>             <span class=n>mask</span><span class=o>=</span><span class=p>[</span><span class=kc>False</span><span class=p>,</span> <span class=kc>False</span><span class=p>,</span> <span class=kc>False</span><span class=p>,</span> <span class=kc>False</span><span class=p>,</span> <span class=kc>False</span><span class=p>,</span> <span class=kc>False</span><span class=p>],</span>
</span></span><span class=line><span class=cl>       <span class=n>fill_value</span><span class=o>=</span><span class=s1>&#39;?&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>dtype</span><span class=o>=</span><span class=nb>object</span><span class=p>),</span> <span class=s1>&#39;params&#39;</span><span class=p>:</span> <span class=p>[{</span><span class=s1>&#39;n_neighbors&#39;</span><span class=p>:</span> <span class=mi>1</span><span class=p>},</span> <span class=p>{</span><span class=s1>&#39;n_neighbors&#39;</span><span class=p>:</span> <span class=mi>3</span><span class=p>},</span> <span class=p>{</span><span class=s1>&#39;n_neighbors&#39;</span><span class=p>:</span> <span class=mi>5</span><span class=p>},</span> <span class=p>{</span><span class=s1>&#39;n_neighbors&#39;</span><span class=p>:</span> <span class=mi>7</span><span class=p>},</span> <span class=p>{</span><span class=s1>&#39;n_neighbors&#39;</span><span class=p>:</span> <span class=mi>9</span><span class=p>},</span> <span class=p>{</span><span class=s1>&#39;n_neighbors&#39;</span><span class=p>:</span> <span class=mi>11</span><span class=p>}],</span> <span class=s1>&#39;split0_test_score&#39;</span><span class=p>:</span> <span class=n>array</span><span class=p>([</span><span class=mf>0.91666667</span><span class=p>,</span> <span class=mf>0.91666667</span><span class=p>,</span> <span class=mf>0.91666667</span><span class=p>,</span> <span class=mf>0.91666667</span><span class=p>,</span> <span class=mf>0.91666667</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>0.91666667</span><span class=p>]),</span> <span class=s1>&#39;split1_test_score&#39;</span><span class=p>:</span> <span class=n>array</span><span class=p>([</span><span class=mf>0.91666667</span><span class=p>,</span> <span class=mf>0.91666667</span><span class=p>,</span> <span class=mf>0.83333333</span><span class=p>,</span> <span class=mf>0.91666667</span><span class=p>,</span> <span class=mf>0.91666667</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>0.91666667</span><span class=p>]),</span> <span class=s1>&#39;split2_test_score&#39;</span><span class=p>:</span> <span class=n>array</span><span class=p>([</span><span class=mf>0.90909091</span><span class=p>,</span> <span class=mf>0.90909091</span><span class=p>,</span> <span class=mf>0.90909091</span><span class=p>,</span> <span class=mf>0.90909091</span><span class=p>,</span> <span class=mf>0.90909091</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>1.</span>        <span class=p>]),</span> <span class=s1>&#39;split3_test_score&#39;</span><span class=p>:</span> <span class=n>array</span><span class=p>([</span><span class=mf>0.90909091</span><span class=p>,</span> <span class=mf>0.90909091</span><span class=p>,</span> <span class=mf>0.90909091</span><span class=p>,</span> <span class=mf>0.90909091</span><span class=p>,</span> <span class=mf>0.90909091</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>0.81818182</span><span class=p>]),</span> <span class=s1>&#39;split4_test_score&#39;</span><span class=p>:</span> <span class=n>array</span><span class=p>([</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>]),</span> <span class=s1>&#39;split5_test_score&#39;</span><span class=p>:</span> <span class=n>array</span><span class=p>([</span><span class=mf>0.90909091</span><span class=p>,</span> <span class=mf>0.90909091</span><span class=p>,</span> <span class=mf>1.</span>        <span class=p>,</span> <span class=mf>1.</span>        <span class=p>,</span> <span class=mf>1.</span>        <span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>1.</span>        <span class=p>]),</span> <span class=s1>&#39;split6_test_score&#39;</span><span class=p>:</span> <span class=n>array</span><span class=p>([</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>]),</span> <span class=s1>&#39;split7_test_score&#39;</span><span class=p>:</span> <span class=n>array</span><span class=p>([</span><span class=mf>0.90909091</span><span class=p>,</span> <span class=mf>0.81818182</span><span class=p>,</span> <span class=mf>0.81818182</span><span class=p>,</span> <span class=mf>0.81818182</span><span class=p>,</span> <span class=mf>0.81818182</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>0.81818182</span><span class=p>]),</span> <span class=s1>&#39;split8_test_score&#39;</span><span class=p>:</span> <span class=n>array</span><span class=p>([</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>]),</span> <span class=s1>&#39;split9_test_score&#39;</span><span class=p>:</span> <span class=n>array</span><span class=p>([</span><span class=mf>1.</span>        <span class=p>,</span> <span class=mf>0.90909091</span><span class=p>,</span> <span class=mf>1.</span>        <span class=p>,</span> <span class=mf>1.</span>        <span class=p>,</span> <span class=mf>1.</span>        <span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>0.90909091</span><span class=p>]),</span> <span class=s1>&#39;mean_test_score&#39;</span><span class=p>:</span> <span class=n>array</span><span class=p>([</span><span class=mf>0.9469697</span> <span class=p>,</span> <span class=mf>0.92878788</span><span class=p>,</span> <span class=mf>0.93863636</span><span class=p>,</span> <span class=mf>0.9469697</span> <span class=p>,</span> <span class=mf>0.9469697</span> <span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>0.93787879</span><span class=p>]),</span> <span class=s1>&#39;std_test_score&#39;</span><span class=p>:</span> <span class=n>array</span><span class=p>([</span><span class=mf>0.04338734</span><span class=p>,</span> <span class=mf>0.05412294</span><span class=p>,</span> <span class=mf>0.06830376</span><span class=p>,</span> <span class=mf>0.05945884</span><span class=p>,</span> <span class=mf>0.05945884</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=mf>0.07048305</span><span class=p>]),</span> <span class=s1>&#39;rank_test_score&#39;</span><span class=p>:</span> <span class=n>array</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>6</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>5</span><span class=p>])}</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=朴素贝叶斯算法>朴素贝叶斯算法</h3><p>（Naive Bayes）</p><blockquote><p>相互独立的特征 + 贝叶斯公式</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.naive_bayes</span> <span class=kn>import</span> <span class=n>MultinomialNB</span>
</span></span></code></pre></td></tr></table></div></div><p>朴素：特征与特征之间是相互独立的</p><p>朴素贝叶斯算法经常用于文本分类, 因为文章转换成机器学习算法识别的数据是以单词为特征的</p><ul><li>优点：<ul><li>朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。</li><li>对缺失数据不太敏感，算法也比较简单，常用于文本分类。</li><li>分类准确度高，速度快</li></ul></li><li>缺点：<ul><li>由于使用了样本属性独立性的假设，所以如果特征属性有关联时其效果不好</li></ul></li></ul><h4 id=原理-1>原理</h4><h5 id=贝叶斯公式>贝叶斯公式</h5><p>以文本分类为例
$$
P(C|F_1,F_2,\ldots)=\cfrac{P(F_1,F_2,\ldots|C)P(C)}{P(F_1,F_2,\ldots)}
$$</p><ul><li>$P(C)$：每个文档类别的概率(某文档类别数／总文档数量)</li><li>$P(W│C)$：给定类别下特征（被预测文档中出现的词）的概率<ul><li>$W$ 为给定文档的特征值（频数统计）</li><li>计算方法：$P(F_1│C)=N_i/N$ （训练文档中去计算）<ul><li>$N_i$：该 $F_1$ 词在 $C$ 类别所有文档中出现的次数</li><li>$N$：所属类别 $C$ 下的文档的文本总和</li></ul></li></ul></li><li>$P(F_1,F_2,\ldots)$ 预测文档中每个词的概率</li></ul><h5 id=拉普拉斯平滑系数>拉普拉斯平滑系数</h5><p>目的：防止计算出的分类概率为0
$$
P(F_1|C)=\cfrac{N_i+\alpha}{N+\alpha m}
$$</p><ul><li>$\alpha$：预先指定的系数，默认为 1</li><li>$m$：训练文档中特征词的种类数</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 因为样本数量不够，会出现特征词不在一类文本中出现的情况</span>
</span></span><span class=line><span class=cl><span class=n>P</span><span class=p>(</span><span class=n>娱乐</span><span class=o>|</span><span class=n>影院</span><span class=p>,</span><span class=n>支付宝</span><span class=p>,</span><span class=n>云计算</span><span class=p>)</span> <span class=o>=</span> <span class=n>𝑃</span><span class=p>(</span><span class=n>影院</span><span class=p>,</span><span class=n>支付宝</span><span class=p>,</span><span class=n>云计算</span><span class=o>|</span><span class=n>娱乐</span><span class=p>)</span><span class=err>∗</span><span class=n>P</span><span class=p>(</span><span class=n>娱乐</span><span class=p>)</span><span class=o>=</span><span class=p>(</span><span class=mi>56</span><span class=o>/</span><span class=mi>121</span><span class=p>)</span><span class=err>∗</span><span class=p>(</span><span class=mi>15</span><span class=o>/</span><span class=mi>121</span><span class=p>)</span><span class=err>∗</span><span class=p>(</span><span class=mi>0</span><span class=o>/</span><span class=mi>121</span><span class=p>)</span><span class=err>∗</span><span class=p>(</span><span class=mi>60</span><span class=o>/</span><span class=mi>90</span><span class=p>)</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=c1># 此时需要实用到拉普拉斯平滑系数</span>
</span></span><span class=line><span class=cl><span class=n>P</span><span class=p>(</span><span class=n>娱乐</span><span class=o>|</span><span class=n>影院</span><span class=p>,</span><span class=n>支付宝</span><span class=p>,</span><span class=n>云计算</span><span class=p>)</span> <span class=o>=</span><span class=n>P</span><span class=p>(</span><span class=n>影院</span><span class=p>,</span><span class=n>支付宝</span><span class=p>,</span><span class=n>云计算</span><span class=o>|</span><span class=n>娱乐</span><span class=p>)</span><span class=n>P</span><span class=p>(</span><span class=n>娱乐</span><span class=p>)</span><span class=o>=</span><span class=p>(</span><span class=mi>56</span><span class=o>+</span><span class=mi>1</span><span class=o>/</span><span class=mi>121</span><span class=o>+</span><span class=mi>4</span><span class=p>)(</span><span class=mi>15</span><span class=o>+</span><span class=mi>1</span><span class=o>/</span><span class=mi>121</span><span class=o>+</span><span class=mi>4</span><span class=p>)(</span><span class=mi>0</span><span class=o>+</span><span class=mi>1</span><span class=o>/</span><span class=mi>121</span><span class=o>+</span><span class=mi>1</span><span class=o>*</span><span class=mi>4</span><span class=p>)(</span><span class=mi>60</span><span class=o>/</span><span class=mi>90</span><span class=p>)</span> <span class=o>=</span> <span class=mf>0.00002</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=api-11>API</h4><p><em><strong>MultinomialNB(alpha = 1.0)</strong></em></p><ul><li><em>alpha</em>：拉普拉斯平滑系数</li></ul><h4 id=例20类新闻分类>例：20类新闻分类</h4><p>分析</p><ul><li>划分数据集</li><li>tfidf 进行的特征抽取</li><li>朴素贝叶斯预测</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>fetch_20newsgroups</span><span class=p>,</span> <span class=n>load_files</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.feature_extraction.text</span> <span class=kn>import</span> <span class=n>TfidfVectorizer</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.naive_bayes</span> <span class=kn>import</span> <span class=n>MultinomialNB</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>fetch_20newsgroups</span><span class=p>(</span><span class=n>subset</span><span class=o>=</span><span class=s2>&#34;all&#34;</span><span class=p>)</span> 
</span></span><span class=line><span class=cl><span class=n>x_train</span><span class=p>,</span> <span class=n>x_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> \
</span></span><span class=line><span class=cl>        <span class=n>train_test_split</span><span class=p>(</span><span class=n>data</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>data</span><span class=o>.</span><span class=n>target</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>22</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 文本分类</span>
</span></span><span class=line><span class=cl><span class=n>transfer</span> <span class=o>=</span> <span class=n>TfidfVectorizer</span><span class=p>()</span>  
</span></span><span class=line><span class=cl><span class=n>x_train</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>x_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x_test</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>x_test</span><span class=p>)</span>  
</span></span><span class=line><span class=cl><span class=c1># 朴素贝叶斯</span>
</span></span><span class=line><span class=cl><span class=n>estimator</span> <span class=o>=</span> <span class=n>MultinomialNB</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>estimator</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>x_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y_predict</span> <span class=o>=</span> <span class=n>estimator</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>x_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;预测值为:&#34;</span><span class=p>,</span> <span class=n>y_predict</span><span class=p>,</span> <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>真实值为:&#34;</span><span class=p>,</span> <span class=n>y_test</span><span class=p>,</span> <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>比较结果为:&#34;</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>==</span> <span class=n>y_predict</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>score</span> <span class=o>=</span> <span class=n>estimator</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>x_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;准确率为: &#34;</span><span class=p>,</span> <span class=n>score</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>预测值为</span><span class=p>:</span> <span class=p>[</span><span class=mi>15</span> <span class=mi>13</span> <span class=mi>16</span> <span class=o>...</span> <span class=mi>13</span>  <span class=mi>2</span> <span class=mi>13</span><span class=p>]</span> 
</span></span><span class=line><span class=cl><span class=n>真实值为</span><span class=p>:</span> <span class=p>[</span><span class=mi>15</span> <span class=mi>13</span> <span class=mi>16</span> <span class=o>...</span> <span class=mi>13</span>  <span class=mi>2</span> <span class=mi>13</span><span class=p>]</span> 
</span></span><span class=line><span class=cl><span class=n>比较结果为</span><span class=p>:</span> <span class=p>[</span> <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span> <span class=o>...</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>准确率为</span><span class=p>:</span>  <span class=mf>0.8511936339522547</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=决策树>决策树</h3><p>（Decision Tree）</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.tree</span> <span class=kn>import</span> <span class=n>DecisionTreeClassifier</span>
</span></span></code></pre></td></tr></table></div></div><blockquote><p>if - else 根据特征的信息熵筛选</p></blockquote><h4 id=原理-2>原理</h4><h5 id=信息熵>信息熵</h5><p>$$
H(X)=-\sum\limits_{i=1}^n P(x_i)log_bP(x_i)
$$</p><h5 id=条件信息熵>条件信息熵</h5><p>$$
H(D|A)=\sum\limits_{i=1}^n \cfrac{|D_i|}{|D|}H(D_i)
$$</p><h5 id=信息增益>信息增益</h5><p>决策树的划分依据之一</p><p>特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D,A)$,定义为集合 D 的信息熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的信息条件熵 $H(D|A)$ 之差
$$
g(D,A)=H(D)=H(D|A)
$$</p><h5 id=三种算法实现>三种算法实现</h5><ul><li>ID3<ul><li>信息增益 最大的准则</li></ul></li><li>C4.5<ul><li>信息增益比 最大的准则</li></ul></li><li>CART<ul><li>分类树: 基尼系数 最小的准则 在sklearn中可以选择划分的默认原则</li><li>优势：划分更加细致（从后面例子的树显示来理解）</li></ul></li></ul><h4 id=api-12>API</h4><p><em><strong>DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)</strong></em></p><p>决策树分类器</p><ul><li>criterion：默认是 ’gini’ 系数，也可以选择信息增益的熵 ’entropy’</li><li>max_depth：树的深度大小</li><li>random_state：随机数种子</li></ul><h4 id=保存树的结构>保存树的结构</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.tree</span> <span class=kn>import</span> <span class=n>export_graphviz</span>
</span></span></code></pre></td></tr></table></div></div><p><em><strong>export_graphviz()</strong></em></p><p>该函数能够导出 DOT 格式</p><ul><li>tree.export_graphviz(estimator, out_file=path, feature_names)</li></ul><h4 id=例-9>例</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>datasets</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.tree</span> <span class=kn>import</span> <span class=n>DecisionTreeClassifier</span><span class=p>,</span> <span class=n>export_graphviz</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>iris</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>load_iris</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>x_train</span><span class=p>,</span> <span class=n>x_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>iris</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>iris</span><span class=o>.</span><span class=n>target</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>22</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 决策树训练</span>
</span></span><span class=line><span class=cl><span class=n>estimator</span> <span class=o>=</span> <span class=n>DecisionTreeClassifier</span><span class=p>(</span><span class=n>criterion</span><span class=o>=</span><span class=s2>&#34;entropy&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>estimator</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>x_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 生成树文件</span>
</span></span><span class=line><span class=cl><span class=n>export_graphviz</span><span class=p>(</span><span class=n>estimator</span><span class=p>,</span> <span class=n>out_file</span><span class=o>=</span><span class=s2>&#34;tree.dot&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>feature_names</span><span class=o>=</span><span class=n>iris</span><span class=o>.</span><span class=n>feature_names</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>y_pred</span> <span class=o>=</span> <span class=n>estimator</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>x_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;预测值为:&#34;</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>,</span> <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>真实值为:&#34;</span><span class=p>,</span> <span class=n>y_test</span><span class=p>,</span> <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>比较结果为:&#34;</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>==</span> <span class=n>y_pred</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>score</span> <span class=o>=</span> <span class=n>estimator</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>x_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;准确率为: &#34;</span><span class=p>,</span> <span class=n>score</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>预测值为</span><span class=p>:</span> <span class=p>[</span><span class=mi>0</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl> <span class=mi>0</span><span class=p>]</span> 
</span></span><span class=line><span class=cl><span class=n>真实值为</span><span class=p>:</span> <span class=p>[</span><span class=mi>0</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl> <span class=mi>0</span><span class=p>]</span> 
</span></span><span class=line><span class=cl><span class=n>比较结果为</span><span class=p>:</span> <span class=p>[</span> <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span> <span class=kc>False</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>
</span></span><span class=line><span class=cl>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span> <span class=kc>False</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>
</span></span><span class=line><span class=cl>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span> <span class=kc>False</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>
</span></span><span class=line><span class=cl>  <span class=kc>True</span>  <span class=kc>True</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>准确率为</span><span class=p>:</span>  <span class=mf>0.9210526315789473</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=随机森林>随机森林</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.ensemble</span> <span class=kn>import</span> <span class=n>RandomForestClassifier</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>随机森林是一个包含多个决策树的分类器</strong>，并且其输出的类别是由个别树输出的类别的众数而定。</p><h4 id=原理-3>原理</h4><p>学习算法根据下列算法而建造每棵树</p><ul><li>用 N 来表示 样本个数，M 表示特征数目。<ul><li>1、有放回地抽样（bootstrap），一次随机选出一个样本，重复 N 次</li><li>2、随机选出 m 个特征, m &#171;M，建立决策树</li></ul></li></ul><h4 id=api-13>API</h4><p><em><strong>RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2)</strong></em></p><ul><li><p>随机森林分类器</p></li><li><p><em>n_estimators</em>：integer，optional（default = 10）森林里的树木数量 120,200,300,500,800,1200</p></li><li><p><em>criteria</em>：string，可选（default =“gini”）分割特征的测量方法</p></li><li><p><em>max_depth</em>：integer 或 None，可选（默认=无）树的最大深度 5,8,15,25,30</p></li><li><p><em>max_features=&ldquo;auto”</em>：每个决策树的最大特征数量</p><ul><li>If &ldquo;auto&rdquo;, then <code>max_features=sqrt(n_features)</code>.</li><li>If &ldquo;sqrt&rdquo;, then <code>max_features=sqrt(n_features)</code> (same as &ldquo;auto&rdquo;).</li><li>If &ldquo;log2&rdquo;, then <code>max_features=log2(n_features)</code>.</li><li>If None, then <code>max_features=n_features</code>.</li></ul></li><li><p><em>bootstrap</em>：boolean，optional（default = True）是否在构建树时使用放回抽样</p></li><li><p><em>min_samples_split</em>：节点划分最少样本数</p></li><li><p><em>min_samples_leaf</em>：叶子节点的最小样本数</p></li><li><p>超参数：n_estimator, max_depth, min_samples_split,min_samples_leaf</p></li></ul><h5 id=例-10>例</h5><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>datasets</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.ensemble</span> <span class=kn>import</span> <span class=n>RandomForestClassifier</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>GridSearchCV</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>iris</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>load_iris</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># 数据集划分</span>
</span></span><span class=line><span class=cl><span class=n>x_train</span><span class=p>,</span> <span class=n>x_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>iris</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>iris</span><span class=o>.</span><span class=n>target</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>22</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># RandomForest</span>
</span></span><span class=line><span class=cl><span class=n>estimator_rf</span> <span class=o>=</span> <span class=n>RandomForestClassifier</span><span class=p>()</span> 
</span></span><span class=line><span class=cl><span class=c1># 超参调优</span>
</span></span><span class=line><span class=cl><span class=n>param_dict</span> <span class=o>=</span> <span class=p>{</span><span class=s2>&#34;n_estimators&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>120</span><span class=p>,</span><span class=mi>200</span><span class=p>,</span><span class=mi>300</span><span class=p>,</span><span class=mi>500</span><span class=p>,</span><span class=mi>800</span><span class=p>,</span><span class=mi>1200</span><span class=p>],</span> <span class=s2>&#34;max_depth&#34;</span><span class=p>:</span> <span class=p>[</span><span class=mi>5</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>15</span><span class=p>,</span> <span class=mi>25</span><span class=p>,</span> <span class=mi>30</span><span class=p>]}</span>
</span></span><span class=line><span class=cl><span class=n>estimator_rf</span> <span class=o>=</span> <span class=n>GridSearchCV</span><span class=p>(</span><span class=n>estimator_rf</span><span class=p>,</span> <span class=n>param_grid</span><span class=o>=</span><span class=n>param_dict</span><span class=p>,</span><span class=n>cv</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 训练</span>
</span></span><span class=line><span class=cl><span class=n>estimator_rf</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>x_train</span><span class=p>,</span><span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y_pred</span> <span class=o>=</span> <span class=n>estimator_rf</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>x_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 输出结果</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;预测值为:&#34;</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>,</span> <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>真实值为:&#34;</span><span class=p>,</span> <span class=n>y_test</span><span class=p>,</span> <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>比较结果为:&#34;</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>==</span> <span class=n>y_pred</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;准确率为：</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>estimator_rf</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>x_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;最佳参数:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>estimator_rf</span><span class=o>.</span><span class=n>best_params_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;最佳结果:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>estimator_rf</span><span class=o>.</span><span class=n>best_score_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;最佳估计器:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>estimator_rf</span><span class=o>.</span><span class=n>best_estimator_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;交叉验证结果:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>estimator_rf</span><span class=o>.</span><span class=n>cv_results_</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>预测值为</span><span class=p>:</span> <span class=p>[</span><span class=mi>0</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl> <span class=mi>0</span><span class=p>]</span> 
</span></span><span class=line><span class=cl><span class=n>真实值为</span><span class=p>:</span> <span class=p>[</span><span class=mi>0</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl> <span class=mi>0</span><span class=p>]</span> 
</span></span><span class=line><span class=cl><span class=n>比较结果为</span><span class=p>:</span> <span class=p>[</span> <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span> <span class=kc>False</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>
</span></span><span class=line><span class=cl>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span> <span class=kc>False</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>
</span></span><span class=line><span class=cl>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span> <span class=kc>False</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>  <span class=kc>True</span>
</span></span><span class=line><span class=cl>  <span class=kc>True</span>  <span class=kc>True</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>准确率为</span><span class=err>：</span>
</span></span><span class=line><span class=cl> <span class=mf>0.9210526315789473</span>
</span></span><span class=line><span class=cl><span class=n>最佳参数</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s1>&#39;max_depth&#39;</span><span class=p>:</span> <span class=mi>5</span><span class=p>,</span> <span class=s1>&#39;n_estimators&#39;</span><span class=p>:</span> <span class=mi>120</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>最佳结果</span><span class=p>:</span>
</span></span><span class=line><span class=cl> <span class=mf>0.9553571428571429</span>
</span></span><span class=line><span class=cl><span class=n>最佳估计器</span><span class=p>:</span>
</span></span><span class=line><span class=cl> <span class=n>RandomForestClassifier</span><span class=p>(</span><span class=n>bootstrap</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>class_weight</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>criterion</span><span class=o>=</span><span class=s1>&#39;gini&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>max_depth</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>max_features</span><span class=o>=</span><span class=s1>&#39;auto&#39;</span><span class=p>,</span> <span class=n>max_leaf_nodes</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>min_impurity_decrease</span><span class=o>=</span><span class=mf>0.0</span><span class=p>,</span> <span class=n>min_impurity_split</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>min_samples_leaf</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>min_samples_split</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>min_weight_fraction_leaf</span><span class=o>=</span><span class=mf>0.0</span><span class=p>,</span> <span class=n>n_estimators</span><span class=o>=</span><span class=mi>120</span><span class=p>,</span> <span class=n>n_jobs</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>oob_score</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>warm_start</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>交叉验证结果</span><span class=p>:</span>
</span></span><span class=line><span class=cl> <span class=p>{</span><span class=s1>&#39;mean_fit_time&#39;</span><span class=p>:</span> <span class=n>array</span><span class=p>([</span><span class=mf>0.32009826</span><span class=p>,</span> <span class=mf>0.57197742</span><span class=p>,</span> <span class=o>.......</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>])}</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=逻辑回归>逻辑回归</h3><p>（Logistic Regression）</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>LogisticRegression</span>
</span></span></code></pre></td></tr></table></div></div><p>逻辑回归是机器学习中的一种分类模型，逻辑回归是一种分类算法，虽然名字中带有回归，但是它与回归之间有一定的联系。</p><h4 id=原理-4>原理</h4><h5 id=输入>输入</h5><p>逻辑回归的输入就是一个线性回归的结果。
$$
h(w)=w_1x_1+w_2x_2+w_3x_3\ldots+b
$$</p><h5 id=激活函数>激活函数</h5><p>sigmoid 函数
$$
g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}
$$
分析</p><ul><li>回归的结果输入到 sigmoid 函数当中</li><li>输出结果：[0, 1] 区间中的一个概率值，默认为 0.5 为阈值</li></ul><blockquote><p>逻辑回归最终的分类是通过属于某个类别的概率值来判断是否属于某个类别，并且这个类别默认标记为 1 (正例),另外的一个类别会标记为 0 (反例)。（方便损失计算）</p></blockquote><img src=https://trou.oss-cn-shanghai.aliyuncs.com/img/逻辑回归运算过程.png alt=逻辑回归运算过程 style=zoom:67%><h5 id=损失函数>损失函数</h5><p>逻辑回归的损失，称之为<strong>对数似然损失</strong>
$$
cost(h_\theta(x),y)=\sum\limits_{i=1}^m-y_ilog(h_\theta(x))-(1-y_i)log(1-h_\theta(x))
$$
<img src=https://trou.oss-cn-shanghai.aliyuncs.com/img/损失计算过程.png alt=损失计算过程 style=zoom:67%></p><h5 id=优化>优化</h5><p>同样使用梯度下降优化算法，去减少损失函数的值。这样去更新逻辑回归前面对应算法的权重参数，提升原本属于 1 类别的概率，降低原本是 0 类别的概率。</p><h4 id=api-14>API</h4><p><em><strong>LogisticRegression(solver=&lsquo;liblinear&rsquo;, penalty=‘l2’, C = 1.0)</strong></em></p><ul><li><em>solver</em>：优化求解方式（默认开源的 liblinear 库实现，内部使用了坐标轴下降法来迭代优化损失函数）<ul><li><em>sag</em>：根据数据集自动选择，随机平均梯度下降</li></ul></li><li><em>penalty</em>：正则化的种类</li><li><em>C</em>：正则化力度</li></ul><p>LogisticRegression 方法相当于 <code>SGDClassifier(loss="log", penalty=" ")</code>, SGDClassifier 实现了一个普通的随机梯度下降学习，也支持平均随机梯度下降法（ASGD），可以通过设置 average=True。而使用 LogisticRegression (实现了 SAG)</p><h4 id=例-11>例</h4><p>案例：癌症分类预测-良／恶性乳腺癌肿瘤预测</p><blockquote><p>数据描述</p><p>（1）699条样本，共11列数据，第一列用语检索的id，后9列分别是与肿瘤</p><p>相关的医学特征，最后一列表示肿瘤类型的数值。</p><p>（2）包含16个缺失值，用 ”?” 标出。</p></blockquote><p>分析</p><ul><li>缺失值处理</li><li>标准化处理</li><li>逻辑回归预测</li></ul><h5 id=初始化>初始化</h5><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>LogisticRegression</span><span class=p>,</span> <span class=n>SGDClassifier</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>StandardScaler</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>classification_report</span><span class=p>,</span> <span class=n>roc_auc_score</span><span class=p>,</span> <span class=n>roc_curve</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 文件读取</span>
</span></span><span class=line><span class=cl><span class=n>column_name</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;Sample code number&#39;</span><span class=p>,</span> <span class=s1>&#39;Clump Thickness&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>               <span class=s1>&#39;Uniformity of Cell Size&#39;</span><span class=p>,</span> <span class=s1>&#39;Uniformity of Cell Shape&#39;</span><span class=p>,</span> <span class=s1>&#39;Marginal Adhesion&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>               <span class=s1>&#39;Single Epithelial Cell Size&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>               <span class=s1>&#39;Bare Nuclei&#39;</span><span class=p>,</span> <span class=s1>&#39;Bland Chromatin&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>               <span class=s1>&#39;Normal Nucleoli&#39;</span><span class=p>,</span> <span class=s1>&#39;Mitoses&#39;</span><span class=p>,</span> <span class=s1>&#39;Class&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>original_data</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;../Data/breast-cancer-wisconsin.data&#39;</span><span class=p>,</span><span class=n>names</span><span class=o>=</span><span class=n>column_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>original_data</span><span class=o>.</span><span class=n>head</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># 缺失值处理</span>
</span></span><span class=line><span class=cl><span class=c1># 第一步先替换 ? 为 nan</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>original_data</span><span class=o>.</span><span class=n>replace</span><span class=p>(</span><span class=n>to_replace</span><span class=o>=</span><span class=s2>&#34;?&#34;</span><span class=p>,</span> <span class=n>value</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>nan</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>data</span><span class=o>.</span><span class=n>dropna</span><span class=p>(</span><span class=n>inplace</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;检测是否还有缺失值(全为false表示没有缺失值)&#34;</span><span class=p>)</span>  <span class=c1># 检测是否还有缺失值</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>data</span><span class=o>.</span><span class=n>isnull</span><span class=p>()</span><span class=o>.</span><span class=n>any</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=c1># 第三步 筛选特征值和目标值</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>  <span class=c1># 表示每一行数据都要, 从第一列到倒数第二列的column字段也要</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>data</span><span class=p>[</span><span class=s2>&#34;Class&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>x_train</span><span class=p>,</span> <span class=n>x_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>检测是否还有缺失值</span><span class=p>(</span><span class=n>全为false表示没有缺失值</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>Sample</span> <span class=n>code</span> <span class=n>number</span>             <span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=n>Clump</span> <span class=n>Thickness</span>                <span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=n>Uniformity</span> <span class=n>of</span> <span class=n>Cell</span> <span class=n>Size</span>        <span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=n>Uniformity</span> <span class=n>of</span> <span class=n>Cell</span> <span class=n>Shape</span>       <span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=n>Marginal</span> <span class=n>Adhesion</span>              <span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=n>Single</span> <span class=n>Epithelial</span> <span class=n>Cell</span> <span class=n>Size</span>    <span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=n>Bare</span> <span class=n>Nuclei</span>                    <span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=n>Bland</span> <span class=n>Chromatin</span>                <span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=n>Normal</span> <span class=n>Nucleoli</span>                <span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=n>Mitoses</span>                        <span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=n>Class</span>                          <span class=kc>False</span>
</span></span></code></pre></td></tr></table></div></div><h5 id=训练>训练</h5><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 第四步: 开始特征工程</span>
</span></span><span class=line><span class=cl><span class=n>transfer</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>x_train</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>x_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x_test</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>x_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 第五步, 预估器流程</span>
</span></span><span class=line><span class=cl><span class=n>estimator</span> <span class=o>=</span> <span class=n>LogisticRegression</span><span class=p>()</span>  <span class=c1># 默认参数</span>
</span></span><span class=line><span class=cl><span class=n>estimator</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>x_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;逻辑回归_权重系数为: &#34;</span><span class=p>,</span> <span class=n>estimator</span><span class=o>.</span><span class=n>coef_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;逻辑回归_偏置为:&#34;</span><span class=p>,</span> <span class=n>estimator</span><span class=o>.</span><span class=n>intercept_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 第六步, 模型评估</span>
</span></span><span class=line><span class=cl><span class=n>y_predict</span> <span class=o>=</span> <span class=n>estimator</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>x_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;逻辑回归_预测结果&#34;</span><span class=p>,</span> <span class=n>y_predict</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;逻辑回归_预测结果对比:&#34;</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>==</span> <span class=n>y_predict</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>score</span> <span class=o>=</span> <span class=n>estimator</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>x_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;准确率为:&#34;</span><span class=p>,</span> <span class=n>score</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 2是良性的 4是恶性的</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>但是实际上这个预测结果不是我们想要的, 以上只能说明预测的正确与否,
</span></span></span><span class=line><span class=cl><span class=s2>而事实上, 我们需要一种评估方式来显示我们对恶性breast的预测成功率, 也就是召回率
</span></span></span><span class=line><span class=cl><span class=s2>同时可以查看F1-score的稳健性
</span></span></span><span class=line><span class=cl><span class=s2>(召回率和精确率看笔记和截图)
</span></span></span><span class=line><span class=cl><span class=s2>所以下面换一种评估方法
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>逻辑回归_权重系数为</span><span class=p>:</span>  <span class=p>[[</span><span class=mf>1.20895973</span> <span class=mf>0.34430535</span> <span class=mf>0.93605533</span> <span class=mf>0.50117234</span> <span class=mf>0.22296947</span> <span class=mf>1.22295345</span>
</span></span><span class=line><span class=cl>  <span class=mf>0.81648447</span> <span class=mf>0.64096012</span> <span class=mf>0.71930684</span><span class=p>]]</span>
</span></span><span class=line><span class=cl><span class=n>逻辑回归_偏置为</span><span class=p>:</span> <span class=p>[</span><span class=o>-</span><span class=mf>0.9875554</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>逻辑回归_预测结果</span> <span class=p>[</span><span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>4</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>4</span> <span class=mi>4</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>4</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>4</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>4</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>4</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>4</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>2</span> <span class=mi>4</span> <span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>逻辑回归_预测结果对比</span><span class=p>:</span> <span class=mi>541</span>     <span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=mi>288</span>     <span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=mi>395</span>     <span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=mi>409</span>     <span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=mi>568</span>     <span class=kc>True</span>
</span></span><span class=line><span class=cl>       <span class=o>...</span>  
</span></span><span class=line><span class=cl><span class=mi>442</span>     <span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=mi>51</span>     <span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=mi>370</span>     <span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=mi>304</span>     <span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=mi>363</span>     <span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=n>Name</span><span class=p>:</span> <span class=n>Class</span><span class=p>,</span> <span class=n>Length</span><span class=p>:</span> <span class=mi>171</span><span class=p>,</span> <span class=n>dtype</span><span class=p>:</span> <span class=nb>bool</span>
</span></span><span class=line><span class=cl><span class=n>准确率为</span><span class=p>:</span> <span class=mf>0.9766081871345029</span>
</span></span></code></pre></td></tr></table></div></div><h5 id=查看精确率召回率f1-score>查看精确率，召回率，F1-score</h5><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>Score</span> <span class=o>=</span> <span class=n>classification_report</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_predict</span><span class=p>,</span> <span class=n>labels</span><span class=o>=</span><span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                              <span class=n>target_names</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;良性&#34;</span><span class=p>,</span> <span class=s2>&#34;恶性&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;查看精确率,召回率,F1-score</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>Score</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># support表示样本量</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>查看精确率</span><span class=p>,</span><span class=n>召回率</span><span class=p>,</span><span class=n>F1</span><span class=o>-</span><span class=n>score</span>
</span></span><span class=line><span class=cl>               <span class=n>precision</span>    <span class=n>recall</span>  <span class=n>f1</span><span class=o>-</span><span class=n>score</span>   <span class=n>support</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>          <span class=n>良性</span>       <span class=mf>0.98</span>      <span class=mf>0.98</span>      <span class=mf>0.98</span>       <span class=mi>114</span>
</span></span><span class=line><span class=cl>          <span class=n>恶性</span>       <span class=mf>0.96</span>      <span class=mf>0.96</span>      <span class=mf>0.96</span>        <span class=mi>57</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>accuracy</span>                           <span class=mf>0.98</span>       <span class=mi>171</span>
</span></span><span class=line><span class=cl>   <span class=n>macro</span> <span class=n>avg</span>       <span class=mf>0.97</span>      <span class=mf>0.97</span>      <span class=mf>0.97</span>       <span class=mi>171</span>
</span></span><span class=line><span class=cl><span class=n>weighted</span> <span class=n>avg</span>       <span class=mf>0.98</span>      <span class=mf>0.98</span>      <span class=mf>0.98</span>       <span class=mi>171</span>
</span></span></code></pre></td></tr></table></div></div><h5 id=查看-roc-曲线和-auc-指标>查看 ROC 曲线和 AUC 指标</h5><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>ROC曲线和AUC指标(样本分类不均衡的情况下,可以使用这种方法)
</span></span></span><span class=line><span class=cl><span class=s2>AUC = 0.5 是瞎猜模型
</span></span></span><span class=line><span class=cl><span class=s2>AUC = 1 是最好的模型
</span></span></span><span class=line><span class=cl><span class=s2>AUC &lt; 0.5 属于反向毒奶
</span></span></span><span class=line><span class=cl><span class=s2>更多的看截图
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># 需要转换为0,1表示</span>
</span></span><span class=line><span class=cl><span class=n>y_true</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>y_test</span> <span class=o>&gt;</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span>  <span class=c1># 表示大于3为1,反之为0(class值为2和4)</span>
</span></span><span class=line><span class=cl><span class=n>return_value</span> <span class=o>=</span> <span class=n>roc_auc_score</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_predict</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;ROC曲线和AUC返回值为(三角形面积)&#34;</span><span class=p>,</span> <span class=n>return_value</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>fpr</span><span class=p>,</span> <span class=n>tpr</span><span class=p>,</span> <span class=n>thresholds</span> <span class=o>=</span> <span class=n>roc_curve</span><span class=p>(</span><span class=n>y_true</span><span class=p>,</span> <span class=n>y_predict</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>fpr</span><span class=p>,</span> <span class=n>tpr</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><img src=https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20210127225009225.png alt=image-20210127225009225 style=zoom:67%><h2 id=回归>回归</h2><p>（Regression）</p><ul><li>小规模数据：<ul><li><strong>LinearRegression(不能解决拟合问题)</strong></li><li>岭回归</li></ul></li><li>大规模数据：SGDRegressor</li></ul><h3 id=线性回归>线性回归</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>LinearRegression</span><span class=p>,</span> <span class=n>SGDRegressor</span>
</span></span></code></pre></td></tr></table></div></div><p>（Linear Regression）</p><p>线性回归 (Linear regression) 是利用<strong>回归方程(函数)<strong>对一个或</strong>多个自变量(特征值)和因变量(目标值)之间</strong>关系进行建模的一种分析方式</p><h4 id=api-15>API</h4><h5 id=正规方程>正规方程</h5><p>正规方程的优化方法，不能解决拟合问题，一次性求解，针对小数据</p><ul><li><em><strong>LinearRegression(fit_intercept=True)</strong></em><ul><li><em>fit_intercept</em>：是否计算偏置</li><li>属性<ul><li><em>LinearRegression.coef_</em>：权重系数（回归系数）</li><li><em>LinearRegression.intercept_</em>：偏置</li></ul></li></ul></li></ul><h5 id=梯度下降>梯度下降</h5><p>其实是随机梯度下降</p><ul><li><em><strong>SGDRegressor(loss=&ldquo;squared_loss&rdquo;, fit_intercept=True, learning_rate =&lsquo;invscaling&rsquo;, eta0=0.01)</strong></em><ul><li>SGDRegressor类实现了随机梯度下降学习，它支持不同的 <strong>loss 函数和正则化惩罚项</strong>来拟合线性回归模型。</li><li><em>loss</em>：损失类型<ul><li><strong>loss=”squared_loss”: 普通最小二乘法</strong></li></ul></li><li><em>fit_intercept</em>：是否计算偏置</li><li><em>learning_rate</em>：string, optional<ul><li>学习率填充，对于一个常数值的学习率来说，可以使用 learning_rate=’constant’ ，并使用 eta0 来指定学习率。</li><li>&lsquo;constant&rsquo;：eta = eta0</li><li>&lsquo;optimal&rsquo;：eta = 1.0 / (alpha * (t + t0)) [default]</li><li>&lsquo;invscaling&rsquo;：eta = eta0 / pow(t, power_t=0.25)</li></ul></li><li><em>max_iter</em>：迭代次数</li><li>属性<ul><li><em>SGDRegressor.coef_</em>：回归系数</li><li><em>SGDRegressor.intercept_</em>：偏置</li></ul></li></ul></li></ul><h5 id=对比>对比</h5><table><thead><tr><th style=text-align:center>梯度下降</th><th style=text-align:center>正规方程</th></tr></thead><tbody><tr><td style=text-align:center>需要选择学习率</td><td style=text-align:center>不需要</td></tr><tr><td style=text-align:center>需要迭代求解</td><td style=text-align:center>一次运算得出</td></tr><tr><td style=text-align:center>特征数量较大可以使用</td><td style=text-align:center>需要计算方程，时间复杂度高O(n3)</td></tr></tbody></table><h4 id=回归性能评估>回归性能评估</h4><h5 id=均方误差-mse>均方误差 MSE</h5><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>mean_squared_error</span>
</span></span></code></pre></td></tr></table></div></div><p>均方误差（Mean Squared Error）MSE 评价机制</p><p><em><strong>mean_squared_error(y_test, y_pred)</strong></em></p><ul><li>均方误差回归损失</li><li><em>y_test</em>：真实值</li><li><em>y_pred</em>：预测值</li><li><em>return</em>：浮点数结果</li></ul><h5 id=平均绝对误差-mae>平均绝对误差 MAE</h5><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>mean_absolute_error</span>
</span></span></code></pre></td></tr></table></div></div><p><em><strong>mean_absolute_error(y_test, y_pred)</strong></em></p><ul><li>平均绝对误差回归损失</li><li><em>y_test</em>：真实值</li><li><em>y_pred</em>：预测值</li><li><em>return</em>：浮点数结果</li></ul><h4 id=例-12>例</h4><h5 id=初始化-1>初始化</h5><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>LinearRegression</span><span class=p>,</span> <span class=n>SGDRegressor</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>load_boston</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>StandardScaler</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>mean_squared_error</span>  <span class=c1># 均方误差</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>boston_data</span> <span class=o>=</span> <span class=n>load_boston</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;特征数量为:(样本数,特征数)&#34;</span><span class=p>,</span> <span class=n>boston_data</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x_train</span><span class=p>,</span> <span class=n>x_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>boston_data</span><span class=o>.</span><span class=n>data</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                                    <span class=n>boston_data</span><span class=o>.</span><span class=n>target</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>22</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 标准化</span>
</span></span><span class=line><span class=cl><span class=n>transfer</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>x_train</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>x_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x_test</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>x_test</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h5 id=正规方程-1>正规方程</h5><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>estimator</span> <span class=o>=</span> <span class=n>LinearRegression</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>estimator</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>x_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;正规方程_权重系数为: &#34;</span><span class=p>,</span> <span class=n>estimator</span><span class=o>.</span><span class=n>coef_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;正规方程_偏置为:&#34;</span><span class=p>,</span> <span class=n>estimator</span><span class=o>.</span><span class=n>intercept_</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>y_predict</span> <span class=o>=</span> <span class=n>estimator</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>x_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>error</span> <span class=o>=</span> <span class=n>mean_squared_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_predict</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;正规方程_房价预测:&#34;</span><span class=p>,</span> <span class=n>y_predict</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;正规方程_均分误差:&#34;</span><span class=p>,</span> <span class=n>error</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>正规方程_权重系数为</span><span class=p>:</span>  <span class=p>[</span><span class=o>-</span><span class=mf>0.64817766</span>  <span class=mf>1.14673408</span> <span class=o>-</span><span class=mf>0.05949444</span>  <span class=mf>0.74216553</span> <span class=o>-</span><span class=mf>1.95515269</span>  <span class=mf>2.70902585</span>
</span></span><span class=line><span class=cl> <span class=o>-</span><span class=mf>0.07737374</span> <span class=o>-</span><span class=mf>3.29889391</span>  <span class=mf>2.50267196</span> <span class=o>-</span><span class=mf>1.85679269</span> <span class=o>-</span><span class=mf>1.75044624</span>  <span class=mf>0.87341624</span>
</span></span><span class=line><span class=cl> <span class=o>-</span><span class=mf>3.91336869</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>正规方程_偏置为</span><span class=p>:</span> <span class=mf>22.62137203166228</span>
</span></span><span class=line><span class=cl><span class=n>正规方程_房价预测</span><span class=p>:</span> <span class=p>[</span><span class=mf>28.22944896</span> <span class=mf>31.5122308</span>  <span class=mf>21.11612841</span> <span class=mf>32.6663189</span>  <span class=mf>20.0023467</span>  <span class=mf>19.07315705</span>
</span></span><span class=line><span class=cl> <span class=mf>21.09772798</span> <span class=mf>19.61400153</span> <span class=mf>19.61907059</span> <span class=mf>32.87611987</span> <span class=mf>20.97911561</span> <span class=mf>27.52898011</span>
</span></span><span class=line><span class=cl> <span class=mf>15.54701758</span> <span class=mf>19.78630176</span> <span class=o>......</span>
</span></span><span class=line><span class=cl> <span class=mf>15.17700342</span>  <span class=mf>3.81620663</span> <span class=mf>29.18194848</span> <span class=mf>20.68544417</span> <span class=mf>22.32934783</span> <span class=mf>28.01568563</span>
</span></span><span class=line><span class=cl> <span class=mf>28.58237108</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>正规方程_均分误差</span><span class=p>:</span> <span class=mf>20.627513763095386</span>
</span></span></code></pre></td></tr></table></div></div><h5 id=梯度下降-1>梯度下降</h5><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>estimator</span> <span class=o>=</span> <span class=n>SGDRegressor</span><span class=p>(</span><span class=n>learning_rate</span><span class=o>=</span><span class=s2>&#34;constant&#34;</span><span class=p>,</span> <span class=n>eta0</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>10000</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># estimator = SGDRegressor(penalty=&#39;l2&#39;, loss=&#34;squared_loss&#34;)  # 这样设置就相当于岭回归, 但是建议用Ridge方法</span>
</span></span><span class=line><span class=cl><span class=n>estimator</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>x_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;梯度下降_权重系数为: &#34;</span><span class=p>,</span> <span class=n>estimator</span><span class=o>.</span><span class=n>coef_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;梯度下降_偏置为:&#34;</span><span class=p>,</span> <span class=n>estimator</span><span class=o>.</span><span class=n>intercept_</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>y_predict</span> <span class=o>=</span> <span class=n>estimator</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>x_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>error</span> <span class=o>=</span> <span class=n>mean_squared_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_predict</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;梯度下降_房价预测:&#34;</span><span class=p>,</span> <span class=n>y_predict</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;梯度下降_均分误差:&#34;</span><span class=p>,</span> <span class=n>error</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>梯度下降_权重系数为</span><span class=p>:</span>  <span class=p>[</span><span class=o>-</span><span class=mf>0.63057536</span>  <span class=mf>1.10395195</span>  <span class=mf>0.0426204</span>   <span class=mf>1.11219718</span> <span class=o>-</span><span class=mf>1.91486635</span>  <span class=mf>2.72806163</span>
</span></span><span class=line><span class=cl> <span class=o>-</span><span class=mf>0.05021542</span> <span class=o>-</span><span class=mf>3.52443232</span>  <span class=mf>2.47863671</span> <span class=o>-</span><span class=mf>1.62374879</span> <span class=o>-</span><span class=mf>1.9093765</span>   <span class=mf>1.08972091</span>
</span></span><span class=line><span class=cl> <span class=o>-</span><span class=mf>4.48569927</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>梯度下降_偏置为</span><span class=p>:</span> <span class=p>[</span><span class=mf>22.36660176</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>梯度下降_房价预测</span><span class=p>:</span> <span class=p>[</span><span class=mf>28.44139247</span> <span class=mf>31.71808756</span> <span class=mf>20.86031611</span> <span class=mf>34.1638423</span>  <span class=mf>19.35660167</span> <span class=mf>19.18397968</span>
</span></span><span class=line><span class=cl> <span class=mf>20.97064914</span> <span class=mf>18.87641833</span> <span class=mf>18.87914517</span> <span class=o>......</span>
</span></span><span class=line><span class=cl> <span class=mf>14.57654182</span>  <span class=mf>2.54082058</span> <span class=mf>29.38973401</span> <span class=mf>20.80732646</span> <span class=mf>21.65598607</span> <span class=mf>27.85659704</span>
</span></span><span class=line><span class=cl> <span class=mf>29.41864109</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>梯度下降_均分误差</span><span class=p>:</span> <span class=mf>20.997365545229272</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=岭回归>岭回归</h3><p>（Ridge Regression）</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>Ridge</span><span class=p>,</span> <span class=n>RidgeCV</span>
</span></span></code></pre></td></tr></table></div></div><p>岭回归，其实也是一种线性回归。只不过在算法建立回归方程时候，加上正则化的限制，从而达到解决过拟合的效果</p><h4 id=原理-5>原理</h4><p>正则化类别</p><ul><li>L2 正则化<ul><li>作用：可以使得其中一些 W 的都很小，都接近于 0，削弱某个特征的影响</li><li>优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象</li><li>Ridge 回归</li></ul></li><li>L1正则化<ul><li>作用：可以使得其中一些W的值直接为0，删除这个特征的影响</li><li>LASSO 回归</li></ul></li></ul><blockquote><p>线性回归的损失函数用最小二乘法，等价于当预测值与真实值的误差满足正态分布时的极大似然估计；岭回归的损失函数，是最小二乘法+L2范数，等价于当预测值与真实值的误差满足正态分布，且权重值也满足正态分布（先验分布）时的最大后验估计；LASSO的损失函数，是最小二乘法+L1范数，等价于等价于当预测值与真实值的误差满足正态分布，且且权重值满足拉普拉斯分布（先验分布）时的最大后验估计</p></blockquote><h4 id=api-16>API</h4><h5 id=常规岭回归>常规岭回归</h5><p><em><strong>Ridge(alpha=1.0, fit_intercept=True,solver=&ldquo;auto&rdquo;, normalize=False)</strong></em></p><ul><li><p>具有 L2 正则化的线性回归</p></li><li><p><em>alpha</em>：正则化力度，也叫 λ，即惩罚项系数</p><ul><li>λ取值：0~1 1~10</li></ul></li><li><p><em>solver</em>：会根据数据自动选择优化方法</p><ul><li><em>sag</em>：如果数据集、特征都比较大，选择该随机梯度下降优化</li></ul></li><li><p><em>normalize</em>：数据是否进行标准化</p><ul><li><code>normalize=False</code>：可以在 fit 之前调用 preprocessing.StandardScaler 标准化数据</li></ul></li><li><p>属性</p><ul><li><p><em>Ridge.coef_</em>：回归权重</p></li><li><p><em>Ridge.intercept_</em>：回归偏置</p></li></ul></li></ul><p>Ridge 方法相当于 <code>SGDRegressor(penalty='l2', loss="squared_loss")</code>, 只不过 SGDRegressor 实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG)</p><h5 id=交叉验证岭回归>交叉验证岭回归</h5><p><em><strong>RidgeCV(_BaseRidgeCV, RegressorMixin)</strong></em></p><ul><li>具有 L2 正则化的线性回归，可以进行交叉验证</li><li><em>alphas</em>：alpha 列表</li><li><em>cv</em>：交叉验证次数</li><li><em>coef_</em>：回归系数</li></ul><h4 id=例-13>例</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.linear_model</span> <span class=kn>import</span> <span class=n>Ridge</span><span class=p>,</span> <span class=n>RidgeCV</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>load_boston</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_split</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>StandardScaler</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>mean_squared_error</span>  <span class=c1># 均方误差</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>boston_data</span> <span class=o>=</span> <span class=n>load_boston</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>x_train</span><span class=p>,</span> <span class=n>x_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>train_test_split</span><span class=p>(</span><span class=n>boston_data</span><span class=o>.</span><span class=n>data</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                                    <span class=n>boston_data</span><span class=o>.</span><span class=n>target</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>22</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 标准化</span>
</span></span><span class=line><span class=cl><span class=n>transfer</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>x_train</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>x_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>x_test</span> <span class=o>=</span> <span class=n>transfer</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>x_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 岭回归</span>
</span></span><span class=line><span class=cl><span class=n>estimator</span> <span class=o>=</span> <span class=n>Ridge</span><span class=p>(</span><span class=n>max_iter</span><span class=o>=</span><span class=mi>10000</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>  <span class=c1># 岭回归</span>
</span></span><span class=line><span class=cl><span class=c1># estimator = RidgeCV(alphas=[0.1, 0.2, 0.3, 0.5])  # 加了交叉验证的岭回归</span>
</span></span><span class=line><span class=cl><span class=n>estimator</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>x_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;岭回归_权重系数为: &#34;</span><span class=p>,</span> <span class=n>estimator</span><span class=o>.</span><span class=n>coef_</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;岭回归_偏置为:&#34;</span><span class=p>,</span> <span class=n>estimator</span><span class=o>.</span><span class=n>intercept_</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>y_predict</span> <span class=o>=</span> <span class=n>estimator</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>x_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>error</span> <span class=o>=</span> <span class=n>mean_squared_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_predict</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;岭回归_房价预测:&#34;</span><span class=p>,</span> <span class=n>y_predict</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;岭回归_均分误差:&#34;</span><span class=p>,</span> <span class=n>error</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>岭回归_权重系数为</span><span class=p>:</span>  <span class=p>[</span><span class=o>-</span><span class=mf>0.64193209</span>  <span class=mf>1.13369189</span> <span class=o>-</span><span class=mf>0.07675643</span>  <span class=mf>0.74427624</span> <span class=o>-</span><span class=mf>1.93681163</span>  <span class=mf>2.71424838</span>
</span></span><span class=line><span class=cl> <span class=o>-</span><span class=mf>0.08171268</span> <span class=o>-</span><span class=mf>3.27871121</span>  <span class=mf>2.45697934</span> <span class=o>-</span><span class=mf>1.81200596</span> <span class=o>-</span><span class=mf>1.74659067</span>  <span class=mf>0.87272606</span>
</span></span><span class=line><span class=cl> <span class=o>-</span><span class=mf>3.90544403</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>岭回归_偏置为</span><span class=p>:</span> <span class=mf>22.62137203166228</span>
</span></span><span class=line><span class=cl><span class=n>岭回归_房价预测</span><span class=p>:</span> <span class=p>[</span><span class=mf>28.22536271</span> <span class=mf>31.50554479</span> <span class=mf>21.13191715</span> <span class=mf>32.65799504</span> <span class=mf>20.02127243</span> <span class=mf>19.07245621</span>
</span></span><span class=line><span class=cl> <span class=mf>21.10832868</span> <span class=mf>19.61646071</span> <span class=o>.....</span><span class=mf>.15.19441045</span>  <span class=mf>3.81755887</span> <span class=mf>29.1743764</span>  <span class=mf>20.68219692</span> <span class=mf>22.33163756</span> <span class=mf>28.01411044</span>
</span></span><span class=line><span class=cl> <span class=mf>28.55668351</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>岭回归_均分误差</span><span class=p>:</span> <span class=mf>20.641771606180914</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=聚类>聚类</h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.cluster</span> <span class=kn>import</span> <span class=n>KMeans</span>
</span></span></code></pre></td></tr></table></div></div><p>K-means（K均值聚类）</p><ul><li>特点：采用迭代式算法，直观易懂并且非常实用</li><li>缺点：容易收敛到局部最优解(多次聚类)</li></ul><h3 id=聚类步骤>聚类步骤</h3><ol><li><p>随机设置 K 个特征空间内的点作为初始的聚类中心</p></li><li><p>对于其他每个点计算到 K 个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别</p></li><li><p>接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值）</p></li><li><p>如果计算得出的新中心点与原中心点一样，那么结束，否则重新进行第二步过程</p></li></ol><img src=https://trou.oss-cn-shanghai.aliyuncs.com/img/K-means过程分析.png alt=K-means过程分析 style=zoom:67%><h3 id=api-17>API</h3><p><em><strong>KMeans(n_clusters=8, init=‘k-means++’…)</strong></em></p><ul><li>n_clusters：开始的聚类中心数量</li><li>init：初始化方法，默认为&rsquo;k-means ++’</li></ul><p><em>KMeans.labels_</em>：默认标记的类型，可以和真实值比较（不是值比较）</p><h3 id=轮廓系数>轮廓系数</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>silhouette_score</span>
</span></span></code></pre></td></tr></table></div></div><p>轮廓系数作为 Kmeans 的性能评估指标</p><h4 id=公式-3>公式</h4><p>$$
sc_i=\cfrac{b_i-a_i}{max(b_i,a_i)}
$$</p><blockquote><p>注：对于每个点 $i$ 为已聚类数据中的样本 ，$b_i$ 为 $i$ 到其它族群的所有样本的距离最小值，$a_i$ 为 $i$ 到本身簇的距离平均值。最终计算出所有的样本点的轮廓系数平均值</p></blockquote><h4 id=轮廓系数值分析>轮廓系数值分析</h4><img src=https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20210118223723565.png alt=image-20210118223723565 style=zoom:60%><p><strong>分析过程</strong>（以一个蓝1点为例）</p><ol><li><p>计算出蓝1离本身族群所有点的距离的平均值$a_i$</p></li><li><p>蓝1到其它两个族群的距离计算出平均值红平均，绿平均，取最小的那个距离作为$b_i$</p></li><li><p>根据公式：极端值考虑：如果$b_i &#187;a_i$，那么公式结果趋近于1；如果$a_i&#187;>b_i$，那么公式结果趋近于-1</p></li></ol><p><strong>结论</strong>：如果$b_i&#187;a_i$，趋近于1，效果好， $b_i&#171;a_i$，趋近于-1，效果不好。轮廓系数的值是介于 [-1,1] ，越趋近于1代表内聚度和分离度都相对较优</p><h4 id=api-18>API</h4><ul><li><em><strong>silhouette_score(X, labels)</strong></em>：计算所有样本的平均轮廓系数<ul><li>X：特征值</li><li>labels：被聚类标记的目标值（聚类结果）</li></ul></li></ul><h3 id=例-14>例</h3><p>分析</p><ol><li><p>降维之后的数据</p></li><li><p>k-means聚类</p></li><li><p>聚类结果显示</p></li><li><p>用户聚类结果评估</p></li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=o>%</span><span class=n>matplotlib</span> <span class=n>inline</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.datasets.samples_generator</span> <span class=kn>import</span> <span class=n>make_blobs</span>
</span></span><span class=line><span class=cl><span class=c1># X为样本特征，Y为样本簇类别， 共1000个样本，每个样本2个特征，共4个簇，簇中心在[-1,-1], [0,0],[1,1], [2,2]， 簇方差分别为[0.4, 0.2, 0.2]</span>
</span></span><span class=line><span class=cl><span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>make_blobs</span><span class=p>(</span><span class=n>n_samples</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span> <span class=n>n_features</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>centers</span><span class=o>=</span><span class=p>[[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span><span class=mi>2</span><span class=p>]],</span> <span class=n>cluster_std</span><span class=o>=</span><span class=p>[</span><span class=mf>0.4</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>marker</span><span class=o>=</span><span class=s1>&#39;o&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>案例样本</p><img src=https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20210119103734104.png alt=image-20210119103734104 style=zoom:67%><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.cluster</span> <span class=kn>import</span> <span class=n>KMeans</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#y_pred = KMeans(n_clusters=4).fit_predict(X)</span>
</span></span><span class=line><span class=cl><span class=n>estimator</span> <span class=o>=</span> <span class=n>KMeans</span><span class=p>(</span><span class=n>n_clusters</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>init</span><span class=o>=</span><span class=s1>&#39;k-means++&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>estimator</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y_pred</span> <span class=o>=</span> <span class=n>estimator</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>y_pred</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><img src=https://trou.oss-cn-shanghai.aliyuncs.com/img/image-20210119104047678.png alt=image-20210119104047678 style=zoom:67%><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>silhouette_score</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>score</span> <span class=o>=</span> <span class=n>silhouette_score</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;模型轮廓系数为(1 最好, -1 最差):&#34;</span><span class=p>,</span> <span class=n>score</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>输出</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>模型轮廓系数为</span><span class=p>(</span><span class=mi>1</span> <span class=n>最好</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span> <span class=n>最差</span><span class=p>):</span> <span class=mf>0.6634549555891298</span>
</span></span></code></pre></td></tr></table></div></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 2022-03-14</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/Notes/tags/python/>Python</a>,&nbsp;<a href=/Notes/tags/ml/>ml</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/Notes/>主页</a></span></section></div><div class=post-nav><a href=/Notes/posts/%E5%88%B7%E9%A2%98/array/209.-%E9%95%BF%E5%BA%A6%E6%9C%80%E5%B0%8F%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84/ class=prev rel=prev title="209. 长度最小的子数组"><i class="fas fa-angle-left fa-fw"></i>209. 长度最小的子数组</a>
<a href=/Notes/posts/python/dataanalysis/pandas/ class=next rel=next title=pandas>pandas<i class="fas fa-angle-right fa-fw"></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>酒困路长惟欲睡，日高人渴漫思茶</div><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2022</span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title=回到顶部><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title=查看评论><i class="fas fa-comment fa-fw"></i></a></div><link rel=stylesheet href=/Notes/lib/katex/katex.min.css><link rel=stylesheet href=/Notes/lib/katex/copy-tex.min.css><script type=text/javascript src=/Notes/lib/smooth-scroll/smooth-scroll.min.js></script><script type=text/javascript src=/Notes/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/Notes/lib/lunr/lunr.min.js></script><script type=text/javascript src=/Notes/lib/lunr/lunr.stemmer.support.min.js></script><script type=text/javascript src=/Notes/lib/lunr/lunr.zh.min.js></script><script type=text/javascript src=/Notes/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/Notes/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/Notes/lib/katex/katex.min.js></script><script type=text/javascript src=/Notes/lib/katex/auto-render.min.js></script><script type=text/javascript src=/Notes/lib/katex/copy-tex.min.js></script><script type=text/javascript src=/Notes/lib/katex/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:45},comment:{},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/Notes/index.json",lunrLanguageCode:"zh",lunrSegmentitURL:"/Notes/lib/lunr/lunr.segmentit.js",maxResultLength:30,noResultsFound:"没有找到结果",snippetLength:50,type:"lunr"}}</script><script type=text/javascript src=/Notes/js/theme.min.js></script></body></html>